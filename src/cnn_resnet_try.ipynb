{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3027, 0.6614],\n",
      "        [0.4898, 0.1737]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#coding=utf8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as scio\n",
    "import hiddenlayer as h\n",
    "from visdom import Visdom\n",
    "import datetime\n",
    "import os\n",
    "# import reuse\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "\n",
    "# 这里很普通的检查cuda可用性\n",
    "x = torch.rand(2,2)\n",
    "print(x)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_01_04_22:16:23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n",
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "# 以下是检查点路径、监视窗口初始化\n",
    "# 请在当前环境下 CMD 输入python -m visdom.server 启动监视器\n",
    "# 数据处理现在已移至 emgDataprocess.ipynb\n",
    "ckpDir = './/ckp//c10'\n",
    "if not os.path.exists(ckpDir):\n",
    "    os.makedirs(ckpDir)\n",
    "\n",
    "timeForSave = datetime.datetime.now().strftime('%Y_%m_%d_%H:%M:%S')\n",
    "print(timeForSave)\n",
    "vizx = 0\n",
    "viz = Visdom()\n",
    "viz1 = Visdom()\n",
    "viz2 = Visdom()\n",
    "viz3 = Visdom()\n",
    "viz4 = Visdom()\n",
    "viz5 = Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "(tensor([[[0.0171, 0.0073, 0.0024, 0.0024, 0.0024, 0.0024, 0.0757, 0.0317,\n",
      "          0.0024, 0.0146],\n",
      "         [0.0171, 0.0244, 0.0024, 0.0024, 0.0024, 0.0024, 0.0684, 0.0342,\n",
      "          0.0024, 0.0146],\n",
      "         [0.0146, 0.0269, 0.0024, 0.0024, 0.0024, 0.0024, 0.0610, 0.0342,\n",
      "          0.0024, 0.0146],\n",
      "         [0.0220, 0.0195, 0.0024, 0.0024, 0.0024, 0.0024, 0.0513, 0.0366,\n",
      "          0.0024, 0.0098],\n",
      "         [0.0269, 0.0171, 0.0024, 0.0024, 0.0024, 0.0024, 0.0488, 0.0415,\n",
      "          0.0024, 0.0073],\n",
      "         [0.0269, 0.0293, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0488,\n",
      "          0.0024, 0.0073],\n",
      "         [0.0244, 0.0317, 0.0024, 0.0024, 0.0024, 0.0024, 0.1001, 0.0635,\n",
      "          0.0024, 0.0098],\n",
      "         [0.0220, 0.0244, 0.0024, 0.0024, 0.0024, 0.0024, 0.1074, 0.0757,\n",
      "          0.0024, 0.0195],\n",
      "         [0.0171, 0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.1172, 0.0854,\n",
      "          0.0024, 0.0220],\n",
      "         [0.0195, 0.0049, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0854,\n",
      "          0.0024, 0.0220],\n",
      "         [0.0269, 0.0049, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0806,\n",
      "          0.0024, 0.0146],\n",
      "         [0.0342, 0.0171, 0.0024, 0.0024, 0.0024, 0.0024, 0.1221, 0.0781,\n",
      "          0.0024, 0.0098],\n",
      "         [0.0391, 0.0171, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0732,\n",
      "          0.0024, 0.0049],\n",
      "         [0.0488, 0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.1001, 0.0659,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0464, 0.0122, 0.0024, 0.0024, 0.0049, 0.0024, 0.0830, 0.0610,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0464, 0.0073, 0.0024, 0.0024, 0.0024, 0.0024, 0.0757, 0.0562,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0439, 0.0244, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0562,\n",
      "          0.0024, 0.0073],\n",
      "         [0.0488, 0.0415, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0537,\n",
      "          0.0024, 0.0098],\n",
      "         [0.0586, 0.0635, 0.0024, 0.0024, 0.0024, 0.0024, 0.0952, 0.0562,\n",
      "          0.0024, 0.0122],\n",
      "         [0.0537, 0.1196, 0.0049, 0.0024, 0.0024, 0.0024, 0.1025, 0.0537,\n",
      "          0.0098, 0.0098]]], dtype=torch.float64), 0)\n"
     ]
    }
   ],
   "source": [
    "# 自定义数据集类\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图片转换为Tensor,归一化至[0,1]\n",
    "])\n",
    "\n",
    "class EMGDataset(Dataset):\n",
    " \n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transforms = transforms.ToTensor()\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        emgData = self.data[index,:,:,:]\n",
    "        emgData = np.squeeze(emgData)#似乎不应该压缩了\n",
    "        emglabel = self.label[index]\n",
    "        emglabel = emglabel.astype(np.int16)\n",
    "        emgData = self.transforms(emgData)      \n",
    "        \n",
    "        return emgData,emglabel\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    " \n",
    "dataarray = np.load('../data/OpenganDataSet2213.npy',allow_pickle=True)\n",
    "CNNdataset = dataarray.item()\n",
    "print(type(CNNdataset))\n",
    "traindata = CNNdataset['Xtrain']\n",
    "trainlabel = CNNdataset['Ytrain']\n",
    "testdata = CNNdataset['Xtest']\n",
    "testlabel = CNNdataset['Ytest']\n",
    "# trainunknown_data = CNNdataset['Xtrain_unknown']\n",
    "# trainunknownc_label = CNNdataset['Ytrain_unknown']\n",
    "# # print(trainlabel[:,0])\n",
    "\n",
    "trainlabel = trainlabel[:,0]\n",
    "testlabel = testlabel[:,0]\n",
    "# trainunknownc_label = trainunknownc_label[:,0]\n",
    "# print(type(trainlabel))\n",
    "train_set = EMGDataset(traindata, trainlabel)\n",
    "test_set = EMGDataset(testdata, testlabel)\n",
    "# train_unknown = EMGDataset(trainunknown_data,trainunknownc_label)\n",
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, pin_memory=True,\n",
    "#                                             num_workers=3)\n",
    "\n",
    "sample = next(iter(test_set))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=4352, out_features=128, bias=True)\n",
      "  (out): Linear(in_features=128, out_features=6, bias=True)\n",
      "  (dr1): Dropout2d(p=0.2, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [1, 32, 38, 8]             320\n",
      "            Conv2d-2             [1, 32, 35, 5]           9,248\n",
      "            Linear-3                   [1, 128]         557,184\n",
      "         Dropout2d-4                   [1, 128]               0\n",
      "            Linear-5                     [1, 6]             774\n",
      "================================================================\n",
      "Total params: 567,526\n",
      "Trainable params: 567,526\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.12\n",
      "Params size (MB): 2.16\n",
      "Estimated Total Size (MB): 2.29\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.png'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义神经网络,Resnet\n",
    "class Resnet_Basicblock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride = 1):\n",
    "        super(Resnet_Basicblock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\\\n",
    "            stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\\\n",
    "            stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # 保证最后一步求和时特征图的维度一致\n",
    "        if stride !=1 or in_channels != self.expansion*out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion*out_channels,\\\n",
    "                    kernel_size=1, stride=stride, bias = False),\n",
    "                nn.BatchNorm2d(self.expansion*out_channels)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 1st 3*3, k = 64\n",
    "        out = F.relu(self.bn1(self.conv1))\n",
    "        # 2nd 3*3, k = 64\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        # add input x\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "class Resnet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(Resnet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1,\\\n",
    "            padding=1, bis=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (out): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dr1): Dropout2d(p=0.2, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [1, 32, 20, 10]             320\n",
      "            Conv2d-2             [1, 32, 19, 9]           9,248\n",
      "            Linear-3                   [1, 128]         589,952\n",
      "         Dropout2d-4                   [1, 128]               0\n",
      "            Linear-5                    [1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 600,810\n",
      "Trainable params: 600,810\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.09\n",
      "Params size (MB): 2.29\n",
      "Estimated Total Size (MB): 2.39\n",
      "----------------------------------------------------------------\n",
      "Outputshape: torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.png'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义神经网络,CNN\n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32,\\\n",
    "             kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32,\\\n",
    "             kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, \\\n",
    "            kernel_size=3, padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=32 * 18 * 8, out_features=128)\n",
    "        self.out = nn.Linear(in_features=128, out_features=10)\n",
    "        self.dr1 = nn.Dropout2d(0.2)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        # t = self.dr1(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 32 * 18 * 8)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.dr1(t)\n",
    "\n",
    "        # (5) output layer\n",
    "        t = self.out(t)\n",
    "\n",
    "        return t\n",
    "\n",
    "net = Network()\n",
    "# 打印网络，检查输入输出 shape是否正确\n",
    "print(net)\n",
    "summary(net,(1,20,10),batch_size = 1,device = \"cpu\")\n",
    "\n",
    "# 可视化结构，hiddenlayer\n",
    "# vis_graph = h.build_graph(net, torch.zeros([1,1,40,10]))\n",
    "# vis_graph.theme = h.graph.THEMES[\"blue\"].copy()\n",
    "# vis_graph.save(\"./CNNtrynetframe\")\n",
    "# 可视化结构，torchviz\n",
    "sampleInput = torch.randn(1,1,20,10).requires_grad_(True)\n",
    "sampleOutput = net(sampleInput)\n",
    "print('Outputshape:',sampleOutput.shape)\n",
    "framevision = make_dot(sampleOutput, params=dict(list(net.named_parameters()) + [('x',sampleInput)]))\n",
    "framevision.format = \"png\"\n",
    "framevision.direcory = \"./\"\n",
    "framevision.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4228\n"
     ]
    }
   ],
   "source": [
    "# import datetime\n",
    "\n",
    "# timeForSave = datetime.datetime.now().strftime('%Y_%m_%d_%H:%M:%S')\n",
    "# print(timeForSave)\n",
    "print(trainlabel.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_train_acc: 0.17478713339640492 loss: 11.292065620422363 test_loss: 2.235746145248413 test_acc: 0.20526960784313725\n",
      "epoch 1 total_train_acc: 0.21665089877010407 loss: 10.881728410720825 test_loss: 2.0872159004211426 test_acc: 0.2261029411764706\n",
      "epoch 2 total_train_acc: 0.25733207190160834 loss: 10.523176431655884 test_loss: 2.1652181148529053 test_acc: 0.27144607843137253\n",
      "epoch 3 total_train_acc: 0.29730368968779564 loss: 10.059801936149597 test_loss: 1.9547343254089355 test_acc: 0.3002450980392157\n",
      "epoch 4 total_train_acc: 0.3098391674550615 loss: 9.6390141248703 test_loss: 1.7625138759613037 test_acc: 0.31924019607843135\n",
      "epoch 5 total_train_acc: 0.3403500473036897 loss: 9.344666361808777 test_loss: 1.7929034233093262 test_acc: 0.3321078431372549\n",
      "epoch 6 total_train_acc: 0.36258278145695366 loss: 8.936329245567322 test_loss: 1.689946174621582 test_acc: 0.39950980392156865\n",
      "epoch 7 total_train_acc: 0.39120151371807 loss: 8.66004490852356 test_loss: 1.6948480606079102 test_acc: 0.45465686274509803\n",
      "epoch 8 total_train_acc: 0.4198202459791864 loss: 8.318572282791138 test_loss: 1.7957977056503296 test_acc: 0.4736519607843137\n",
      "epoch 9 total_train_acc: 0.43732261116367077 loss: 8.020718812942505 test_loss: 1.3119843006134033 test_acc: 0.508578431372549\n",
      "epoch 10 total_train_acc: 0.4824976348155156 loss: 7.783445835113525 test_loss: 1.363629698753357 test_acc: 0.5514705882352942\n",
      "epoch 11 total_train_acc: 0.5165562913907285 loss: 7.563135743141174 test_loss: 1.6053026914596558 test_acc: 0.5631127450980392\n",
      "epoch 12 total_train_acc: 0.5215231788079471 loss: 7.3634116649627686 test_loss: 1.3433517217636108 test_acc: 0.5631127450980392\n",
      "epoch 13 total_train_acc: 0.5373699148533586 loss: 7.0354942083358765 test_loss: 1.3639163970947266 test_acc: 0.5857843137254902\n",
      "epoch 14 total_train_acc: 0.5648060548722801 loss: 6.846453428268433 test_loss: 1.5376062393188477 test_acc: 0.6133578431372549\n",
      "epoch 15 total_train_acc: 0.575685903500473 loss: 6.628004431724548 test_loss: 1.382171392440796 test_acc: 0.6237745098039216\n",
      "epoch 16 total_train_acc: 0.5934247871333964 loss: 6.393461465835571 test_loss: 1.099356770515442 test_acc: 0.6158088235294118\n",
      "epoch 17 total_train_acc: 0.5991012298959318 loss: 6.256406188011169 test_loss: 1.2361162900924683 test_acc: 0.6580882352941176\n",
      "epoch 18 total_train_acc: 0.6241721854304636 loss: 6.047335147857666 test_loss: 1.0196586847305298 test_acc: 0.6599264705882353\n",
      "epoch 19 total_train_acc: 0.6234626300851467 loss: 6.009353518486023 test_loss: 1.3767461776733398 test_acc: 0.6721813725490197\n",
      "epoch 20 total_train_acc: 0.640964995269631 loss: 5.754440665245056 test_loss: 0.9193629622459412 test_acc: 0.6875\n",
      "epoch 21 total_train_acc: 0.6478240302743614 loss: 5.741224050521851 test_loss: 0.9494733810424805 test_acc: 0.6850490196078431\n",
      "epoch 22 total_train_acc: 0.6568117313150426 loss: 5.51983904838562 test_loss: 0.872902512550354 test_acc: 0.7071078431372549\n",
      "epoch 23 total_train_acc: 0.6773888363292336 loss: 5.397328615188599 test_loss: 0.4993268549442291 test_acc: 0.7058823529411765\n",
      "epoch 24 total_train_acc: 0.6840113528855251 loss: 5.179171860218048 test_loss: 0.8240790963172913 test_acc: 0.7169117647058824\n",
      "epoch 25 total_train_acc: 0.684484389782403 loss: 5.2987454533576965 test_loss: 1.189949870109558 test_acc: 0.7169117647058824\n",
      "epoch 26 total_train_acc: 0.7038789025543992 loss: 5.073033332824707 test_loss: 0.8485361933708191 test_acc: 0.7267156862745098\n",
      "epoch 27 total_train_acc: 0.7012771996215705 loss: 4.947934448719025 test_loss: 1.0349124670028687 test_acc: 0.727328431372549\n",
      "epoch 28 total_train_acc: 0.7003311258278145 loss: 4.921935796737671 test_loss: 1.1475056409835815 test_acc: 0.7334558823529411\n",
      "epoch 29 total_train_acc: 0.7185430463576159 loss: 4.707971453666687 test_loss: 1.173669695854187 test_acc: 0.7377450980392157\n",
      "epoch 30 total_train_acc: 0.7071901608325449 loss: 4.692768216133118 test_loss: 0.8628419637680054 test_acc: 0.7328431372549019\n",
      "epoch 31 total_train_acc: 0.7209082308420057 loss: 4.63984489440918 test_loss: 1.026696801185608 test_acc: 0.7481617647058824\n",
      "epoch 32 total_train_acc: 0.7313150425733207 loss: 4.60599559545517 test_loss: 0.8400018811225891 test_acc: 0.7463235294117647\n",
      "epoch 33 total_train_acc: 0.7242194891201513 loss: 4.457412540912628 test_loss: 0.882255494594574 test_acc: 0.7463235294117647\n",
      "epoch 34 total_train_acc: 0.7362819299905392 loss: 4.367789447307587 test_loss: 0.8503834009170532 test_acc: 0.7536764705882353\n",
      "epoch 35 total_train_acc: 0.7429044465468306 loss: 4.344416320323944 test_loss: 0.6453878879547119 test_acc: 0.7561274509803921\n",
      "epoch 36 total_train_acc: 0.7426679280983917 loss: 4.384039282798767 test_loss: 0.9477861523628235 test_acc: 0.7542892156862745\n",
      "epoch 37 total_train_acc: 0.7478713339640491 loss: 4.174862027168274 test_loss: 0.8099226355552673 test_acc: 0.7640931372549019\n",
      "epoch 38 total_train_acc: 0.7492904446546831 loss: 4.086676061153412 test_loss: 1.1327705383300781 test_acc: 0.7640931372549019\n",
      "epoch 39 total_train_acc: 0.7466887417218543 loss: 4.062967419624329 test_loss: 0.8174245953559875 test_acc: 0.7665441176470589\n",
      "epoch 40 total_train_acc: 0.7587511825922422 loss: 3.9366976618766785 test_loss: 1.0958298444747925 test_acc: 0.7696078431372549\n",
      "epoch 41 total_train_acc: 0.7630085146641438 loss: 3.954415440559387 test_loss: 0.7201400399208069 test_acc: 0.7720588235294118\n",
      "epoch 42 total_train_acc: 0.7651371807000946 loss: 3.9105721712112427 test_loss: 0.7275235056877136 test_acc: 0.7738970588235294\n",
      "epoch 43 total_train_acc: 0.7585146641438032 loss: 3.918674945831299 test_loss: 0.8026885986328125 test_acc: 0.7769607843137255\n",
      "epoch 44 total_train_acc: 0.7731788079470199 loss: 3.8840332627296448 test_loss: 0.6288115978240967 test_acc: 0.7763480392156863\n",
      "epoch 45 total_train_acc: 0.771996215704825 loss: 3.7140506505966187 test_loss: 0.6152724027633667 test_acc: 0.7824754901960784\n",
      "epoch 46 total_train_acc: 0.77081362346263 loss: 3.7310975790023804 test_loss: 0.7329548597335815 test_acc: 0.7855392156862745\n",
      "epoch 47 total_train_acc: 0.7783822138126774 loss: 3.6904983520507812 test_loss: 0.6160621643066406 test_acc: 0.7794117647058824\n",
      "epoch 48 total_train_acc: 0.7750709555345316 loss: 3.596498668193817 test_loss: 0.9757533073425293 test_acc: 0.7837009803921569\n",
      "epoch 49 total_train_acc: 0.7831125827814569 loss: 3.5528169870376587 test_loss: 0.49098849296569824 test_acc: 0.7916666666666666\n",
      "epoch 50 total_train_acc: 0.7790917691579943 loss: 3.440393030643463 test_loss: 0.462526798248291 test_acc: 0.7904411764705882\n",
      "epoch 51 total_train_acc: 0.782639545884579 loss: 3.415213882923126 test_loss: 0.742526113986969 test_acc: 0.7971813725490197\n",
      "epoch 52 total_train_acc: 0.783349101229896 loss: 3.4127569794654846 test_loss: 0.9130011200904846 test_acc: 0.7996323529411765\n",
      "epoch 53 total_train_acc: 0.7909176915799432 loss: 3.4110182523727417 test_loss: 0.7376628518104553 test_acc: 0.8002450980392157\n",
      "epoch 54 total_train_acc: 0.7949385052034059 loss: 3.37009334564209 test_loss: 0.45434194803237915 test_acc: 0.7996323529411765\n",
      "epoch 55 total_train_acc: 0.7994323557237465 loss: 3.2181732654571533 test_loss: 0.6873347759246826 test_acc: 0.8100490196078431\n",
      "epoch 56 total_train_acc: 0.8010879848628193 loss: 3.304830312728882 test_loss: 0.6206569671630859 test_acc: 0.8075980392156863\n",
      "epoch 57 total_train_acc: 0.7947019867549668 loss: 3.2397223114967346 test_loss: 0.8864191174507141 test_acc: 0.8131127450980392\n",
      "epoch 58 total_train_acc: 0.8055818353831599 loss: 3.153441309928894 test_loss: 0.7077854871749878 test_acc: 0.8082107843137255\n",
      "epoch 59 total_train_acc: 0.8020340586565752 loss: 3.1875758171081543 test_loss: 0.5637157559394836 test_acc: 0.8075980392156863\n",
      "epoch 60 total_train_acc: 0.8015610217596972 loss: 3.102462887763977 test_loss: 0.7851836681365967 test_acc: 0.8112745098039216\n",
      "epoch 61 total_train_acc: 0.8110217596972564 loss: 3.100053548812866 test_loss: 0.4640493094921112 test_acc: 0.8131127450980392\n",
      "epoch 62 total_train_acc: 0.8093661305581835 loss: 3.0668448209762573 test_loss: 0.6704401969909668 test_acc: 0.8155637254901961\n",
      "epoch 63 total_train_acc: 0.8124408703878903 loss: 3.0074212551116943 test_loss: 0.8043947815895081 test_acc: 0.8229166666666666\n",
      "epoch 64 total_train_acc: 0.8105487228003785 loss: 3.037099778652191 test_loss: 0.41109776496887207 test_acc: 0.8137254901960784\n",
      "epoch 65 total_train_acc: 0.815515610217597 loss: 2.991036295890808 test_loss: 0.2643889784812927 test_acc: 0.8235294117647058\n",
      "epoch 66 total_train_acc: 0.8188268684957427 loss: 2.943733811378479 test_loss: 0.5713770985603333 test_acc: 0.8272058823529411\n",
      "epoch 67 total_train_acc: 0.8188268684957427 loss: 2.900832414627075 test_loss: 0.6002203822135925 test_acc: 0.8259803921568627\n",
      "epoch 68 total_train_acc: 0.8209555345316935 loss: 2.886298716068268 test_loss: 0.5792555809020996 test_acc: 0.8241421568627451\n",
      "epoch 69 total_train_acc: 0.8190633869441817 loss: 2.8471906781196594 test_loss: 0.6905944347381592 test_acc: 0.8284313725490197\n",
      "epoch 70 total_train_acc: 0.8207190160832545 loss: 2.924258232116699 test_loss: 0.8496096134185791 test_acc: 0.8265931372549019\n",
      "epoch 71 total_train_acc: 0.8301797540208137 loss: 2.8162702322006226 test_loss: 0.5817207098007202 test_acc: 0.8284313725490197\n",
      "epoch 72 total_train_acc: 0.8289971617786187 loss: 2.879643440246582 test_loss: 0.6833779811859131 test_acc: 0.8308823529411765\n",
      "epoch 73 total_train_acc: 0.8226111636707664 loss: 2.7560077905654907 test_loss: 0.7459589242935181 test_acc: 0.8321078431372549\n",
      "epoch 74 total_train_acc: 0.836329233680227 loss: 2.700420618057251 test_loss: 0.32878193259239197 test_acc: 0.8308823529411765\n",
      "epoch 75 total_train_acc: 0.8304162724692526 loss: 2.6720360815525055 test_loss: 0.27078956365585327 test_acc: 0.8345588235294118\n",
      "epoch 76 total_train_acc: 0.836565752128666 loss: 2.6331396102905273 test_loss: 0.4796651601791382 test_acc: 0.8376225490196079\n",
      "epoch 77 total_train_acc: 0.8294701986754967 loss: 2.663787543773651 test_loss: 0.6070438027381897 test_acc: 0.8296568627450981\n",
      "epoch 78 total_train_acc: 0.8339640491958372 loss: 2.6066744327545166 test_loss: 0.733342170715332 test_acc: 0.8400735294117647\n",
      "epoch 79 total_train_acc: 0.8377483443708609 loss: 2.55237478017807 test_loss: 0.7326955199241638 test_acc: 0.8370098039215687\n",
      "epoch 80 total_train_acc: 0.8342005676442763 loss: 2.537811905145645 test_loss: 0.6697283983230591 test_acc: 0.8333333333333334\n",
      "epoch 81 total_train_acc: 0.8403500473036897 loss: 2.572670638561249 test_loss: 0.6067896485328674 test_acc: 0.8388480392156863\n",
      "epoch 82 total_train_acc: 0.847682119205298 loss: 2.5128870010375977 test_loss: 0.48151737451553345 test_acc: 0.8419117647058824\n",
      "epoch 83 total_train_acc: 0.8412961210974456 loss: 2.447320342063904 test_loss: 0.3528525233268738 test_acc: 0.8419117647058824\n",
      "epoch 84 total_train_acc: 0.8427152317880795 loss: 2.5231728553771973 test_loss: 0.44218361377716064 test_acc: 0.8431372549019608\n",
      "epoch 85 total_train_acc: 0.8375118259224219 loss: 2.4885494112968445 test_loss: 0.3980313539505005 test_acc: 0.8431372549019608\n",
      "epoch 86 total_train_acc: 0.8460264900662252 loss: 2.4929221272468567 test_loss: 0.5284698605537415 test_acc: 0.8486519607843137\n",
      "epoch 87 total_train_acc: 0.848155156102176 loss: 2.453940451145172 test_loss: 0.49313050508499146 test_acc: 0.8394607843137255\n",
      "epoch 88 total_train_acc: 0.848155156102176 loss: 2.406934469938278 test_loss: 0.64555823802948 test_acc: 0.8455882352941176\n",
      "epoch 89 total_train_acc: 0.847445600756859 loss: 2.3847286701202393 test_loss: 0.5296431183815002 test_acc: 0.8468137254901961\n",
      "epoch 90 total_train_acc: 0.8488647114474929 loss: 2.3882356584072113 test_loss: 0.7602938413619995 test_acc: 0.8504901960784313\n",
      "epoch 91 total_train_acc: 0.8528855250709555 loss: 2.3385420441627502 test_loss: 0.44410890340805054 test_acc: 0.8462009803921569\n",
      "epoch 92 total_train_acc: 0.8486281929990539 loss: 2.3555606603622437 test_loss: 0.4259802997112274 test_acc: 0.8468137254901961\n",
      "epoch 93 total_train_acc: 0.8507568590350048 loss: 2.299635171890259 test_loss: 0.583820641040802 test_acc: 0.8498774509803921\n",
      "epoch 94 total_train_acc: 0.8436613055818354 loss: 2.2667214572429657 test_loss: 0.5056225657463074 test_acc: 0.8480392156862745\n",
      "epoch 95 total_train_acc: 0.847682119205298 loss: 2.37834632396698 test_loss: 0.5644289255142212 test_acc: 0.8529411764705882\n",
      "epoch 96 total_train_acc: 0.8547776726584674 loss: 2.37078458070755 test_loss: 0.6457173824310303 test_acc: 0.8547794117647058\n",
      "epoch 97 total_train_acc: 0.8573793755912961 loss: 2.23012837767601 test_loss: 0.6159757375717163 test_acc: 0.8517156862745098\n",
      "epoch 98 total_train_acc: 0.859035004730369 loss: 2.2612313330173492 test_loss: 0.5005214214324951 test_acc: 0.8455882352941176\n",
      "epoch 99 total_train_acc: 0.8488647114474929 loss: 2.2683300375938416 test_loss: 0.5299587249755859 test_acc: 0.8590686274509803\n",
      "epoch 100 total_train_acc: 0.858561967833491 loss: 2.287918508052826 test_loss: 0.4184836745262146 test_acc: 0.8504901960784313\n",
      "epoch 101 total_train_acc: 0.8526490066225165 loss: 2.1890269219875336 test_loss: 0.27116072177886963 test_acc: 0.8590686274509803\n",
      "epoch 102 total_train_acc: 0.8550141911069064 loss: 2.1602646708488464 test_loss: 0.2136974334716797 test_acc: 0.8529411764705882\n",
      "epoch 103 total_train_acc: 0.8545411542100284 loss: 2.1762897670269012 test_loss: 0.4912203848361969 test_acc: 0.8602941176470589\n",
      "epoch 104 total_train_acc: 0.85879848628193 loss: 2.189451664686203 test_loss: 0.7140066623687744 test_acc: 0.8449754901960784\n",
      "epoch 105 total_train_acc: 0.8545411542100284 loss: 2.1164348125457764 test_loss: 0.23201316595077515 test_acc: 0.8602941176470589\n",
      "epoch 106 total_train_acc: 0.8616367076631978 loss: 2.13260480761528 test_loss: 0.5068142414093018 test_acc: 0.8590686274509803\n",
      "epoch 107 total_train_acc: 0.8559602649006622 loss: 2.17930468916893 test_loss: 0.32094115018844604 test_acc: 0.8627450980392157\n",
      "epoch 108 total_train_acc: 0.8632923368022706 loss: 2.054752618074417 test_loss: 0.5628323554992676 test_acc: 0.8602941176470589\n",
      "epoch 109 total_train_acc: 0.8616367076631978 loss: 2.083648145198822 test_loss: 0.3657523989677429 test_acc: 0.8535539215686274\n",
      "epoch 110 total_train_acc: 0.8628192999053926 loss: 2.113652676343918 test_loss: 0.281464546918869 test_acc: 0.8639705882352942\n",
      "epoch 111 total_train_acc: 0.8642384105960265 loss: 2.044695198535919 test_loss: 0.29756420850753784 test_acc: 0.8541666666666666\n",
      "epoch 112 total_train_acc: 0.8651844843897823 loss: 2.0303587913513184 test_loss: 0.29211047291755676 test_acc: 0.8658088235294118\n",
      "epoch 113 total_train_acc: 0.8654210028382214 loss: 2.0160772800445557 test_loss: 0.2646730840206146 test_acc: 0.8639705882352942\n",
      "epoch 114 total_train_acc: 0.8682592242194891 loss: 2.08564230799675 test_loss: 0.15973490476608276 test_acc: 0.8615196078431373\n",
      "epoch 115 total_train_acc: 0.8642384105960265 loss: 1.9909983575344086 test_loss: 0.22302508354187012 test_acc: 0.8609068627450981\n",
      "epoch 116 total_train_acc: 0.8694418164616841 loss: 2.0578354001045227 test_loss: 0.42483916878700256 test_acc: 0.8694852941176471\n",
      "epoch 117 total_train_acc: 0.8670766319772942 loss: 2.022346317768097 test_loss: 0.3743123412132263 test_acc: 0.8596813725490197\n",
      "epoch 118 total_train_acc: 0.8661305581835383 loss: 2.018620491027832 test_loss: 0.32057103514671326 test_acc: 0.8676470588235294\n",
      "epoch 119 total_train_acc: 0.8713339640491958 loss: 1.9905269742012024 test_loss: 0.35970252752304077 test_acc: 0.8658088235294118\n",
      "epoch 120 total_train_acc: 0.8637653736991485 loss: 2.019450604915619 test_loss: 0.20013822615146637 test_acc: 0.8621323529411765\n",
      "epoch 121 total_train_acc: 0.8668401135288553 loss: 1.9570006430149078 test_loss: 0.38330382108688354 test_acc: 0.8609068627450981\n",
      "epoch 122 total_train_acc: 0.8670766319772942 loss: 1.9835392236709595 test_loss: 0.4762868285179138 test_acc: 0.866421568627451\n",
      "epoch 123 total_train_acc: 0.8736991485335857 loss: 1.8772053718566895 test_loss: 0.5574971437454224 test_acc: 0.8676470588235294\n",
      "epoch 124 total_train_acc: 0.8651844843897823 loss: 2.0030858516693115 test_loss: 0.2181536853313446 test_acc: 0.8719362745098039\n",
      "epoch 125 total_train_acc: 0.8649479659413434 loss: 1.9405400156974792 test_loss: 0.2616523504257202 test_acc: 0.8639705882352942\n",
      "epoch 126 total_train_acc: 0.8734626300851467 loss: 1.9556243121623993 test_loss: 0.2752770483493805 test_acc: 0.8719362745098039\n",
      "epoch 127 total_train_acc: 0.8703878902554399 loss: 1.874842494726181 test_loss: 0.2902106046676636 test_acc: 0.8639705882352942\n",
      "epoch 128 total_train_acc: 0.8720435193945127 loss: 1.8744934499263763 test_loss: 0.3841764032840729 test_acc: 0.8700980392156863\n",
      "epoch 129 total_train_acc: 0.8767738883632923 loss: 1.9063938856124878 test_loss: 0.5956104397773743 test_acc: 0.8713235294117647\n",
      "epoch 130 total_train_acc: 0.8727530747398297 loss: 1.8854382038116455 test_loss: 0.31483426690101624 test_acc: 0.8670343137254902\n",
      "epoch 131 total_train_acc: 0.8682592242194891 loss: 1.857934445142746 test_loss: 0.3216823935508728 test_acc: 0.8694852941176471\n",
      "epoch 132 total_train_acc: 0.8758278145695364 loss: 1.8905111849308014 test_loss: 0.2859165370464325 test_acc: 0.8682598039215687\n",
      "epoch 133 total_train_acc: 0.8770104068117314 loss: 1.9117384850978851 test_loss: 0.13570530712604523 test_acc: 0.8713235294117647\n",
      "epoch 134 total_train_acc: 0.8732261116367077 loss: 1.8335605263710022 test_loss: 0.4274143576622009 test_acc: 0.8731617647058824\n",
      "epoch 135 total_train_acc: 0.8779564806054873 loss: 1.7949834167957306 test_loss: 0.524194061756134 test_acc: 0.8713235294117647\n",
      "epoch 136 total_train_acc: 0.8815042573320719 loss: 1.7449779212474823 test_loss: 0.3312683403491974 test_acc: 0.8694852941176471\n",
      "epoch 137 total_train_acc: 0.8777199621570483 loss: 1.8286531865596771 test_loss: 0.2865319848060608 test_acc: 0.8762254901960784\n",
      "epoch 138 total_train_acc: 0.8826868495742668 loss: 1.8071652948856354 test_loss: 0.2396775633096695 test_acc: 0.8756127450980392\n",
      "epoch 139 total_train_acc: 0.8807947019867549 loss: 1.806351900100708 test_loss: 0.453925222158432 test_acc: 0.8774509803921569\n",
      "epoch 140 total_train_acc: 0.8763008514664143 loss: 1.813827097415924 test_loss: 0.5129439830780029 test_acc: 0.8743872549019608\n",
      "epoch 141 total_train_acc: 0.8765373699148533 loss: 1.7860760390758514 test_loss: 0.29573604464530945 test_acc: 0.8756127450980392\n",
      "epoch 142 total_train_acc: 0.8763008514664143 loss: 1.8147786855697632 test_loss: 0.31458231806755066 test_acc: 0.8792892156862745\n",
      "epoch 143 total_train_acc: 0.8767738883632923 loss: 1.7928330898284912 test_loss: 0.35745105147361755 test_acc: 0.8811274509803921\n",
      "epoch 144 total_train_acc: 0.8779564806054873 loss: 1.7296931147575378 test_loss: 0.25710466504096985 test_acc: 0.875\n",
      "epoch 145 total_train_acc: 0.881267738883633 loss: 1.8033355474472046 test_loss: 0.37176743149757385 test_acc: 0.8817401960784313\n",
      "epoch 146 total_train_acc: 0.8871807000946074 loss: 1.744980901479721 test_loss: 0.4133657217025757 test_acc: 0.8756127450980392\n",
      "epoch 147 total_train_acc: 0.8786660359508042 loss: 1.772648572921753 test_loss: 0.3948418200016022 test_acc: 0.8823529411764706\n",
      "epoch 148 total_train_acc: 0.8859981078524125 loss: 1.7348779439926147 test_loss: 0.24930277466773987 test_acc: 0.8780637254901961\n",
      "epoch 149 total_train_acc: 0.8784295175023652 loss: 1.6916275322437286 test_loss: 0.3841361701488495 test_acc: 0.8780637254901961\n",
      "epoch 150 total_train_acc: 0.8871807000946074 loss: 1.668460637331009 test_loss: 0.37176135182380676 test_acc: 0.8768382352941176\n",
      "epoch 151 total_train_acc: 0.8829233680227058 loss: 1.6710059344768524 test_loss: 0.22946204245090485 test_acc: 0.8762254901960784\n",
      "epoch 152 total_train_acc: 0.8864711447492905 loss: 1.681887298822403 test_loss: 0.19969603419303894 test_acc: 0.8817401960784313\n",
      "epoch 153 total_train_acc: 0.8850520340586566 loss: 1.7288575768470764 test_loss: 0.40182819962501526 test_acc: 0.8786764705882353\n",
      "epoch 154 total_train_acc: 0.8876537369914853 loss: 1.6764173209667206 test_loss: 0.3096843957901001 test_acc: 0.8848039215686274\n",
      "epoch 155 total_train_acc: 0.8857615894039735 loss: 1.6947867274284363 test_loss: 0.13709129393100739 test_acc: 0.8799019607843137\n",
      "epoch 156 total_train_acc: 0.8876537369914853 loss: 1.7015306055545807 test_loss: 0.6958403587341309 test_acc: 0.8811274509803921\n",
      "epoch 157 total_train_acc: 0.8871807000946074 loss: 1.6216511130332947 test_loss: 0.6445033550262451 test_acc: 0.8780637254901961\n",
      "epoch 158 total_train_acc: 0.8845789971617786 loss: 1.6718144714832306 test_loss: 0.6064422130584717 test_acc: 0.8878676470588235\n",
      "epoch 159 total_train_acc: 0.8850520340586566 loss: 1.6577607691287994 test_loss: 0.40010592341423035 test_acc: 0.8792892156862745\n",
      "epoch 160 total_train_acc: 0.8874172185430463 loss: 1.6793236434459686 test_loss: 0.5523931980133057 test_acc: 0.8805147058823529\n",
      "epoch 161 total_train_acc: 0.8888363292336803 loss: 1.6512491405010223 test_loss: 0.2637970447540283 test_acc: 0.883578431372549\n",
      "epoch 162 total_train_acc: 0.8871807000946074 loss: 1.6258794665336609 test_loss: 0.2441030591726303 test_acc: 0.8823529411764706\n",
      "epoch 163 total_train_acc: 0.8893093661305582 loss: 1.6428910195827484 test_loss: 0.22397352755069733 test_acc: 0.8799019607843137\n",
      "epoch 164 total_train_acc: 0.8859981078524125 loss: 1.5993418991565704 test_loss: 0.42045772075653076 test_acc: 0.883578431372549\n",
      "epoch 165 total_train_acc: 0.8876537369914853 loss: 1.6553234159946442 test_loss: 0.3650946319103241 test_acc: 0.8823529411764706\n",
      "epoch 166 total_train_acc: 0.8933301797540208 loss: 1.5788903534412384 test_loss: 0.14363278448581696 test_acc: 0.8841911764705882\n",
      "epoch 167 total_train_acc: 0.8893093661305582 loss: 1.592736929655075 test_loss: 0.18716761469841003 test_acc: 0.8860294117647058\n",
      "epoch 168 total_train_acc: 0.890728476821192 loss: 1.5699378848075867 test_loss: 0.1625967025756836 test_acc: 0.8817401960784313\n",
      "epoch 169 total_train_acc: 0.8900189214758751 loss: 1.6223273575305939 test_loss: 0.12336266785860062 test_acc: 0.8811274509803921\n",
      "epoch 170 total_train_acc: 0.8895458845789972 loss: 1.572321742773056 test_loss: 0.31036290526390076 test_acc: 0.8805147058823529\n",
      "epoch 171 total_train_acc: 0.8897824030274362 loss: 1.5918081104755402 test_loss: 0.18179671466350555 test_acc: 0.883578431372549\n",
      "epoch 172 total_train_acc: 0.8904919583727531 loss: 1.637521654367447 test_loss: 0.25606265664100647 test_acc: 0.8805147058823529\n",
      "epoch 173 total_train_acc: 0.8930936613055819 loss: 1.5362758338451385 test_loss: 0.4175802767276764 test_acc: 0.8860294117647058\n",
      "epoch 174 total_train_acc: 0.8928571428571429 loss: 1.5422143042087555 test_loss: 0.4459061920642853 test_acc: 0.8860294117647058\n",
      "epoch 175 total_train_acc: 0.8902554399243141 loss: 1.5680764615535736 test_loss: 0.3845856785774231 test_acc: 0.8860294117647058\n",
      "epoch 176 total_train_acc: 0.8923841059602649 loss: 1.6466595232486725 test_loss: 0.4599634110927582 test_acc: 0.8860294117647058\n",
      "epoch 177 total_train_acc: 0.89120151371807 loss: 1.6050842702388763 test_loss: 0.2392321527004242 test_acc: 0.8829656862745098\n",
      "epoch 178 total_train_acc: 0.8947492904446547 loss: 1.5478704571723938 test_loss: 0.35564714670181274 test_acc: 0.8848039215686274\n",
      "epoch 179 total_train_acc: 0.8990066225165563 loss: 1.5424274802207947 test_loss: 0.45456987619400024 test_acc: 0.8860294117647058\n",
      "epoch 180 total_train_acc: 0.8959318826868495 loss: 1.50050488114357 test_loss: 0.2778630554676056 test_acc: 0.8878676470588235\n",
      "epoch 181 total_train_acc: 0.8940397350993378 loss: 1.5183892846107483 test_loss: 0.4534052014350891 test_acc: 0.8878676470588235\n",
      "epoch 182 total_train_acc: 0.8942762535477767 loss: 1.5415664315223694 test_loss: 0.8843757510185242 test_acc: 0.8915441176470589\n",
      "epoch 183 total_train_acc: 0.8990066225165563 loss: 1.5726907551288605 test_loss: 0.4232432246208191 test_acc: 0.8939950980392157\n",
      "epoch 184 total_train_acc: 0.9001892147587511 loss: 1.4695010483264923 test_loss: 0.2613270878791809 test_acc: 0.8811274509803921\n",
      "epoch 185 total_train_acc: 0.8904919583727531 loss: 1.569047451019287 test_loss: 0.27816489338874817 test_acc: 0.8817401960784313\n",
      "epoch 186 total_train_acc: 0.89120151371807 loss: 1.5856925547122955 test_loss: 0.1234879270195961 test_acc: 0.8897058823529411\n",
      "epoch 187 total_train_acc: 0.8966414380321666 loss: 1.49844029545784 test_loss: 0.15913338959217072 test_acc: 0.8860294117647058\n",
      "epoch 188 total_train_acc: 0.8987701040681173 loss: 1.5325049757957458 test_loss: 0.14486750960350037 test_acc: 0.8909313725490197\n",
      "epoch 189 total_train_acc: 0.8968779564806055 loss: 1.5267425775527954 test_loss: 0.48969343304634094 test_acc: 0.8860294117647058\n",
      "epoch 190 total_train_acc: 0.8980605487228004 loss: 1.4394315034151077 test_loss: 0.3374733030796051 test_acc: 0.8897058823529411\n",
      "epoch 191 total_train_acc: 0.8980605487228004 loss: 1.4684051275253296 test_loss: 0.15417426824569702 test_acc: 0.8903186274509803\n",
      "epoch 192 total_train_acc: 0.8982970671712394 loss: 1.5148699283599854 test_loss: 0.23829081654548645 test_acc: 0.8866421568627451\n",
      "epoch 193 total_train_acc: 0.8966414380321666 loss: 1.5500272512435913 test_loss: 0.4601275622844696 test_acc: 0.8897058823529411\n",
      "epoch 194 total_train_acc: 0.8973509933774835 loss: 1.5015969276428223 test_loss: 0.43926769495010376 test_acc: 0.8909313725490197\n",
      "epoch 195 total_train_acc: 0.902554399243141 loss: 1.4759458303451538 test_loss: 0.28709837794303894 test_acc: 0.8915441176470589\n",
      "epoch 196 total_train_acc: 0.8971144749290445 loss: 1.5041331350803375 test_loss: 0.5888557434082031 test_acc: 0.8854166666666666\n",
      "epoch 197 total_train_acc: 0.8990066225165563 loss: 1.4178772568702698 test_loss: 0.3068106174468994 test_acc: 0.8860294117647058\n",
      "epoch 198 total_train_acc: 0.8968779564806055 loss: 1.4963648915290833 test_loss: 0.16417323052883148 test_acc: 0.8921568627450981\n",
      "epoch 199 total_train_acc: 0.8985335856196783 loss: 1.4207150042057037 test_loss: 0.26821914315223694 test_acc: 0.8890931372549019\n",
      "epoch 200 total_train_acc: 0.8985335856196783 loss: 1.4760567396879196 test_loss: 0.35846856236457825 test_acc: 0.8946078431372549\n",
      "epoch 201 total_train_acc: 0.9006622516556292 loss: 1.3443807661533356 test_loss: 0.283698707818985 test_acc: 0.8939950980392157\n",
      "epoch 202 total_train_acc: 0.9013718070009461 loss: 1.4022843688726425 test_loss: 0.12474024295806885 test_acc: 0.8964460784313726\n",
      "epoch 203 total_train_acc: 0.9004257332071902 loss: 1.438012182712555 test_loss: 0.12280683219432831 test_acc: 0.8958333333333334\n",
      "epoch 204 total_train_acc: 0.9032639545884579 loss: 1.3893227279186249 test_loss: 0.2445380538702011 test_acc: 0.8933823529411765\n",
      "epoch 205 total_train_acc: 0.9032639545884579 loss: 1.4136907756328583 test_loss: 0.2975310683250427 test_acc: 0.8909313725490197\n",
      "epoch 206 total_train_acc: 0.9001892147587511 loss: 1.4401324093341827 test_loss: 0.28863558173179626 test_acc: 0.8915441176470589\n",
      "epoch 207 total_train_acc: 0.8961684011352885 loss: 1.4096210896968842 test_loss: 0.17102700471878052 test_acc: 0.8933823529411765\n",
      "epoch 208 total_train_acc: 0.9004257332071902 loss: 1.4210503101348877 test_loss: 0.23203247785568237 test_acc: 0.8903186274509803\n",
      "epoch 209 total_train_acc: 0.8997161778618732 loss: 1.43472021818161 test_loss: 0.36296379566192627 test_acc: 0.8915441176470589\n",
      "epoch 210 total_train_acc: 0.8999526963103122 loss: 1.441054791212082 test_loss: 0.22901196777820587 test_acc: 0.8958333333333334\n",
      "epoch 211 total_train_acc: 0.9039735099337748 loss: 1.3916128277778625 test_loss: 0.4188920855522156 test_acc: 0.8927696078431373\n",
      "epoch 212 total_train_acc: 0.902081362346263 loss: 1.387777417898178 test_loss: 0.2682107388973236 test_acc: 0.8903186274509803\n",
      "epoch 213 total_train_acc: 0.8964049195837275 loss: 1.5207561552524567 test_loss: 0.3571797013282776 test_acc: 0.8915441176470589\n",
      "epoch 214 total_train_acc: 0.9004257332071902 loss: 1.3635031878948212 test_loss: 0.2597721815109253 test_acc: 0.8921568627450981\n",
      "epoch 215 total_train_acc: 0.9032639545884579 loss: 1.4016469717025757 test_loss: 0.36315467953681946 test_acc: 0.8946078431372549\n",
      "epoch 216 total_train_acc: 0.9046830652790918 loss: 1.3692533671855927 test_loss: 0.30778348445892334 test_acc: 0.8958333333333334\n",
      "epoch 217 total_train_acc: 0.9058656575212867 loss: 1.3096171468496323 test_loss: 0.25791946053504944 test_acc: 0.9007352941176471\n",
      "epoch 218 total_train_acc: 0.9061021759697256 loss: 1.3409923017024994 test_loss: 0.289320707321167 test_acc: 0.8946078431372549\n",
      "epoch 219 total_train_acc: 0.9049195837275308 loss: 1.4145824909210205 test_loss: 0.22514164447784424 test_acc: 0.8995098039215687\n",
      "epoch 220 total_train_acc: 0.9061021759697256 loss: 1.3836509585380554 test_loss: 0.33919501304626465 test_acc: 0.8970588235294118\n",
      "epoch 221 total_train_acc: 0.9061021759697256 loss: 1.3432756662368774 test_loss: 0.31468454003334045 test_acc: 0.8927696078431373\n",
      "epoch 222 total_train_acc: 0.9061021759697256 loss: 1.3415171802043915 test_loss: 0.12088383734226227 test_acc: 0.8964460784313726\n",
      "epoch 223 total_train_acc: 0.9091769157994324 loss: 1.333805650472641 test_loss: 0.15105585753917694 test_acc: 0.8970588235294118\n",
      "epoch 224 total_train_acc: 0.9094134342478714 loss: 1.3781136870384216 test_loss: 0.19654053449630737 test_acc: 0.8921568627450981\n",
      "epoch 225 total_train_acc: 0.9058656575212867 loss: 1.2890961468219757 test_loss: 0.20360326766967773 test_acc: 0.8958333333333334\n",
      "epoch 226 total_train_acc: 0.9077578051087984 loss: 1.3289978355169296 test_loss: 0.08853968977928162 test_acc: 0.8952205882352942\n",
      "epoch 227 total_train_acc: 0.9072847682119205 loss: 1.3779411613941193 test_loss: 0.16034477949142456 test_acc: 0.8958333333333334\n",
      "epoch 228 total_train_acc: 0.9056291390728477 loss: 1.3621537685394287 test_loss: 0.4434342086315155 test_acc: 0.8995098039215687\n",
      "epoch 229 total_train_acc: 0.9027909176915799 loss: 1.3631604313850403 test_loss: 0.5341428518295288 test_acc: 0.8982843137254902\n",
      "epoch 230 total_train_acc: 0.9103595080416272 loss: 1.3161965012550354 test_loss: 0.16588194668293 test_acc: 0.8982843137254902\n",
      "epoch 231 total_train_acc: 0.9089403973509934 loss: 1.3137740790843964 test_loss: 0.27674567699432373 test_acc: 0.897671568627451\n",
      "epoch 232 total_train_acc: 0.9087038789025544 loss: 1.2923907935619354 test_loss: 0.1376906931400299 test_acc: 0.8964460784313726\n",
      "epoch 233 total_train_acc: 0.913434247871334 loss: 1.3427179753780365 test_loss: 0.43709495663642883 test_acc: 0.8933823529411765\n",
      "epoch 234 total_train_acc: 0.912961210974456 loss: 1.296924963593483 test_loss: 0.23440144956111908 test_acc: 0.8988970588235294\n",
      "epoch 235 total_train_acc: 0.9094134342478714 loss: 1.3211467266082764 test_loss: 0.2875826060771942 test_acc: 0.9001225490196079\n",
      "epoch 236 total_train_acc: 0.902554399243141 loss: 1.384471446275711 test_loss: 0.2138070911169052 test_acc: 0.8952205882352942\n",
      "epoch 237 total_train_acc: 0.9072847682119205 loss: 1.3117283433675766 test_loss: 0.27455276250839233 test_acc: 0.9025735294117647\n",
      "epoch 238 total_train_acc: 0.9101229895931883 loss: 1.3034655451774597 test_loss: 0.15819372236728668 test_acc: 0.8982843137254902\n",
      "epoch 239 total_train_acc: 0.9120151371807 loss: 1.3086790442466736 test_loss: 0.20135699212551117 test_acc: 0.8988970588235294\n",
      "epoch 240 total_train_acc: 0.9084673604541155 loss: 1.294106900691986 test_loss: 0.4315330982208252 test_acc: 0.9025735294117647\n",
      "epoch 241 total_train_acc: 0.9075212866603595 loss: 1.2010987848043442 test_loss: 0.2848052382469177 test_acc: 0.8970588235294118\n",
      "epoch 242 total_train_acc: 0.9094134342478714 loss: 1.2467385232448578 test_loss: 0.4040418565273285 test_acc: 0.9013480392156863\n",
      "epoch 243 total_train_acc: 0.9105960264900662 loss: 1.28945991396904 test_loss: 0.172672301530838 test_acc: 0.9044117647058824\n",
      "epoch 244 total_train_acc: 0.9091769157994324 loss: 1.2741766721010208 test_loss: 0.1712925285100937 test_acc: 0.9007352941176471\n",
      "epoch 245 total_train_acc: 0.9063386944181646 loss: 1.2925721108913422 test_loss: 0.1720098853111267 test_acc: 0.8995098039215687\n",
      "epoch 246 total_train_acc: 0.9103595080416272 loss: 1.3009880781173706 test_loss: 0.15251049399375916 test_acc: 0.9031862745098039\n",
      "epoch 247 total_train_acc: 0.9124881740775781 loss: 1.2679712921380997 test_loss: 0.33672574162483215 test_acc: 0.9013480392156863\n",
      "epoch 248 total_train_acc: 0.913434247871334 loss: 1.2172345668077469 test_loss: 0.19693677127361298 test_acc: 0.9013480392156863\n",
      "epoch 249 total_train_acc: 0.912961210974456 loss: 1.2016514241695404 test_loss: 0.30203977227211 test_acc: 0.8995098039215687\n",
      "epoch 250 total_train_acc: 0.9160359508041628 loss: 1.2644600868225098 test_loss: 0.22073954343795776 test_acc: 0.9001225490196079\n",
      "epoch 251 total_train_acc: 0.913670766319773 loss: 1.239658921957016 test_loss: 0.4635663628578186 test_acc: 0.9007352941176471\n",
      "epoch 252 total_train_acc: 0.9167455061494797 loss: 1.2179576754570007 test_loss: 0.4114410877227783 test_acc: 0.9031862745098039\n",
      "epoch 253 total_train_acc: 0.9096499526963103 loss: 1.2540926784276962 test_loss: 0.2534751892089844 test_acc: 0.9025735294117647\n",
      "epoch 254 total_train_acc: 0.9070482497634815 loss: 1.267393246293068 test_loss: 0.07660303264856339 test_acc: 0.8995098039215687\n",
      "epoch 255 total_train_acc: 0.9103595080416272 loss: 1.2464747875928879 test_loss: 0.25557366013526917 test_acc: 0.9025735294117647\n",
      "epoch 256 total_train_acc: 0.912961210974456 loss: 1.204367995262146 test_loss: 0.3355183005332947 test_acc: 0.8970588235294118\n",
      "epoch 257 total_train_acc: 0.9110690633869442 loss: 1.2927969694137573 test_loss: 0.17713141441345215 test_acc: 0.9050245098039216\n",
      "epoch 258 total_train_acc: 0.9181646168401135 loss: 1.2685070484876633 test_loss: 0.276242196559906 test_acc: 0.9037990196078431\n",
      "epoch 259 total_train_acc: 0.9155629139072847 loss: 1.215781643986702 test_loss: 0.46020999550819397 test_acc: 0.9007352941176471\n",
      "epoch 260 total_train_acc: 0.9148533585619678 loss: 1.2169157564640045 test_loss: 0.2551233768463135 test_acc: 0.9044117647058824\n",
      "epoch 261 total_train_acc: 0.913197729422895 loss: 1.1945375800132751 test_loss: 0.15315662324428558 test_acc: 0.9013480392156863\n",
      "epoch 262 total_train_acc: 0.9169820245979187 loss: 1.2104512304067612 test_loss: 0.23447415232658386 test_acc: 0.90625\n",
      "epoch 263 total_train_acc: 0.9174550614947966 loss: 1.19554103910923 test_loss: 0.2525402009487152 test_acc: 0.9025735294117647\n",
      "epoch 264 total_train_acc: 0.9169820245979187 loss: 1.1800947040319443 test_loss: 0.2672283351421356 test_acc: 0.90625\n",
      "epoch 265 total_train_acc: 0.9148533585619678 loss: 1.2551510781049728 test_loss: 0.2699868977069855 test_acc: 0.9019607843137255\n",
      "epoch 266 total_train_acc: 0.9148533585619678 loss: 1.1923978924751282 test_loss: 0.2652687728404999 test_acc: 0.9087009803921569\n",
      "epoch 267 total_train_acc: 0.9143803216650899 loss: 1.2826924473047256 test_loss: 0.20687520503997803 test_acc: 0.9031862745098039\n",
      "epoch 268 total_train_acc: 0.913670766319773 loss: 1.2301276326179504 test_loss: 0.26013365387916565 test_acc: 0.9037990196078431\n",
      "epoch 269 total_train_acc: 0.9174550614947966 loss: 1.1191683411598206 test_loss: 0.1572510302066803 test_acc: 0.9031862745098039\n",
      "epoch 270 total_train_acc: 0.9139072847682119 loss: 1.2273798882961273 test_loss: 0.18559235334396362 test_acc: 0.9087009803921569\n",
      "epoch 271 total_train_acc: 0.9172185430463576 loss: 1.2263442426919937 test_loss: 0.4411226212978363 test_acc: 0.9068627450980392\n",
      "epoch 272 total_train_acc: 0.9143803216650899 loss: 1.2083474844694138 test_loss: 0.2632312774658203 test_acc: 0.9117647058823529\n",
      "epoch 273 total_train_acc: 0.9172185430463576 loss: 1.2119351625442505 test_loss: 0.13321280479431152 test_acc: 0.90625\n",
      "epoch 274 total_train_acc: 0.9169820245979187 loss: 1.2047362625598907 test_loss: 0.478821724653244 test_acc: 0.9044117647058824\n",
      "epoch 275 total_train_acc: 0.913197729422895 loss: 1.251815915107727 test_loss: 0.38771528005599976 test_acc: 0.9001225490196079\n",
      "epoch 276 total_train_acc: 0.9172185430463576 loss: 1.1948953568935394 test_loss: 0.3760560154914856 test_acc: 0.9044117647058824\n",
      "epoch 277 total_train_acc: 0.9228949858088931 loss: 1.2243372350931168 test_loss: 0.2025294452905655 test_acc: 0.9074754901960784\n",
      "epoch 278 total_train_acc: 0.9198202459791863 loss: 1.1442119628190994 test_loss: 0.26635921001434326 test_acc: 0.9050245098039216\n",
      "epoch 279 total_train_acc: 0.913434247871334 loss: 1.1723906993865967 test_loss: 0.3028058707714081 test_acc: 0.9123774509803921\n",
      "epoch 280 total_train_acc: 0.9210028382213813 loss: 1.1301319301128387 test_loss: 0.5596926808357239 test_acc: 0.9044117647058824\n",
      "epoch 281 total_train_acc: 0.9195837275307474 loss: 1.1309007853269577 test_loss: 0.27168306708335876 test_acc: 0.9050245098039216\n",
      "epoch 282 total_train_acc: 0.9202932828760644 loss: 1.1219360828399658 test_loss: 0.2283954620361328 test_acc: 0.9037990196078431\n",
      "epoch 283 total_train_acc: 0.9207663197729423 loss: 1.18686081469059 test_loss: 0.5692270398139954 test_acc: 0.9087009803921569\n",
      "epoch 284 total_train_acc: 0.9214758751182592 loss: 1.1299650073051453 test_loss: 0.23332220315933228 test_acc: 0.9105392156862745\n",
      "epoch 285 total_train_acc: 0.9146168401135288 loss: 1.1439143419265747 test_loss: 0.25149285793304443 test_acc: 0.9099264705882353\n",
      "epoch 286 total_train_acc: 0.9224219489120151 loss: 1.1598190069198608 test_loss: 0.3628098666667938 test_acc: 0.9087009803921569\n",
      "epoch 287 total_train_acc: 0.9191106906338694 loss: 1.1587203443050385 test_loss: 0.3082539439201355 test_acc: 0.9044117647058824\n",
      "epoch 288 total_train_acc: 0.9224219489120151 loss: 1.0944428443908691 test_loss: 0.33678576350212097 test_acc: 0.9080882352941176\n",
      "epoch 289 total_train_acc: 0.9219489120151372 loss: 1.0754992961883545 test_loss: 0.18692150712013245 test_acc: 0.9050245098039216\n",
      "epoch 290 total_train_acc: 0.9221854304635762 loss: 1.1162459701299667 test_loss: 0.23225562274456024 test_acc: 0.9074754901960784\n",
      "epoch 291 total_train_acc: 0.9259697256385998 loss: 1.1089283674955368 test_loss: 0.4318446218967438 test_acc: 0.90625\n",
      "epoch 292 total_train_acc: 0.9212393566698203 loss: 1.156541958451271 test_loss: 0.33243024349212646 test_acc: 0.9093137254901961\n",
      "epoch 293 total_train_acc: 0.9153263954588458 loss: 1.1653092801570892 test_loss: 0.3506811261177063 test_acc: 0.9123774509803921\n",
      "epoch 294 total_train_acc: 0.924314096499527 loss: 1.121410921216011 test_loss: 0.1797247976064682 test_acc: 0.9117647058823529\n",
      "epoch 295 total_train_acc: 0.9210028382213813 loss: 1.2177545577287674 test_loss: 0.2521846294403076 test_acc: 0.9068627450980392\n",
      "epoch 296 total_train_acc: 0.9238410596026491 loss: 1.0792742520570755 test_loss: 0.452411949634552 test_acc: 0.9117647058823529\n",
      "epoch 297 total_train_acc: 0.9278618732261117 loss: 1.0396738648414612 test_loss: 0.15304899215698242 test_acc: 0.9037990196078431\n",
      "epoch 298 total_train_acc: 0.924314096499527 loss: 1.05320443212986 test_loss: 0.08690611273050308 test_acc: 0.9129901960784313\n",
      "epoch 299 total_train_acc: 0.9188741721854304 loss: 1.135556936264038 test_loss: 0.3229580521583557 test_acc: 0.9123774509803921\n",
      "epoch 300 total_train_acc: 0.9198202459791863 loss: 1.1462835222482681 test_loss: 0.25115665793418884 test_acc: 0.9166666666666666\n",
      "epoch 301 total_train_acc: 0.9290444654683065 loss: 1.0303196460008621 test_loss: 0.18275853991508484 test_acc: 0.914828431372549\n",
      "epoch 302 total_train_acc: 0.9228949858088931 loss: 1.10116907954216 test_loss: 0.1985464096069336 test_acc: 0.9074754901960784\n",
      "epoch 303 total_train_acc: 0.923131504257332 loss: 1.0413765460252762 test_loss: 0.34622663259506226 test_acc: 0.9099264705882353\n",
      "epoch 304 total_train_acc: 0.92360454115421 loss: 1.0870301574468613 test_loss: 0.3202366828918457 test_acc: 0.9111519607843137\n",
      "epoch 305 total_train_acc: 0.9212393566698203 loss: 1.1285441666841507 test_loss: 0.23594070971012115 test_acc: 0.9129901960784313\n",
      "epoch 306 total_train_acc: 0.923368022705771 loss: 1.0532086044549942 test_loss: 0.16813208162784576 test_acc: 0.9074754901960784\n",
      "epoch 307 total_train_acc: 0.924550614947966 loss: 1.0866335481405258 test_loss: 0.19378112256526947 test_acc: 0.9172794117647058\n",
      "epoch 308 total_train_acc: 0.9250236518448439 loss: 1.0233090966939926 test_loss: 0.221210777759552 test_acc: 0.9050245098039216\n",
      "epoch 309 total_train_acc: 0.9228949858088931 loss: 1.0235086232423782 test_loss: 0.2603689432144165 test_acc: 0.9123774509803921\n",
      "epoch 310 total_train_acc: 0.9224219489120151 loss: 1.1012348681688309 test_loss: 0.10667772591114044 test_acc: 0.9111519607843137\n",
      "epoch 311 total_train_acc: 0.924787133396405 loss: 1.0139876157045364 test_loss: 0.18115337193012238 test_acc: 0.9142156862745098\n",
      "epoch 312 total_train_acc: 0.9259697256385998 loss: 1.0716548413038254 test_loss: 0.21574118733406067 test_acc: 0.9142156862745098\n",
      "epoch 313 total_train_acc: 0.9273888363292336 loss: 1.0237334966659546 test_loss: 0.23881809413433075 test_acc: 0.9093137254901961\n",
      "epoch 314 total_train_acc: 0.9283349101229896 loss: 1.0178761333227158 test_loss: 0.3388606011867523 test_acc: 0.9154411764705882\n",
      "epoch 315 total_train_acc: 0.924314096499527 loss: 1.0509801357984543 test_loss: 0.14690406620502472 test_acc: 0.9105392156862745\n",
      "epoch 316 total_train_acc: 0.9283349101229896 loss: 1.054923489689827 test_loss: 0.21381798386573792 test_acc: 0.9142156862745098\n",
      "epoch 317 total_train_acc: 0.9290444654683065 loss: 1.037169486284256 test_loss: 0.36925384402275085 test_acc: 0.9160539215686274\n",
      "epoch 318 total_train_acc: 0.9266792809839167 loss: 1.0286169797182083 test_loss: 0.27227723598480225 test_acc: 0.9154411764705882\n",
      "epoch 319 total_train_acc: 0.9283349101229896 loss: 1.068600133061409 test_loss: 0.12352243810892105 test_acc: 0.914828431372549\n",
      "epoch 320 total_train_acc: 0.9271523178807947 loss: 1.035087689757347 test_loss: 0.21538367867469788 test_acc: 0.914828431372549\n",
      "epoch 321 total_train_acc: 0.9285714285714286 loss: 1.0110865086317062 test_loss: 0.18467086553573608 test_acc: 0.9142156862745098\n",
      "epoch 322 total_train_acc: 0.9290444654683065 loss: 0.9951469749212265 test_loss: 0.1898675411939621 test_acc: 0.9172794117647058\n",
      "epoch 323 total_train_acc: 0.924550614947966 loss: 1.0114710479974747 test_loss: 0.4252772927284241 test_acc: 0.9136029411764706\n",
      "epoch 324 total_train_acc: 0.9302270577105014 loss: 0.9854773730039597 test_loss: 0.6287652850151062 test_acc: 0.9166666666666666\n",
      "epoch 325 total_train_acc: 0.9273888363292336 loss: 1.0675591230392456 test_loss: 0.19058488309383392 test_acc: 0.9074754901960784\n",
      "epoch 326 total_train_acc: 0.9205298013245033 loss: 1.098104402422905 test_loss: 0.18534159660339355 test_acc: 0.9136029411764706\n",
      "epoch 327 total_train_acc: 0.9264427625354777 loss: 1.0395821779966354 test_loss: 0.21610718965530396 test_acc: 0.9129901960784313\n",
      "epoch 328 total_train_acc: 0.9266792809839167 loss: 1.0581463277339935 test_loss: 0.13309316337108612 test_acc: 0.9142156862745098\n",
      "epoch 329 total_train_acc: 0.9278618732261117 loss: 0.9910025149583817 test_loss: 0.16549476981163025 test_acc: 0.9178921568627451\n",
      "epoch 330 total_train_acc: 0.9290444654683065 loss: 1.0207512378692627 test_loss: 0.17252717912197113 test_acc: 0.9080882352941176\n",
      "epoch 331 total_train_acc: 0.9264427625354777 loss: 0.9977780133485794 test_loss: 0.12395134568214417 test_acc: 0.9160539215686274\n",
      "epoch 332 total_train_acc: 0.9254966887417219 loss: 0.9923490881919861 test_loss: 0.0988360196352005 test_acc: 0.9185049019607843\n",
      "epoch 333 total_train_acc: 0.9302270577105014 loss: 1.0125530660152435 test_loss: 0.16580380499362946 test_acc: 0.9142156862745098\n",
      "epoch 334 total_train_acc: 0.9276253547776726 loss: 1.064038246870041 test_loss: 0.09423261880874634 test_acc: 0.914828431372549\n",
      "epoch 335 total_train_acc: 0.9297540208136235 loss: 1.0032179206609726 test_loss: 0.1981322169303894 test_acc: 0.9172794117647058\n",
      "epoch 336 total_train_acc: 0.9276253547776726 loss: 1.0172356367111206 test_loss: 0.12674568593502045 test_acc: 0.9203431372549019\n",
      "epoch 337 total_train_acc: 0.9307000946073793 loss: 0.9930204153060913 test_loss: 0.4094797670841217 test_acc: 0.9166666666666666\n",
      "epoch 338 total_train_acc: 0.9285714285714286 loss: 1.0061813741922379 test_loss: 0.31206369400024414 test_acc: 0.9111519607843137\n",
      "epoch 339 total_train_acc: 0.9304635761589404 loss: 1.0066716223955154 test_loss: 0.19579049944877625 test_acc: 0.9197303921568627\n",
      "epoch 340 total_train_acc: 0.9295175023651845 loss: 0.9816116541624069 test_loss: 0.07995343953371048 test_acc: 0.9166666666666666\n",
      "epoch 341 total_train_acc: 0.9290444654683065 loss: 0.9689604789018631 test_loss: 0.5004577040672302 test_acc: 0.9185049019607843\n",
      "epoch 342 total_train_acc: 0.9297540208136235 loss: 1.0101333856582642 test_loss: 0.0583227165043354 test_acc: 0.9178921568627451\n",
      "epoch 343 total_train_acc: 0.9288079470198676 loss: 1.0065500289201736 test_loss: 0.281412273645401 test_acc: 0.9178921568627451\n",
      "epoch 344 total_train_acc: 0.9295175023651845 loss: 1.0050927847623825 test_loss: 0.3329389989376068 test_acc: 0.9160539215686274\n",
      "epoch 345 total_train_acc: 0.9292809839167455 loss: 1.047999694943428 test_loss: 0.048615962266922 test_acc: 0.9178921568627451\n",
      "epoch 346 total_train_acc: 0.9278618732261117 loss: 1.000018373131752 test_loss: 0.059534817934036255 test_acc: 0.9191176470588235\n",
      "epoch 347 total_train_acc: 0.9266792809839167 loss: 1.015567034482956 test_loss: 0.13035720586776733 test_acc: 0.9166666666666666\n",
      "epoch 348 total_train_acc: 0.9318826868495743 loss: 1.000201478600502 test_loss: 0.15280044078826904 test_acc: 0.9129901960784313\n",
      "epoch 349 total_train_acc: 0.9264427625354777 loss: 1.004360556602478 test_loss: 0.35287368297576904 test_acc: 0.9178921568627451\n",
      "epoch 350 total_train_acc: 0.9304635761589404 loss: 1.0005286931991577 test_loss: 0.3514972925186157 test_acc: 0.9160539215686274\n",
      "epoch 351 total_train_acc: 0.9321192052980133 loss: 0.9984066784381866 test_loss: 0.17974431812763214 test_acc: 0.914828431372549\n",
      "epoch 352 total_train_acc: 0.9316461684011353 loss: 1.053490787744522 test_loss: 0.3333866000175476 test_acc: 0.9221813725490197\n",
      "epoch 353 total_train_acc: 0.9278618732261117 loss: 1.0002716779708862 test_loss: 0.44645723700523376 test_acc: 0.9142156862745098\n",
      "epoch 354 total_train_acc: 0.9288079470198676 loss: 1.030995711684227 test_loss: 0.19338330626487732 test_acc: 0.9154411764705882\n",
      "epoch 355 total_train_acc: 0.9328287606433302 loss: 1.030391201376915 test_loss: 0.3060987889766693 test_acc: 0.9191176470588235\n",
      "epoch 356 total_train_acc: 0.9278618732261117 loss: 0.9604254066944122 test_loss: 0.35001763701438904 test_acc: 0.9227941176470589\n",
      "epoch 357 total_train_acc: 0.9288079470198676 loss: 1.0061152279376984 test_loss: 0.1329227089881897 test_acc: 0.9178921568627451\n",
      "epoch 358 total_train_acc: 0.9311731315042573 loss: 1.032586082816124 test_loss: 0.1536003053188324 test_acc: 0.9178921568627451\n",
      "epoch 359 total_train_acc: 0.9299905392620624 loss: 0.9891188591718674 test_loss: 0.1253325343132019 test_acc: 0.9203431372549019\n",
      "epoch 360 total_train_acc: 0.9328287606433302 loss: 0.9606672078371048 test_loss: 0.30263063311576843 test_acc: 0.9178921568627451\n",
      "epoch 361 total_train_acc: 0.9280983916745507 loss: 1.0118136554956436 test_loss: 0.1362505555152893 test_acc: 0.9123774509803921\n",
      "epoch 362 total_train_acc: 0.9307000946073793 loss: 0.9466139674186707 test_loss: 0.14042581617832184 test_acc: 0.9172794117647058\n",
      "epoch 363 total_train_acc: 0.934720908230842 loss: 0.9607600420713425 test_loss: 0.13273262977600098 test_acc: 0.9136029411764706\n",
      "epoch 364 total_train_acc: 0.9283349101229896 loss: 1.0101861655712128 test_loss: 0.028916804119944572 test_acc: 0.9209558823529411\n",
      "epoch 365 total_train_acc: 0.9311731315042573 loss: 0.9719914197921753 test_loss: 0.28313779830932617 test_acc: 0.9178921568627451\n",
      "epoch 366 total_train_acc: 0.9283349101229896 loss: 1.000674769282341 test_loss: 0.15387026965618134 test_acc: 0.9136029411764706\n",
      "epoch 367 total_train_acc: 0.9302270577105014 loss: 1.0470315515995026 test_loss: 0.15013805031776428 test_acc: 0.9234068627450981\n",
      "epoch 368 total_train_acc: 0.9325922421948912 loss: 0.8874129354953766 test_loss: 0.46368277072906494 test_acc: 0.9221813725490197\n",
      "epoch 369 total_train_acc: 0.9311731315042573 loss: 0.9934298098087311 test_loss: 0.2591824233531952 test_acc: 0.9172794117647058\n",
      "epoch 370 total_train_acc: 0.9335383159886471 loss: 0.9679647833108902 test_loss: 0.12813040614128113 test_acc: 0.9209558823529411\n",
      "epoch 371 total_train_acc: 0.9368495742667928 loss: 0.9628870636224747 test_loss: 0.4524829685688019 test_acc: 0.9246323529411765\n",
      "epoch 372 total_train_acc: 0.9385052034058656 loss: 0.862463191151619 test_loss: 0.25175946950912476 test_acc: 0.9191176470588235\n",
      "epoch 373 total_train_acc: 0.9314096499526963 loss: 0.9908850938081741 test_loss: 0.07793343812227249 test_acc: 0.9203431372549019\n",
      "epoch 374 total_train_acc: 0.9330652790917692 loss: 0.9683603793382645 test_loss: 0.0723205953836441 test_acc: 0.9166666666666666\n",
      "epoch 375 total_train_acc: 0.9333017975402081 loss: 0.9286720454692841 test_loss: 0.18932226300239563 test_acc: 0.9240196078431373\n",
      "epoch 376 total_train_acc: 0.935666982024598 loss: 0.932209774851799 test_loss: 0.18449968099594116 test_acc: 0.9221813725490197\n",
      "epoch 377 total_train_acc: 0.9325922421948912 loss: 0.905581146478653 test_loss: 0.33760908246040344 test_acc: 0.9129901960784313\n",
      "epoch 378 total_train_acc: 0.9318826868495743 loss: 0.953767254948616 test_loss: 0.597101628780365 test_acc: 0.9185049019607843\n",
      "epoch 379 total_train_acc: 0.9333017975402081 loss: 0.8980822712182999 test_loss: 0.225584015250206 test_acc: 0.9142156862745098\n",
      "epoch 380 total_train_acc: 0.934247871333964 loss: 0.9258832484483719 test_loss: 0.10357493162155151 test_acc: 0.9191176470588235\n",
      "epoch 381 total_train_acc: 0.9328287606433302 loss: 0.9024269282817841 test_loss: 0.19343462586402893 test_acc: 0.9172794117647058\n",
      "epoch 382 total_train_acc: 0.9382686849574267 loss: 0.9013006687164307 test_loss: 0.3147423565387726 test_acc: 0.9215686274509803\n",
      "epoch 383 total_train_acc: 0.9361400189214759 loss: 0.9178179353475571 test_loss: 0.33265653252601624 test_acc: 0.9252450980392157\n",
      "epoch 384 total_train_acc: 0.9380321665089877 loss: 0.8524945676326752 test_loss: 0.16457661986351013 test_acc: 0.9178921568627451\n",
      "epoch 385 total_train_acc: 0.9389782403027436 loss: 0.9119232147932053 test_loss: 0.4436010718345642 test_acc: 0.9264705882352942\n",
      "epoch 386 total_train_acc: 0.9368495742667928 loss: 0.9204502552747726 test_loss: 0.13755865395069122 test_acc: 0.9234068627450981\n",
      "epoch 387 total_train_acc: 0.9363765373699149 loss: 0.9754878580570221 test_loss: 0.18199144303798676 test_acc: 0.9191176470588235\n",
      "epoch 388 total_train_acc: 0.9382686849574267 loss: 0.8824779987335205 test_loss: 0.09645725041627884 test_acc: 0.9283088235294118\n",
      "epoch 389 total_train_acc: 0.9403973509933775 loss: 0.8690597265958786 test_loss: 0.07341113686561584 test_acc: 0.9227941176470589\n",
      "epoch 390 total_train_acc: 0.9351939451277199 loss: 0.9093100875616074 test_loss: 0.171035498380661 test_acc: 0.9185049019607843\n",
      "epoch 391 total_train_acc: 0.9366130558183539 loss: 0.913301482796669 test_loss: 0.18785040080547333 test_acc: 0.9191176470588235\n",
      "epoch 392 total_train_acc: 0.9389782403027436 loss: 0.9107726514339447 test_loss: 0.2960624098777771 test_acc: 0.9240196078431373\n",
      "epoch 393 total_train_acc: 0.9382686849574267 loss: 0.835678443312645 test_loss: 0.25055912137031555 test_acc: 0.9215686274509803\n",
      "epoch 394 total_train_acc: 0.934247871333964 loss: 0.9340289086103439 test_loss: 0.16171403229236603 test_acc: 0.9252450980392157\n",
      "epoch 395 total_train_acc: 0.9406338694418165 loss: 0.9031783938407898 test_loss: 0.3088940978050232 test_acc: 0.9191176470588235\n",
      "epoch 396 total_train_acc: 0.9375591296121097 loss: 0.8790897130966187 test_loss: 0.24338987469673157 test_acc: 0.9252450980392157\n",
      "epoch 397 total_train_acc: 0.9354304635761589 loss: 0.9207335859537125 test_loss: 0.28118467330932617 test_acc: 0.9246323529411765\n",
      "epoch 398 total_train_acc: 0.935666982024598 loss: 0.9192252308130264 test_loss: 0.1526305079460144 test_acc: 0.9215686274509803\n",
      "epoch 399 total_train_acc: 0.9399243140964996 loss: 0.8659938424825668 test_loss: 0.0763186439871788 test_acc: 0.9246323529411765\n",
      "epoch 400 total_train_acc: 0.9392147587511825 loss: 0.9271867573261261 test_loss: 0.19179636240005493 test_acc: 0.9227941176470589\n",
      "epoch 401 total_train_acc: 0.9385052034058656 loss: 0.9081622362136841 test_loss: 0.10898514837026596 test_acc: 0.9215686274509803\n",
      "epoch 402 total_train_acc: 0.9375591296121097 loss: 0.9259336292743683 test_loss: 0.221500426530838 test_acc: 0.9307598039215687\n",
      "epoch 403 total_train_acc: 0.945600756859035 loss: 0.7964809685945511 test_loss: 0.20672740042209625 test_acc: 0.9221813725490197\n",
      "epoch 404 total_train_acc: 0.9394512771996215 loss: 0.9107652306556702 test_loss: 0.2172679901123047 test_acc: 0.9203431372549019\n",
      "epoch 405 total_train_acc: 0.9389782403027436 loss: 0.8481160402297974 test_loss: 0.0671502947807312 test_acc: 0.9276960784313726\n",
      "epoch 406 total_train_acc: 0.9399243140964996 loss: 0.8598477095365524 test_loss: 0.43382543325424194 test_acc: 0.9234068627450981\n",
      "epoch 407 total_train_acc: 0.9396877956480606 loss: 0.8652334660291672 test_loss: 0.08007591217756271 test_acc: 0.9221813725490197\n",
      "epoch 408 total_train_acc: 0.9389782403027436 loss: 0.8261657804250717 test_loss: 0.23617056012153625 test_acc: 0.9234068627450981\n",
      "epoch 409 total_train_acc: 0.9401608325449385 loss: 0.8116164058446884 test_loss: 0.2783454358577728 test_acc: 0.9234068627450981\n",
      "epoch 410 total_train_acc: 0.9420529801324503 loss: 0.8066114336252213 test_loss: 0.16384325921535492 test_acc: 0.9283088235294118\n",
      "epoch 411 total_train_acc: 0.9389782403027436 loss: 0.8644894063472748 test_loss: 0.19353792071342468 test_acc: 0.9246323529411765\n",
      "epoch 412 total_train_acc: 0.9387417218543046 loss: 0.8846992254257202 test_loss: 0.17577990889549255 test_acc: 0.9258578431372549\n",
      "epoch 413 total_train_acc: 0.9411069063386944 loss: 0.8106619119644165 test_loss: 0.2154376357793808 test_acc: 0.9270833333333334\n",
      "epoch 414 total_train_acc: 0.9396877956480606 loss: 0.8758585602045059 test_loss: 0.464375376701355 test_acc: 0.9227941176470589\n",
      "epoch 415 total_train_acc: 0.9401608325449385 loss: 0.8168830797076225 test_loss: 0.2732667028903961 test_acc: 0.9240196078431373\n",
      "epoch 416 total_train_acc: 0.934957426679281 loss: 0.8276249915361404 test_loss: 0.38001298904418945 test_acc: 0.9264705882352942\n",
      "epoch 417 total_train_acc: 0.9425260170293283 loss: 0.8192294836044312 test_loss: 0.23073159158229828 test_acc: 0.9240196078431373\n",
      "epoch 418 total_train_acc: 0.9429990539262062 loss: 0.8269675523042679 test_loss: 0.2199334055185318 test_acc: 0.9215686274509803\n",
      "epoch 419 total_train_acc: 0.9434720908230843 loss: 0.8609855026006699 test_loss: 0.21609902381896973 test_acc: 0.9227941176470589\n",
      "epoch 420 total_train_acc: 0.9463103122043519 loss: 0.7697257921099663 test_loss: 0.22082428634166718 test_acc: 0.9276960784313726\n",
      "epoch 421 total_train_acc: 0.9408703878902555 loss: 0.8526522517204285 test_loss: 0.3523988127708435 test_acc: 0.9270833333333334\n",
      "epoch 422 total_train_acc: 0.9403973509933775 loss: 0.885973334312439 test_loss: 0.2588190734386444 test_acc: 0.9264705882352942\n",
      "epoch 423 total_train_acc: 0.9387417218543046 loss: 0.8798367381095886 test_loss: 0.18003106117248535 test_acc: 0.9252450980392157\n",
      "epoch 424 total_train_acc: 0.9418164616840113 loss: 0.8317770957946777 test_loss: 0.13751442730426788 test_acc: 0.9258578431372549\n",
      "epoch 425 total_train_acc: 0.9413434247871334 loss: 0.8095958456397057 test_loss: 0.3094690442085266 test_acc: 0.9246323529411765\n",
      "epoch 426 total_train_acc: 0.9422894985808893 loss: 0.805639386177063 test_loss: 0.30318474769592285 test_acc: 0.9240196078431373\n",
      "epoch 427 total_train_acc: 0.9427625354777672 loss: 0.8143103271722794 test_loss: 0.1495034545660019 test_acc: 0.9258578431372549\n",
      "epoch 428 total_train_acc: 0.9425260170293283 loss: 0.8153755962848663 test_loss: 0.20545971393585205 test_acc: 0.9264705882352942\n",
      "epoch 429 total_train_acc: 0.9446546830652791 loss: 0.7965746521949768 test_loss: 0.16627271473407745 test_acc: 0.9252450980392157\n",
      "epoch 430 total_train_acc: 0.9479659413434248 loss: 0.7740651667118073 test_loss: 0.14492462575435638 test_acc: 0.9258578431372549\n",
      "epoch 431 total_train_acc: 0.9432355723746452 loss: 0.8194930851459503 test_loss: 0.5331997275352478 test_acc: 0.9264705882352942\n",
      "epoch 432 total_train_acc: 0.9394512771996215 loss: 0.8191355764865875 test_loss: 0.1349921077489853 test_acc: 0.9252450980392157\n",
      "epoch 433 total_train_acc: 0.9351939451277199 loss: 0.8551991432905197 test_loss: 0.10549691319465637 test_acc: 0.9258578431372549\n",
      "epoch 434 total_train_acc: 0.9427625354777672 loss: 0.8709001988172531 test_loss: 0.1801801472902298 test_acc: 0.9252450980392157\n",
      "epoch 435 total_train_acc: 0.9425260170293283 loss: 0.8811338096857071 test_loss: 0.09889250248670578 test_acc: 0.9227941176470589\n",
      "epoch 436 total_train_acc: 0.9413434247871334 loss: 0.8744685500860214 test_loss: 0.13520224392414093 test_acc: 0.928921568627451\n",
      "epoch 437 total_train_acc: 0.945837275307474 loss: 0.8609548956155777 test_loss: 0.11465132981538773 test_acc: 0.9234068627450981\n",
      "epoch 438 total_train_acc: 0.9418164616840113 loss: 0.8105064332485199 test_loss: 0.14349395036697388 test_acc: 0.9246323529411765\n",
      "epoch 439 total_train_acc: 0.9429990539262062 loss: 0.8521800488233566 test_loss: 0.4478510618209839 test_acc: 0.9313725490196079\n",
      "epoch 440 total_train_acc: 0.9460737937559129 loss: 0.8254324942827225 test_loss: 0.10112442821264267 test_acc: 0.9234068627450981\n",
      "epoch 441 total_train_acc: 0.9446546830652791 loss: 0.8425345420837402 test_loss: 0.2747969329357147 test_acc: 0.9240196078431373\n",
      "epoch 442 total_train_acc: 0.9446546830652791 loss: 0.8042596727609634 test_loss: 0.40074121952056885 test_acc: 0.9270833333333334\n",
      "epoch 443 total_train_acc: 0.9418164616840113 loss: 0.8307297676801682 test_loss: 0.2594318389892578 test_acc: 0.9264705882352942\n",
      "epoch 444 total_train_acc: 0.945364238410596 loss: 0.7702672854065895 test_loss: 0.23666062951087952 test_acc: 0.9246323529411765\n",
      "epoch 445 total_train_acc: 0.9439451277199622 loss: 0.7988217771053314 test_loss: 0.18513378500938416 test_acc: 0.9301470588235294\n",
      "epoch 446 total_train_acc: 0.9422894985808893 loss: 0.827306866645813 test_loss: 0.3050015866756439 test_acc: 0.9301470588235294\n",
      "epoch 447 total_train_acc: 0.9425260170293283 loss: 0.884619951248169 test_loss: 0.304184228181839 test_acc: 0.9240196078431373\n",
      "epoch 448 total_train_acc: 0.9420529801324503 loss: 0.8027109056711197 test_loss: 0.4966591000556946 test_acc: 0.9283088235294118\n",
      "epoch 449 total_train_acc: 0.9444181646168401 loss: 0.8186669796705246 test_loss: 0.08551789075136185 test_acc: 0.9264705882352942\n",
      "epoch 450 total_train_acc: 0.9396877956480606 loss: 0.8272490799427032 test_loss: 0.3149655759334564 test_acc: 0.9252450980392157\n",
      "epoch 451 total_train_acc: 0.945837275307474 loss: 0.8214901238679886 test_loss: 0.2865879535675049 test_acc: 0.9283088235294118\n",
      "epoch 452 total_train_acc: 0.9425260170293283 loss: 0.8241361081600189 test_loss: 0.21141891181468964 test_acc: 0.9209558823529411\n",
      "epoch 453 total_train_acc: 0.9427625354777672 loss: 0.8128257840871811 test_loss: 0.21395021677017212 test_acc: 0.9191176470588235\n",
      "epoch 454 total_train_acc: 0.9427625354777672 loss: 0.7916597574949265 test_loss: 0.18164081871509552 test_acc: 0.9240196078431373\n",
      "epoch 455 total_train_acc: 0.9401608325449385 loss: 0.8322349339723587 test_loss: 0.290964812040329 test_acc: 0.9246323529411765\n",
      "epoch 456 total_train_acc: 0.9439451277199622 loss: 0.7799694091081619 test_loss: 0.046322308480739594 test_acc: 0.9258578431372549\n",
      "epoch 457 total_train_acc: 0.9396877956480606 loss: 0.8236957788467407 test_loss: 0.3906804621219635 test_acc: 0.9252450980392157\n",
      "epoch 458 total_train_acc: 0.9432355723746452 loss: 0.7817702293395996 test_loss: 0.180049866437912 test_acc: 0.9240196078431373\n",
      "epoch 459 total_train_acc: 0.9429990539262062 loss: 0.7956219464540482 test_loss: 0.0999927893280983 test_acc: 0.9246323529411765\n",
      "epoch 460 total_train_acc: 0.9439451277199622 loss: 0.8103640526533127 test_loss: 0.26970425248146057 test_acc: 0.9234068627450981\n",
      "epoch 461 total_train_acc: 0.9437086092715232 loss: 0.7807686775922775 test_loss: 0.4650551676750183 test_acc: 0.928921568627451\n",
      "epoch 462 total_train_acc: 0.9396877956480606 loss: 0.7898220419883728 test_loss: 0.3205330967903137 test_acc: 0.9270833333333334\n",
      "epoch 463 total_train_acc: 0.9441816461684012 loss: 0.7840080261230469 test_loss: 0.13566187024116516 test_acc: 0.9276960784313726\n",
      "epoch 464 total_train_acc: 0.9439451277199622 loss: 0.7868795692920685 test_loss: 0.3463347554206848 test_acc: 0.9295343137254902\n",
      "epoch 465 total_train_acc: 0.945127719962157 loss: 0.7890812456607819 test_loss: 0.04982949420809746 test_acc: 0.9252450980392157\n",
      "epoch 466 total_train_acc: 0.9429990539262062 loss: 0.7547645270824432 test_loss: 0.49352914094924927 test_acc: 0.9258578431372549\n",
      "epoch 467 total_train_acc: 0.9429990539262062 loss: 0.7719236016273499 test_loss: 0.3696332573890686 test_acc: 0.9283088235294118\n",
      "epoch 468 total_train_acc: 0.9460737937559129 loss: 0.8377172648906708 test_loss: 0.24956431984901428 test_acc: 0.9227941176470589\n",
      "epoch 469 total_train_acc: 0.9437086092715232 loss: 0.7879993915557861 test_loss: 0.04043637588620186 test_acc: 0.9252450980392157\n",
      "epoch 470 total_train_acc: 0.9439451277199622 loss: 0.8265099078416824 test_loss: 0.07764013111591339 test_acc: 0.928921568627451\n",
      "epoch 471 total_train_acc: 0.9439451277199622 loss: 0.8661129474639893 test_loss: 0.34223508834838867 test_acc: 0.9258578431372549\n",
      "epoch 472 total_train_acc: 0.9446546830652791 loss: 0.7765027433633804 test_loss: 0.12296920269727707 test_acc: 0.9270833333333334\n",
      "epoch 473 total_train_acc: 0.9408703878902555 loss: 0.8135579824447632 test_loss: 0.238885298371315 test_acc: 0.9252450980392157\n",
      "epoch 474 total_train_acc: 0.9448912015137181 loss: 0.7780011743307114 test_loss: 0.22211237251758575 test_acc: 0.9270833333333334\n",
      "epoch 475 total_train_acc: 0.9403973509933775 loss: 0.8424522131681442 test_loss: 0.4035789668560028 test_acc: 0.9252450980392157\n",
      "epoch 476 total_train_acc: 0.945837275307474 loss: 0.813031867146492 test_loss: 0.2825833261013031 test_acc: 0.9270833333333334\n",
      "epoch 477 total_train_acc: 0.9415799432355724 loss: 0.777734711766243 test_loss: 0.2521229386329651 test_acc: 0.9246323529411765\n",
      "epoch 478 total_train_acc: 0.9415799432355724 loss: 0.7940790504217148 test_loss: 0.20687457919120789 test_acc: 0.9252450980392157\n",
      "epoch 479 total_train_acc: 0.9432355723746452 loss: 0.7836629897356033 test_loss: 0.03780430555343628 test_acc: 0.928921568627451\n",
      "epoch 480 total_train_acc: 0.9439451277199622 loss: 0.7470308691263199 test_loss: 0.4080084264278412 test_acc: 0.9258578431372549\n",
      "epoch 481 total_train_acc: 0.9439451277199622 loss: 0.7672135680913925 test_loss: 0.18567466735839844 test_acc: 0.9252450980392157\n",
      "epoch 482 total_train_acc: 0.945364238410596 loss: 0.7626820206642151 test_loss: 0.2890976369380951 test_acc: 0.9270833333333334\n",
      "epoch 483 total_train_acc: 0.9446546830652791 loss: 0.8012861609458923 test_loss: 0.2776482105255127 test_acc: 0.9234068627450981\n",
      "epoch 484 total_train_acc: 0.9463103122043519 loss: 0.8093727380037308 test_loss: 0.1731662154197693 test_acc: 0.9258578431372549\n",
      "epoch 485 total_train_acc: 0.9439451277199622 loss: 0.7851830720901489 test_loss: 0.23114842176437378 test_acc: 0.9307598039215687\n",
      "epoch 486 total_train_acc: 0.9441816461684012 loss: 0.7484793066978455 test_loss: 0.20012550055980682 test_acc: 0.9295343137254902\n",
      "epoch 487 total_train_acc: 0.9420529801324503 loss: 0.8162542581558228 test_loss: 0.11968842148780823 test_acc: 0.9246323529411765\n",
      "epoch 488 total_train_acc: 0.9434720908230843 loss: 0.8214782774448395 test_loss: 0.2027355283498764 test_acc: 0.9252450980392157\n",
      "epoch 489 total_train_acc: 0.9460737937559129 loss: 0.8000858873128891 test_loss: 0.21117207407951355 test_acc: 0.9283088235294118\n",
      "epoch 490 total_train_acc: 0.9477294228949859 loss: 0.7509379386901855 test_loss: 0.03276984021067619 test_acc: 0.9270833333333334\n",
      "epoch 491 total_train_acc: 0.945837275307474 loss: 0.781006246805191 test_loss: 0.12039069086313248 test_acc: 0.9252450980392157\n",
      "epoch 492 total_train_acc: 0.9444181646168401 loss: 0.7537147998809814 test_loss: 0.2970789670944214 test_acc: 0.9307598039215687\n",
      "epoch 493 total_train_acc: 0.9463103122043519 loss: 0.8212283998727798 test_loss: 0.3705707788467407 test_acc: 0.9270833333333334\n",
      "epoch 494 total_train_acc: 0.9479659413434248 loss: 0.7966475188732147 test_loss: 0.491590291261673 test_acc: 0.9252450980392157\n",
      "epoch 495 total_train_acc: 0.9484389782403028 loss: 0.7387067899107933 test_loss: 0.5343535542488098 test_acc: 0.9246323529411765\n",
      "epoch 496 total_train_acc: 0.945127719962157 loss: 0.7591938972473145 test_loss: 0.2026147097349167 test_acc: 0.9252450980392157\n",
      "epoch 497 total_train_acc: 0.945364238410596 loss: 0.7661662697792053 test_loss: 0.12369616329669952 test_acc: 0.9301470588235294\n",
      "epoch 498 total_train_acc: 0.945364238410596 loss: 0.7560731023550034 test_loss: 0.0438416562974453 test_acc: 0.9240196078431373\n",
      "epoch 499 total_train_acc: 0.9484389782403028 loss: 0.7723170965909958 test_loss: 0.24381960928440094 test_acc: 0.9264705882352942\n",
      "epoch 500 total_train_acc: 0.9470198675496688 loss: 0.7364467009902 test_loss: 0.08824428915977478 test_acc: 0.9270833333333334\n",
      "epoch 501 total_train_acc: 0.9446546830652791 loss: 0.7787043303251266 test_loss: 0.49155378341674805 test_acc: 0.9301470588235294\n",
      "epoch 502 total_train_acc: 0.9467833491012299 loss: 0.777136966586113 test_loss: 0.271302193403244 test_acc: 0.9270833333333334\n",
      "epoch 503 total_train_acc: 0.9508041627246925 loss: 0.7027705013751984 test_loss: 0.06738477945327759 test_acc: 0.9252450980392157\n",
      "epoch 504 total_train_acc: 0.9479659413434248 loss: 0.7689944058656693 test_loss: 0.5134976506233215 test_acc: 0.9264705882352942\n",
      "epoch 505 total_train_acc: 0.9472563859981078 loss: 0.7155523672699928 test_loss: 0.07732658833265305 test_acc: 0.9264705882352942\n",
      "epoch 506 total_train_acc: 0.9470198675496688 loss: 0.7381526976823807 test_loss: 0.37859705090522766 test_acc: 0.9332107843137255\n",
      "epoch 507 total_train_acc: 0.9418164616840113 loss: 0.7674400955438614 test_loss: 0.20272931456565857 test_acc: 0.9246323529411765\n",
      "epoch 508 total_train_acc: 0.9444181646168401 loss: 0.784105658531189 test_loss: 0.16666196286678314 test_acc: 0.9246323529411765\n",
      "epoch 509 total_train_acc: 0.9470198675496688 loss: 0.7427524030208588 test_loss: 0.3129671514034271 test_acc: 0.9338235294117647\n",
      "epoch 510 total_train_acc: 0.9429990539262062 loss: 0.738018311560154 test_loss: 0.31464967131614685 test_acc: 0.9276960784313726\n",
      "epoch 511 total_train_acc: 0.9470198675496688 loss: 0.7349594384431839 test_loss: 0.10586130619049072 test_acc: 0.9264705882352942\n",
      "epoch 512 total_train_acc: 0.9470198675496688 loss: 0.7259005010128021 test_loss: 0.23751747608184814 test_acc: 0.9325980392156863\n",
      "epoch 513 total_train_acc: 0.9482024597918638 loss: 0.7472180873155594 test_loss: 0.3926030695438385 test_acc: 0.9258578431372549\n",
      "epoch 514 total_train_acc: 0.945600756859035 loss: 0.7274070531129837 test_loss: 0.09911512583494186 test_acc: 0.9252450980392157\n",
      "epoch 515 total_train_acc: 0.9446546830652791 loss: 0.8059002012014389 test_loss: 0.2153744250535965 test_acc: 0.9295343137254902\n",
      "epoch 516 total_train_acc: 0.9489120151371807 loss: 0.7761341780424118 test_loss: 0.43102404475212097 test_acc: 0.9276960784313726\n",
      "epoch 517 total_train_acc: 0.9463103122043519 loss: 0.738628476858139 test_loss: 0.14430871605873108 test_acc: 0.9276960784313726\n",
      "epoch 518 total_train_acc: 0.9465468306527909 loss: 0.7585866153240204 test_loss: 0.32981041073799133 test_acc: 0.9264705882352942\n",
      "epoch 519 total_train_acc: 0.9467833491012299 loss: 0.7733050137758255 test_loss: 0.1959747076034546 test_acc: 0.9276960784313726\n",
      "epoch 520 total_train_acc: 0.9500946073793756 loss: 0.7597176283597946 test_loss: 0.37962445616722107 test_acc: 0.9246323529411765\n",
      "epoch 521 total_train_acc: 0.9482024597918638 loss: 0.753569632768631 test_loss: 0.2785378694534302 test_acc: 0.928921568627451\n",
      "epoch 522 total_train_acc: 0.9479659413434248 loss: 0.7928226590156555 test_loss: 0.0717073455452919 test_acc: 0.9240196078431373\n",
      "epoch 523 total_train_acc: 0.9406338694418165 loss: 0.8114573210477829 test_loss: 0.30833354592323303 test_acc: 0.9252450980392157\n",
      "epoch 524 total_train_acc: 0.9482024597918638 loss: 0.7692688703536987 test_loss: 0.16317269206047058 test_acc: 0.9301470588235294\n",
      "epoch 525 total_train_acc: 0.945127719962157 loss: 0.7832300961017609 test_loss: 0.3062478303909302 test_acc: 0.9295343137254902\n",
      "epoch 526 total_train_acc: 0.9470198675496688 loss: 0.7664043307304382 test_loss: 0.19732628762722015 test_acc: 0.9234068627450981\n",
      "epoch 527 total_train_acc: 0.9463103122043519 loss: 0.72782401740551 test_loss: 0.3671237528324127 test_acc: 0.9313725490196079\n",
      "epoch 528 total_train_acc: 0.9489120151371807 loss: 0.7511954456567764 test_loss: 0.06927759945392609 test_acc: 0.9307598039215687\n",
      "epoch 529 total_train_acc: 0.945364238410596 loss: 0.7251275032758713 test_loss: 0.41152969002723694 test_acc: 0.9252450980392157\n",
      "epoch 530 total_train_acc: 0.9486754966887417 loss: 0.6999749392271042 test_loss: 0.2741198241710663 test_acc: 0.9246323529411765\n",
      "epoch 531 total_train_acc: 0.945837275307474 loss: 0.7183293104171753 test_loss: 0.37372544407844543 test_acc: 0.9221813725490197\n",
      "epoch 532 total_train_acc: 0.9486754966887417 loss: 0.7347785234451294 test_loss: 0.2271571159362793 test_acc: 0.9234068627450981\n",
      "epoch 533 total_train_acc: 0.9503311258278145 loss: 0.7761555314064026 test_loss: 0.22490285336971283 test_acc: 0.9270833333333334\n",
      "epoch 534 total_train_acc: 0.9472563859981078 loss: 0.75409235060215 test_loss: 0.2000509649515152 test_acc: 0.9276960784313726\n",
      "epoch 535 total_train_acc: 0.9460737937559129 loss: 0.7686759531497955 test_loss: 0.17791104316711426 test_acc: 0.9307598039215687\n",
      "epoch 536 total_train_acc: 0.9465468306527909 loss: 0.7444326728582382 test_loss: 0.29724380373954773 test_acc: 0.9276960784313726\n",
      "epoch 537 total_train_acc: 0.9493850520340587 loss: 0.7600353881716728 test_loss: 0.3827179968357086 test_acc: 0.9295343137254902\n",
      "epoch 538 total_train_acc: 0.9486754966887417 loss: 0.7354158908128738 test_loss: 0.2834089696407318 test_acc: 0.9252450980392157\n",
      "epoch 539 total_train_acc: 0.945127719962157 loss: 0.7642032504081726 test_loss: 0.2622893750667572 test_acc: 0.9234068627450981\n",
      "epoch 540 total_train_acc: 0.9477294228949859 loss: 0.776975080370903 test_loss: 0.1852075308561325 test_acc: 0.928921568627451\n",
      "epoch 541 total_train_acc: 0.9463103122043519 loss: 0.7830779850482941 test_loss: 0.05924854427576065 test_acc: 0.9307598039215687\n",
      "epoch 542 total_train_acc: 0.9470198675496688 loss: 0.7243348360061646 test_loss: 0.05786750093102455 test_acc: 0.9270833333333334\n",
      "epoch 543 total_train_acc: 0.945600756859035 loss: 0.7194768637418747 test_loss: 0.11386581510305405 test_acc: 0.9295343137254902\n",
      "epoch 544 total_train_acc: 0.945837275307474 loss: 0.7900830656290054 test_loss: 0.17366661131381989 test_acc: 0.9270833333333334\n",
      "epoch 545 total_train_acc: 0.9503311258278145 loss: 0.7337665855884552 test_loss: 0.062111996114254 test_acc: 0.9252450980392157\n",
      "epoch 546 total_train_acc: 0.9479659413434248 loss: 0.7034820839762688 test_loss: 0.1903190016746521 test_acc: 0.9313725490196079\n",
      "epoch 547 total_train_acc: 0.9470198675496688 loss: 0.7023032829165459 test_loss: 0.2203298956155777 test_acc: 0.928921568627451\n",
      "epoch 548 total_train_acc: 0.9505676442762535 loss: 0.6803137436509132 test_loss: 0.3515418469905853 test_acc: 0.9264705882352942\n",
      "epoch 549 total_train_acc: 0.9489120151371807 loss: 0.7167777121067047 test_loss: 0.09520826488733292 test_acc: 0.928921568627451\n",
      "epoch 550 total_train_acc: 0.9474929044465469 loss: 0.7669528126716614 test_loss: 0.2280016392469406 test_acc: 0.9258578431372549\n",
      "epoch 551 total_train_acc: 0.9524597918637654 loss: 0.7717363238334656 test_loss: 0.1542973518371582 test_acc: 0.9270833333333334\n",
      "epoch 552 total_train_acc: 0.9493850520340587 loss: 0.7470582872629166 test_loss: 0.22048233449459076 test_acc: 0.9301470588235294\n",
      "epoch 553 total_train_acc: 0.9484389782403028 loss: 0.7437665313482285 test_loss: 0.6079732179641724 test_acc: 0.9252450980392157\n",
      "epoch 554 total_train_acc: 0.9486754966887417 loss: 0.7636151164770126 test_loss: 0.3265434503555298 test_acc: 0.9276960784313726\n",
      "epoch 555 total_train_acc: 0.9503311258278145 loss: 0.7194255143404007 test_loss: 0.2110353261232376 test_acc: 0.9295343137254902\n",
      "epoch 556 total_train_acc: 0.9479659413434248 loss: 0.7500409930944443 test_loss: 0.23195159435272217 test_acc: 0.9227941176470589\n",
      "epoch 557 total_train_acc: 0.9472563859981078 loss: 0.762302502989769 test_loss: 0.053652431815862656 test_acc: 0.9295343137254902\n",
      "epoch 558 total_train_acc: 0.9486754966887417 loss: 0.7040885537862778 test_loss: 0.06361556053161621 test_acc: 0.928921568627451\n",
      "epoch 559 total_train_acc: 0.9486754966887417 loss: 0.7298929840326309 test_loss: 0.13133668899536133 test_acc: 0.9301470588235294\n",
      "epoch 560 total_train_acc: 0.9512771996215705 loss: 0.7185969352722168 test_loss: 0.20248736441135406 test_acc: 0.9307598039215687\n",
      "epoch 561 total_train_acc: 0.9548249763481551 loss: 0.6897200345993042 test_loss: 0.05341944098472595 test_acc: 0.928921568627451\n",
      "epoch 562 total_train_acc: 0.9503311258278145 loss: 0.6687131449580193 test_loss: 0.11310621351003647 test_acc: 0.9258578431372549\n",
      "epoch 563 total_train_acc: 0.9489120151371807 loss: 0.6924688071012497 test_loss: 0.3301302194595337 test_acc: 0.928921568627451\n",
      "epoch 564 total_train_acc: 0.9503311258278145 loss: 0.7117024883627892 test_loss: 0.16288217902183533 test_acc: 0.9240196078431373\n",
      "epoch 565 total_train_acc: 0.9467833491012299 loss: 0.7413417994976044 test_loss: 0.2309020310640335 test_acc: 0.9264705882352942\n",
      "epoch 566 total_train_acc: 0.9486754966887417 loss: 0.7249728888273239 test_loss: 0.21981778740882874 test_acc: 0.9283088235294118\n",
      "epoch 567 total_train_acc: 0.9505676442762535 loss: 0.6606001630425453 test_loss: 0.2723349332809448 test_acc: 0.9252450980392157\n",
      "epoch 568 total_train_acc: 0.9541154210028382 loss: 0.687325082719326 test_loss: 0.12498123943805695 test_acc: 0.9295343137254902\n",
      "epoch 569 total_train_acc: 0.9486754966887417 loss: 0.6997407749295235 test_loss: 0.0743044763803482 test_acc: 0.9332107843137255\n",
      "epoch 570 total_train_acc: 0.9510406811731315 loss: 0.6888542622327805 test_loss: 0.2172386795282364 test_acc: 0.9301470588235294\n",
      "epoch 571 total_train_acc: 0.9484389782403028 loss: 0.7145228087902069 test_loss: 0.2347419112920761 test_acc: 0.9270833333333334\n",
      "epoch 572 total_train_acc: 0.9519867549668874 loss: 0.6817607060074806 test_loss: 0.23796983063220978 test_acc: 0.9276960784313726\n",
      "epoch 573 total_train_acc: 0.9500946073793756 loss: 0.6978771388530731 test_loss: 0.23286959528923035 test_acc: 0.9270833333333334\n",
      "epoch 574 total_train_acc: 0.9477294228949859 loss: 0.7466226071119308 test_loss: 0.34661296010017395 test_acc: 0.9258578431372549\n",
      "epoch 575 total_train_acc: 0.9496215704824976 loss: 0.6891849040985107 test_loss: 0.08328583091497421 test_acc: 0.9283088235294118\n",
      "epoch 576 total_train_acc: 0.9470198675496688 loss: 0.7242172285914421 test_loss: 0.15687009692192078 test_acc: 0.9264705882352942\n",
      "epoch 577 total_train_acc: 0.9470198675496688 loss: 0.7390615791082382 test_loss: 0.19793358445167542 test_acc: 0.9270833333333334\n",
      "epoch 578 total_train_acc: 0.9500946073793756 loss: 0.7341371104121208 test_loss: 0.3704589009284973 test_acc: 0.9252450980392157\n",
      "epoch 579 total_train_acc: 0.9493850520340587 loss: 0.7175210565328598 test_loss: 0.11484935134649277 test_acc: 0.9270833333333334\n",
      "epoch 580 total_train_acc: 0.9493850520340587 loss: 0.7113523334264755 test_loss: 0.07785637676715851 test_acc: 0.9246323529411765\n",
      "epoch 581 total_train_acc: 0.9498580889309366 loss: 0.6499403938651085 test_loss: 0.25594577193260193 test_acc: 0.9270833333333334\n",
      "epoch 582 total_train_acc: 0.9548249763481551 loss: 0.6651209518313408 test_loss: 0.2911931872367859 test_acc: 0.9301470588235294\n",
      "epoch 583 total_train_acc: 0.9486754966887417 loss: 0.6489609107375145 test_loss: 0.27850696444511414 test_acc: 0.9301470588235294\n",
      "epoch 584 total_train_acc: 0.9534058656575213 loss: 0.7030339315533638 test_loss: 0.47248849272727966 test_acc: 0.9246323529411765\n",
      "epoch 585 total_train_acc: 0.9505676442762535 loss: 0.688819020986557 test_loss: 0.35366785526275635 test_acc: 0.9276960784313726\n",
      "epoch 586 total_train_acc: 0.9496215704824976 loss: 0.7101039737462997 test_loss: 0.2880549728870392 test_acc: 0.9295343137254902\n",
      "epoch 587 total_train_acc: 0.9472563859981078 loss: 0.7348862588405609 test_loss: 0.3576122224330902 test_acc: 0.9240196078431373\n",
      "epoch 588 total_train_acc: 0.9482024597918638 loss: 0.7138704210519791 test_loss: 0.22207969427108765 test_acc: 0.9270833333333334\n",
      "epoch 589 total_train_acc: 0.9448912015137181 loss: 0.7455790489912033 test_loss: 0.35913753509521484 test_acc: 0.9252450980392157\n",
      "epoch 590 total_train_acc: 0.9517502365184485 loss: 0.6809099316596985 test_loss: 0.07942213863134384 test_acc: 0.9307598039215687\n",
      "epoch 591 total_train_acc: 0.9529328287606433 loss: 0.7136112749576569 test_loss: 0.22623737156391144 test_acc: 0.9258578431372549\n",
      "epoch 592 total_train_acc: 0.9503311258278145 loss: 0.6863695383071899 test_loss: 0.23223361372947693 test_acc: 0.9283088235294118\n",
      "epoch 593 total_train_acc: 0.9474929044465469 loss: 0.7191635966300964 test_loss: 0.5912932753562927 test_acc: 0.928921568627451\n",
      "epoch 594 total_train_acc: 0.9477294228949859 loss: 0.6874351054430008 test_loss: 0.1312311738729477 test_acc: 0.9295343137254902\n",
      "epoch 595 total_train_acc: 0.9517502365184485 loss: 0.7333306968212128 test_loss: 0.25757041573524475 test_acc: 0.9276960784313726\n",
      "epoch 596 total_train_acc: 0.9536423841059603 loss: 0.6747482791543007 test_loss: 0.07733951508998871 test_acc: 0.9283088235294118\n",
      "epoch 597 total_train_acc: 0.9524597918637654 loss: 0.6802725419402122 test_loss: 0.037626057863235474 test_acc: 0.928921568627451\n",
      "epoch 598 total_train_acc: 0.9482024597918638 loss: 0.705791562795639 test_loss: 0.32752424478530884 test_acc: 0.9301470588235294\n",
      "epoch 599 total_train_acc: 0.9522232734153264 loss: 0.6573779582977295 test_loss: 0.21708963811397552 test_acc: 0.928921568627451\n",
      "epoch 600 total_train_acc: 0.9500946073793756 loss: 0.6995685398578644 test_loss: 0.10722899436950684 test_acc: 0.9283088235294118\n",
      "epoch 601 total_train_acc: 0.9522232734153264 loss: 0.6361029744148254 test_loss: 0.17420227825641632 test_acc: 0.9295343137254902\n",
      "epoch 602 total_train_acc: 0.9512771996215705 loss: 0.70078194886446 test_loss: 0.21453601121902466 test_acc: 0.9307598039215687\n",
      "epoch 603 total_train_acc: 0.956717123935667 loss: 0.7110524997115135 test_loss: 0.2656741142272949 test_acc: 0.9301470588235294\n",
      "epoch 604 total_train_acc: 0.9477294228949859 loss: 0.715999111533165 test_loss: 0.07930097728967667 test_acc: 0.9325980392156863\n",
      "epoch 605 total_train_acc: 0.9496215704824976 loss: 0.67717045545578 test_loss: 0.05913212522864342 test_acc: 0.9227941176470589\n",
      "epoch 606 total_train_acc: 0.9503311258278145 loss: 0.6936929225921631 test_loss: 0.3495769500732422 test_acc: 0.9234068627450981\n",
      "epoch 607 total_train_acc: 0.9550614947965941 loss: 0.6631880551576614 test_loss: 0.033554039895534515 test_acc: 0.9258578431372549\n",
      "epoch 608 total_train_acc: 0.9531693472090823 loss: 0.6432162746787071 test_loss: 0.4984663724899292 test_acc: 0.9319852941176471\n",
      "epoch 609 total_train_acc: 0.9482024597918638 loss: 0.7221898883581161 test_loss: 0.3744937777519226 test_acc: 0.9295343137254902\n",
      "epoch 610 total_train_acc: 0.9484389782403028 loss: 0.6867574974894524 test_loss: 0.11843224614858627 test_acc: 0.9301470588235294\n",
      "epoch 611 total_train_acc: 0.9491485335856197 loss: 0.6524303033947945 test_loss: 0.3703867495059967 test_acc: 0.9270833333333334\n",
      "epoch 612 total_train_acc: 0.9519867549668874 loss: 0.6885436773300171 test_loss: 0.41055911779403687 test_acc: 0.9258578431372549\n",
      "epoch 613 total_train_acc: 0.9534058656575213 loss: 0.651995025575161 test_loss: 0.05935774743556976 test_acc: 0.9270833333333334\n",
      "epoch 614 total_train_acc: 0.9524597918637654 loss: 0.657581515610218 test_loss: 0.31786084175109863 test_acc: 0.9270833333333334\n",
      "epoch 615 total_train_acc: 0.9545884578997161 loss: 0.6431075185537338 test_loss: 0.17713770270347595 test_acc: 0.9258578431372549\n",
      "epoch 616 total_train_acc: 0.9510406811731315 loss: 0.6586790084838867 test_loss: 0.3630858361721039 test_acc: 0.928921568627451\n",
      "epoch 617 total_train_acc: 0.9531693472090823 loss: 0.6702307164669037 test_loss: 0.09123453497886658 test_acc: 0.9252450980392157\n",
      "epoch 618 total_train_acc: 0.9526963103122044 loss: 0.7033544629812241 test_loss: 0.26365962624549866 test_acc: 0.9270833333333334\n",
      "epoch 619 total_train_acc: 0.9552980132450332 loss: 0.6602664291858673 test_loss: 0.13651686906814575 test_acc: 0.9319852941176471\n",
      "epoch 620 total_train_acc: 0.9524597918637654 loss: 0.6751297488808632 test_loss: 0.3098491430282593 test_acc: 0.928921568627451\n",
      "epoch 621 total_train_acc: 0.9508041627246925 loss: 0.6856339052319527 test_loss: 0.2546989917755127 test_acc: 0.928921568627451\n",
      "epoch 622 total_train_acc: 0.9515137180700095 loss: 0.6930454447865486 test_loss: 0.36043864488601685 test_acc: 0.9319852941176471\n",
      "epoch 623 total_train_acc: 0.9510406811731315 loss: 0.6958853304386139 test_loss: 0.1688329130411148 test_acc: 0.9276960784313726\n",
      "epoch 624 total_train_acc: 0.9522232734153264 loss: 0.6691554114222527 test_loss: 0.1326790452003479 test_acc: 0.928921568627451\n",
      "epoch 625 total_train_acc: 0.9560075685903501 loss: 0.6408028975129128 test_loss: 0.19332915544509888 test_acc: 0.9295343137254902\n",
      "epoch 626 total_train_acc: 0.9529328287606433 loss: 0.6523370817303658 test_loss: 0.27988478541374207 test_acc: 0.9283088235294118\n",
      "epoch 627 total_train_acc: 0.9491485335856197 loss: 0.6791718155145645 test_loss: 0.472982257604599 test_acc: 0.9295343137254902\n",
      "epoch 628 total_train_acc: 0.9526963103122044 loss: 0.6423670426011086 test_loss: 0.22735072672367096 test_acc: 0.9307598039215687\n",
      "epoch 629 total_train_acc: 0.9508041627246925 loss: 0.6700588688254356 test_loss: 0.11533673852682114 test_acc: 0.9234068627450981\n",
      "epoch 630 total_train_acc: 0.9524597918637654 loss: 0.6823515072464943 test_loss: 0.11118850111961365 test_acc: 0.9301470588235294\n",
      "epoch 631 total_train_acc: 0.9526963103122044 loss: 0.7305131629109383 test_loss: 0.4409063458442688 test_acc: 0.9283088235294118\n",
      "epoch 632 total_train_acc: 0.9505676442762535 loss: 0.74745724350214 test_loss: 0.3018645942211151 test_acc: 0.9270833333333334\n",
      "epoch 633 total_train_acc: 0.9536423841059603 loss: 0.6917238980531693 test_loss: 0.03207050636410713 test_acc: 0.9313725490196079\n",
      "epoch 634 total_train_acc: 0.9538789025543992 loss: 0.6706851124763489 test_loss: 0.40766966342926025 test_acc: 0.9258578431372549\n",
      "epoch 635 total_train_acc: 0.9519867549668874 loss: 0.7018228471279144 test_loss: 0.10970962792634964 test_acc: 0.9319852941176471\n",
      "epoch 636 total_train_acc: 0.9503311258278145 loss: 0.7096360474824905 test_loss: 0.1154128685593605 test_acc: 0.9270833333333334\n",
      "epoch 637 total_train_acc: 0.9531693472090823 loss: 0.6539618447422981 test_loss: 0.30571770668029785 test_acc: 0.9252450980392157\n",
      "epoch 638 total_train_acc: 0.9477294228949859 loss: 0.681685745716095 test_loss: 0.2688024044036865 test_acc: 0.9325980392156863\n",
      "epoch 639 total_train_acc: 0.9548249763481551 loss: 0.6605999171733856 test_loss: 0.34813839197158813 test_acc: 0.9319852941176471\n",
      "epoch 640 total_train_acc: 0.9496215704824976 loss: 0.643598809838295 test_loss: 0.09642260521650314 test_acc: 0.9313725490196079\n",
      "epoch 641 total_train_acc: 0.9505676442762535 loss: 0.6741326972842216 test_loss: 0.389078289270401 test_acc: 0.9283088235294118\n",
      "epoch 642 total_train_acc: 0.9512771996215705 loss: 0.6475390270352364 test_loss: 0.31802305579185486 test_acc: 0.9270833333333334\n",
      "epoch 643 total_train_acc: 0.9508041627246925 loss: 0.685249924659729 test_loss: 0.42972829937934875 test_acc: 0.9325980392156863\n",
      "epoch 644 total_train_acc: 0.9531693472090823 loss: 0.6550669148564339 test_loss: 0.22177673876285553 test_acc: 0.9252450980392157\n",
      "epoch 645 total_train_acc: 0.9486754966887417 loss: 0.7238566428422928 test_loss: 0.24413910508155823 test_acc: 0.9283088235294118\n",
      "epoch 646 total_train_acc: 0.9548249763481551 loss: 0.6684906780719757 test_loss: 0.4839332103729248 test_acc: 0.9313725490196079\n",
      "epoch 647 total_train_acc: 0.9557710501419111 loss: 0.702594131231308 test_loss: 0.11076249182224274 test_acc: 0.9270833333333334\n",
      "epoch 648 total_train_acc: 0.9491485335856197 loss: 0.6887735053896904 test_loss: 0.19826902449131012 test_acc: 0.9215686274509803\n",
      "epoch 649 total_train_acc: 0.9517502365184485 loss: 0.6387302502989769 test_loss: 0.17891913652420044 test_acc: 0.9301470588235294\n",
      "epoch 650 total_train_acc: 0.9543519394512772 loss: 0.6861651390790939 test_loss: 0.5148123502731323 test_acc: 0.9307598039215687\n",
      "epoch 651 total_train_acc: 0.9545884578997161 loss: 0.6278295665979385 test_loss: 0.03200163692235947 test_acc: 0.9283088235294118\n",
      "epoch 652 total_train_acc: 0.9555345316934721 loss: 0.6370804831385612 test_loss: 0.11806011199951172 test_acc: 0.9313725490196079\n",
      "epoch 653 total_train_acc: 0.9550614947965941 loss: 0.6884214282035828 test_loss: 0.18297001719474792 test_acc: 0.9295343137254902\n",
      "epoch 654 total_train_acc: 0.9541154210028382 loss: 0.6808868050575256 test_loss: 0.33746337890625 test_acc: 0.928921568627451\n",
      "epoch 655 total_train_acc: 0.9543519394512772 loss: 0.6618331372737885 test_loss: 0.23184123635292053 test_acc: 0.9332107843137255\n",
      "epoch 656 total_train_acc: 0.9571901608325449 loss: 0.6132877096533775 test_loss: 0.5237784385681152 test_acc: 0.928921568627451\n",
      "epoch 657 total_train_acc: 0.9531693472090823 loss: 0.639647588133812 test_loss: 0.3036555349826813 test_acc: 0.9295343137254902\n",
      "epoch 658 total_train_acc: 0.9552980132450332 loss: 0.6044477820396423 test_loss: 0.40889066457748413 test_acc: 0.9307598039215687\n",
      "epoch 659 total_train_acc: 0.9538789025543992 loss: 0.6301845610141754 test_loss: 0.2825000584125519 test_acc: 0.9301470588235294\n",
      "epoch 660 total_train_acc: 0.9550614947965941 loss: 0.6915519088506699 test_loss: 0.42988404631614685 test_acc: 0.9301470588235294\n",
      "epoch 661 total_train_acc: 0.9531693472090823 loss: 0.6746688932180405 test_loss: 0.27574849128723145 test_acc: 0.9325980392156863\n",
      "epoch 662 total_train_acc: 0.9550614947965941 loss: 0.6777762398123741 test_loss: 0.14292219281196594 test_acc: 0.9276960784313726\n",
      "epoch 663 total_train_acc: 0.9512771996215705 loss: 0.7028420865535736 test_loss: 0.1572076827287674 test_acc: 0.928921568627451\n",
      "epoch 664 total_train_acc: 0.9552980132450332 loss: 0.6689548119902611 test_loss: 0.2083444893360138 test_acc: 0.928921568627451\n",
      "epoch 665 total_train_acc: 0.9586092715231788 loss: 0.6016085371375084 test_loss: 0.15958161652088165 test_acc: 0.9295343137254902\n",
      "epoch 666 total_train_acc: 0.9555345316934721 loss: 0.6260448545217514 test_loss: 0.20300833880901337 test_acc: 0.9313725490196079\n",
      "epoch 667 total_train_acc: 0.9529328287606433 loss: 0.6696368381381035 test_loss: 0.23569472134113312 test_acc: 0.9313725490196079\n",
      "epoch 668 total_train_acc: 0.9538789025543992 loss: 0.6206366941332817 test_loss: 0.24485594034194946 test_acc: 0.9264705882352942\n",
      "epoch 669 total_train_acc: 0.9581362346263008 loss: 0.6464786604046822 test_loss: 0.0954698771238327 test_acc: 0.9301470588235294\n",
      "epoch 670 total_train_acc: 0.9534058656575213 loss: 0.6293925121426582 test_loss: 0.17961663007736206 test_acc: 0.9301470588235294\n",
      "epoch 671 total_train_acc: 0.956717123935667 loss: 0.6433666795492172 test_loss: 0.3209300935268402 test_acc: 0.9307598039215687\n",
      "epoch 672 total_train_acc: 0.9515137180700095 loss: 0.6173244416713715 test_loss: 0.2158644050359726 test_acc: 0.9307598039215687\n",
      "epoch 673 total_train_acc: 0.9545884578997161 loss: 0.598608061671257 test_loss: 0.21445611119270325 test_acc: 0.9313725490196079\n",
      "epoch 674 total_train_acc: 0.9543519394512772 loss: 0.6151659712195396 test_loss: 0.23023544251918793 test_acc: 0.9258578431372549\n",
      "epoch 675 total_train_acc: 0.9581362346263008 loss: 0.6085961684584618 test_loss: 0.19257378578186035 test_acc: 0.9301470588235294\n",
      "epoch 676 total_train_acc: 0.9517502365184485 loss: 0.6543113440275192 test_loss: 0.22800865769386292 test_acc: 0.9283088235294118\n",
      "epoch 677 total_train_acc: 0.9543519394512772 loss: 0.6430827528238297 test_loss: 0.14177051186561584 test_acc: 0.9301470588235294\n",
      "epoch 678 total_train_acc: 0.9545884578997161 loss: 0.6534437164664268 test_loss: 0.3048909306526184 test_acc: 0.9319852941176471\n",
      "epoch 679 total_train_acc: 0.9505676442762535 loss: 0.6892712563276291 test_loss: 0.5380544662475586 test_acc: 0.928921568627451\n",
      "epoch 680 total_train_acc: 0.9555345316934721 loss: 0.640723705291748 test_loss: 0.24982653558254242 test_acc: 0.9283088235294118\n",
      "epoch 681 total_train_acc: 0.9522232734153264 loss: 0.6758320555090904 test_loss: 0.2505151033401489 test_acc: 0.9307598039215687\n",
      "epoch 682 total_train_acc: 0.9576631977294229 loss: 0.6643908992409706 test_loss: 0.17833806574344635 test_acc: 0.9295343137254902\n",
      "epoch 683 total_train_acc: 0.9560075685903501 loss: 0.6018961071968079 test_loss: 0.15981435775756836 test_acc: 0.9276960784313726\n",
      "epoch 684 total_train_acc: 0.956244087038789 loss: 0.5884125158190727 test_loss: 0.14876437187194824 test_acc: 0.9319852941176471\n",
      "epoch 685 total_train_acc: 0.9557710501419111 loss: 0.622938483953476 test_loss: 0.19386662542819977 test_acc: 0.9295343137254902\n",
      "epoch 686 total_train_acc: 0.9534058656575213 loss: 0.641961082816124 test_loss: 0.07408933341503143 test_acc: 0.9307598039215687\n",
      "epoch 687 total_train_acc: 0.9578997161778618 loss: 0.6339873597025871 test_loss: 0.4998379051685333 test_acc: 0.9276960784313726\n",
      "epoch 688 total_train_acc: 0.9526963103122044 loss: 0.62942885607481 test_loss: 0.08187194168567657 test_acc: 0.9313725490196079\n",
      "epoch 689 total_train_acc: 0.9505676442762535 loss: 0.6685054525732994 test_loss: 0.3051355481147766 test_acc: 0.928921568627451\n",
      "epoch 690 total_train_acc: 0.956480605487228 loss: 0.6050048395991325 test_loss: 0.27523207664489746 test_acc: 0.9252450980392157\n",
      "epoch 691 total_train_acc: 0.9586092715231788 loss: 0.6509004384279251 test_loss: 0.45428669452667236 test_acc: 0.9332107843137255\n",
      "epoch 692 total_train_acc: 0.9583727530747398 loss: 0.5929615795612335 test_loss: 0.18918147683143616 test_acc: 0.9344362745098039\n",
      "epoch 693 total_train_acc: 0.9531693472090823 loss: 0.6589978486299515 test_loss: 0.5195813775062561 test_acc: 0.9325980392156863\n",
      "epoch 694 total_train_acc: 0.9631031220435194 loss: 0.5776610746979713 test_loss: 0.10799829661846161 test_acc: 0.9350490196078431\n",
      "epoch 695 total_train_acc: 0.9538789025543992 loss: 0.6513433679938316 test_loss: 0.47057509422302246 test_acc: 0.9283088235294118\n",
      "epoch 696 total_train_acc: 0.9534058656575213 loss: 0.6365426108241081 test_loss: 0.06999383121728897 test_acc: 0.9270833333333334\n",
      "epoch 697 total_train_acc: 0.9545884578997161 loss: 0.587826281785965 test_loss: 0.24476122856140137 test_acc: 0.9325980392156863\n",
      "epoch 698 total_train_acc: 0.9560075685903501 loss: 0.657822422683239 test_loss: 0.10655368119478226 test_acc: 0.928921568627451\n",
      "epoch 699 total_train_acc: 0.9576631977294229 loss: 0.6017851456999779 test_loss: 0.22125795483589172 test_acc: 0.9264705882352942\n",
      "epoch 700 total_train_acc: 0.9503311258278145 loss: 0.6415537223219872 test_loss: 0.1396912932395935 test_acc: 0.9325980392156863\n",
      "epoch 701 total_train_acc: 0.9552980132450332 loss: 0.6240784078836441 test_loss: 0.251563161611557 test_acc: 0.9295343137254902\n",
      "epoch 702 total_train_acc: 0.956480605487228 loss: 0.5889259576797485 test_loss: 0.11736970394849777 test_acc: 0.9307598039215687\n",
      "epoch 703 total_train_acc: 0.9531693472090823 loss: 0.6060248166322708 test_loss: 0.2751487195491791 test_acc: 0.9307598039215687\n",
      "epoch 704 total_train_acc: 0.956480605487228 loss: 0.6035155430436134 test_loss: 0.34001827239990234 test_acc: 0.9307598039215687\n",
      "epoch 705 total_train_acc: 0.9576631977294229 loss: 0.6438433602452278 test_loss: 0.15284258127212524 test_acc: 0.9319852941176471\n",
      "epoch 706 total_train_acc: 0.9586092715231788 loss: 0.5662546902894974 test_loss: 0.16443490982055664 test_acc: 0.9313725490196079\n",
      "epoch 707 total_train_acc: 0.956717123935667 loss: 0.630505658686161 test_loss: 0.40053462982177734 test_acc: 0.928921568627451\n",
      "epoch 708 total_train_acc: 0.956480605487228 loss: 0.6155074536800385 test_loss: 0.08068156242370605 test_acc: 0.9295343137254902\n",
      "epoch 709 total_train_acc: 0.9602649006622517 loss: 0.6060682386159897 test_loss: 0.05384180322289467 test_acc: 0.9301470588235294\n",
      "epoch 710 total_train_acc: 0.9581362346263008 loss: 0.6156269907951355 test_loss: 0.024531375616788864 test_acc: 0.9332107843137255\n",
      "epoch 711 total_train_acc: 0.9593188268684958 loss: 0.6077247783541679 test_loss: 0.2459976077079773 test_acc: 0.9295343137254902\n",
      "epoch 712 total_train_acc: 0.9529328287606433 loss: 0.6502292454242706 test_loss: 0.30100247263908386 test_acc: 0.9301470588235294\n",
      "epoch 713 total_train_acc: 0.9574266792809839 loss: 0.5970456972718239 test_loss: 0.2996479868888855 test_acc: 0.9301470588235294\n",
      "epoch 714 total_train_acc: 0.9548249763481551 loss: 0.5906543284654617 test_loss: 0.22816532850265503 test_acc: 0.9319852941176471\n",
      "epoch 715 total_train_acc: 0.956480605487228 loss: 0.606332004070282 test_loss: 0.14979110658168793 test_acc: 0.9313725490196079\n",
      "epoch 716 total_train_acc: 0.9571901608325449 loss: 0.6496096849441528 test_loss: 0.48184025287628174 test_acc: 0.9301470588235294\n",
      "epoch 717 total_train_acc: 0.9526963103122044 loss: 0.6408725678920746 test_loss: 0.10218023508787155 test_acc: 0.9307598039215687\n",
      "epoch 718 total_train_acc: 0.9555345316934721 loss: 0.6211847811937332 test_loss: 0.3537556827068329 test_acc: 0.9283088235294118\n",
      "epoch 719 total_train_acc: 0.9555345316934721 loss: 0.6098194643855095 test_loss: 0.19850707054138184 test_acc: 0.9307598039215687\n",
      "epoch 720 total_train_acc: 0.9560075685903501 loss: 0.5738174319267273 test_loss: 0.2460644543170929 test_acc: 0.9295343137254902\n",
      "epoch 721 total_train_acc: 0.956480605487228 loss: 0.6160061433911324 test_loss: 0.5468823313713074 test_acc: 0.9301470588235294\n",
      "epoch 722 total_train_acc: 0.9576631977294229 loss: 0.5956957563757896 test_loss: 0.16523487865924835 test_acc: 0.9319852941176471\n",
      "epoch 723 total_train_acc: 0.9555345316934721 loss: 0.6119478419423103 test_loss: 0.4568997025489807 test_acc: 0.9338235294117647\n",
      "epoch 724 total_train_acc: 0.9595553453169348 loss: 0.5924059674143791 test_loss: 0.1949491947889328 test_acc: 0.928921568627451\n",
      "epoch 725 total_train_acc: 0.9586092715231788 loss: 0.6131021678447723 test_loss: 0.32570987939834595 test_acc: 0.9338235294117647\n",
      "epoch 726 total_train_acc: 0.956717123935667 loss: 0.6128248497843742 test_loss: 0.2249280959367752 test_acc: 0.9301470588235294\n",
      "epoch 727 total_train_acc: 0.9588457899716177 loss: 0.5736465826630592 test_loss: 0.19940462708473206 test_acc: 0.9276960784313726\n",
      "epoch 728 total_train_acc: 0.9583727530747398 loss: 0.6268840879201889 test_loss: 0.03076537698507309 test_acc: 0.9307598039215687\n",
      "epoch 729 total_train_acc: 0.9555345316934721 loss: 0.602133996784687 test_loss: 0.19059598445892334 test_acc: 0.9295343137254902\n",
      "epoch 730 total_train_acc: 0.956717123935667 loss: 0.6449011862277985 test_loss: 0.058524589985609055 test_acc: 0.9319852941176471\n",
      "epoch 731 total_train_acc: 0.9605014191106906 loss: 0.5634816884994507 test_loss: 0.4518241286277771 test_acc: 0.9270833333333334\n",
      "epoch 732 total_train_acc: 0.9602649006622517 loss: 0.5747967436909676 test_loss: 0.3300744891166687 test_acc: 0.9276960784313726\n",
      "epoch 733 total_train_acc: 0.9609744560075686 loss: 0.5632947981357574 test_loss: 0.23109106719493866 test_acc: 0.9295343137254902\n",
      "epoch 734 total_train_acc: 0.9534058656575213 loss: 0.5898271054029465 test_loss: 0.20778143405914307 test_acc: 0.9295343137254902\n",
      "epoch 735 total_train_acc: 0.9541154210028382 loss: 0.6588433086872101 test_loss: 0.22120854258537292 test_acc: 0.9344362745098039\n",
      "epoch 736 total_train_acc: 0.9541154210028382 loss: 0.605556845664978 test_loss: 0.047912925481796265 test_acc: 0.9313725490196079\n",
      "epoch 737 total_train_acc: 0.9555345316934721 loss: 0.6416338235139847 test_loss: 0.38708123564720154 test_acc: 0.928921568627451\n",
      "epoch 738 total_train_acc: 0.9574266792809839 loss: 0.5901763588190079 test_loss: 0.38008779287338257 test_acc: 0.9313725490196079\n",
      "epoch 739 total_train_acc: 0.956953642384106 loss: 0.6025611758232117 test_loss: 0.1882842779159546 test_acc: 0.9283088235294118\n",
      "epoch 740 total_train_acc: 0.9581362346263008 loss: 0.5894002839922905 test_loss: 0.3573455214500427 test_acc: 0.9313725490196079\n",
      "epoch 741 total_train_acc: 0.9576631977294229 loss: 0.635552279651165 test_loss: 0.2276286631822586 test_acc: 0.9295343137254902\n",
      "epoch 742 total_train_acc: 0.956717123935667 loss: 0.5980199798941612 test_loss: 0.27158641815185547 test_acc: 0.928921568627451\n",
      "epoch 743 total_train_acc: 0.9612109744560076 loss: 0.615248367190361 test_loss: 0.12339233607053757 test_acc: 0.9295343137254902\n",
      "epoch 744 total_train_acc: 0.9614474929044465 loss: 0.5861435458064079 test_loss: 0.19049464166164398 test_acc: 0.9307598039215687\n",
      "epoch 745 total_train_acc: 0.9543519394512772 loss: 0.6114156991243362 test_loss: 0.24839258193969727 test_acc: 0.928921568627451\n",
      "epoch 746 total_train_acc: 0.9588457899716177 loss: 0.609154611825943 test_loss: 0.26845207810401917 test_acc: 0.9301470588235294\n",
      "epoch 747 total_train_acc: 0.9534058656575213 loss: 0.6309100389480591 test_loss: 0.12883108854293823 test_acc: 0.9325980392156863\n",
      "epoch 748 total_train_acc: 0.956480605487228 loss: 0.5828833654522896 test_loss: 0.0730709508061409 test_acc: 0.9270833333333334\n",
      "epoch 749 total_train_acc: 0.9571901608325449 loss: 0.5789547339081764 test_loss: 0.2469773143529892 test_acc: 0.9307598039215687\n",
      "epoch 750 total_train_acc: 0.9586092715231788 loss: 0.5861479565501213 test_loss: 0.28388169407844543 test_acc: 0.9319852941176471\n",
      "epoch 751 total_train_acc: 0.9605014191106906 loss: 0.5833372175693512 test_loss: 0.22066356241703033 test_acc: 0.9313725490196079\n",
      "epoch 752 total_train_acc: 0.9612109744560076 loss: 0.5632448196411133 test_loss: 0.13632076978683472 test_acc: 0.9301470588235294\n",
      "epoch 753 total_train_acc: 0.9635761589403974 loss: 0.5536195710301399 test_loss: 0.20190615952014923 test_acc: 0.9301470588235294\n",
      "epoch 754 total_train_acc: 0.956480605487228 loss: 0.5842682719230652 test_loss: 0.1051563248038292 test_acc: 0.9307598039215687\n",
      "epoch 755 total_train_acc: 0.9560075685903501 loss: 0.5745411962270737 test_loss: 0.24648261070251465 test_acc: 0.9307598039215687\n",
      "epoch 756 total_train_acc: 0.9607379375591296 loss: 0.5329854488372803 test_loss: 0.13661324977874756 test_acc: 0.9307598039215687\n",
      "epoch 757 total_train_acc: 0.9581362346263008 loss: 0.6057513132691383 test_loss: 0.10582377761602402 test_acc: 0.928921568627451\n",
      "epoch 758 total_train_acc: 0.9631031220435194 loss: 0.5371721461415291 test_loss: 0.11384361982345581 test_acc: 0.9307598039215687\n",
      "epoch 759 total_train_acc: 0.9581362346263008 loss: 0.6030462458729744 test_loss: 0.039438288658857346 test_acc: 0.9325980392156863\n",
      "epoch 760 total_train_acc: 0.9595553453169348 loss: 0.6129527315497398 test_loss: 0.21804842352867126 test_acc: 0.9319852941176471\n",
      "epoch 761 total_train_acc: 0.956953642384106 loss: 0.6101199686527252 test_loss: 0.1133459210395813 test_acc: 0.9319852941176471\n",
      "epoch 762 total_train_acc: 0.9581362346263008 loss: 0.5874769687652588 test_loss: 0.20176760852336884 test_acc: 0.9332107843137255\n",
      "epoch 763 total_train_acc: 0.9614474929044465 loss: 0.6142137572169304 test_loss: 0.29803696274757385 test_acc: 0.9319852941176471\n",
      "epoch 764 total_train_acc: 0.9578997161778618 loss: 0.5887348800897598 test_loss: 0.06094755232334137 test_acc: 0.9295343137254902\n",
      "epoch 765 total_train_acc: 0.9631031220435194 loss: 0.5627750530838966 test_loss: 0.2590057849884033 test_acc: 0.9325980392156863\n",
      "epoch 766 total_train_acc: 0.9576631977294229 loss: 0.5902426615357399 test_loss: 0.35278329253196716 test_acc: 0.9332107843137255\n",
      "epoch 767 total_train_acc: 0.9574266792809839 loss: 0.5803527534008026 test_loss: 0.2627815902233124 test_acc: 0.9325980392156863\n",
      "epoch 768 total_train_acc: 0.9574266792809839 loss: 0.5787433683872223 test_loss: 0.18485628068447113 test_acc: 0.9307598039215687\n",
      "epoch 769 total_train_acc: 0.9581362346263008 loss: 0.5441434159874916 test_loss: 0.30376261472702026 test_acc: 0.9295343137254902\n",
      "epoch 770 total_train_acc: 0.9595553453169348 loss: 0.5768455415964127 test_loss: 0.28626900911331177 test_acc: 0.9307598039215687\n",
      "epoch 771 total_train_acc: 0.9576631977294229 loss: 0.6003247871994972 test_loss: 0.2209199219942093 test_acc: 0.9313725490196079\n",
      "epoch 772 total_train_acc: 0.9597918637653737 loss: 0.5705192238092422 test_loss: 0.12042118608951569 test_acc: 0.928921568627451\n",
      "epoch 773 total_train_acc: 0.9578997161778618 loss: 0.593816727399826 test_loss: 0.16022904217243195 test_acc: 0.9332107843137255\n",
      "epoch 774 total_train_acc: 0.9593188268684958 loss: 0.5659938380122185 test_loss: 0.2742021381855011 test_acc: 0.9295343137254902\n",
      "epoch 775 total_train_acc: 0.956244087038789 loss: 0.5909372642636299 test_loss: 0.3438681960105896 test_acc: 0.928921568627451\n",
      "epoch 776 total_train_acc: 0.9571901608325449 loss: 0.5921953842043877 test_loss: 0.15665510296821594 test_acc: 0.9319852941176471\n",
      "epoch 777 total_train_acc: 0.9576631977294229 loss: 0.5983742251992226 test_loss: 0.38417965173721313 test_acc: 0.9325980392156863\n",
      "epoch 778 total_train_acc: 0.9609744560075686 loss: 0.5832253322005272 test_loss: 0.07769066095352173 test_acc: 0.9325980392156863\n",
      "epoch 779 total_train_acc: 0.9583727530747398 loss: 0.6212056279182434 test_loss: 0.08528239279985428 test_acc: 0.9301470588235294\n",
      "epoch 780 total_train_acc: 0.9607379375591296 loss: 0.5754024609923363 test_loss: 0.1356680691242218 test_acc: 0.928921568627451\n",
      "epoch 781 total_train_acc: 0.9576631977294229 loss: 0.5512113198637962 test_loss: 0.036653127521276474 test_acc: 0.9319852941176471\n",
      "epoch 782 total_train_acc: 0.9588457899716177 loss: 0.5696972087025642 test_loss: 0.11002251505851746 test_acc: 0.9301470588235294\n",
      "epoch 783 total_train_acc: 0.9607379375591296 loss: 0.6396354362368584 test_loss: 0.36940693855285645 test_acc: 0.9283088235294118\n",
      "epoch 784 total_train_acc: 0.9614474929044465 loss: 0.5639479011297226 test_loss: 0.3027581572532654 test_acc: 0.9325980392156863\n",
      "epoch 785 total_train_acc: 0.9595553453169348 loss: 0.5760778859257698 test_loss: 0.266475647687912 test_acc: 0.9283088235294118\n",
      "epoch 786 total_train_acc: 0.9595553453169348 loss: 0.546247236430645 test_loss: 0.3959744870662689 test_acc: 0.9301470588235294\n",
      "epoch 787 total_train_acc: 0.9593188268684958 loss: 0.5683340951800346 test_loss: 0.2215067744255066 test_acc: 0.9325980392156863\n",
      "epoch 788 total_train_acc: 0.9578997161778618 loss: 0.5293582379817963 test_loss: 0.07708121091127396 test_acc: 0.9295343137254902\n",
      "epoch 789 total_train_acc: 0.9616840113528855 loss: 0.5495673194527626 test_loss: 0.2641584575176239 test_acc: 0.9301470588235294\n",
      "epoch 790 total_train_acc: 0.9619205298013245 loss: 0.5691560283303261 test_loss: 0.28914278745651245 test_acc: 0.9332107843137255\n",
      "epoch 791 total_train_acc: 0.9612109744560076 loss: 0.5839866325259209 test_loss: 0.06542041152715683 test_acc: 0.9332107843137255\n",
      "epoch 792 total_train_acc: 0.9602649006622517 loss: 0.537609376013279 test_loss: 0.24545760452747345 test_acc: 0.9295343137254902\n",
      "epoch 793 total_train_acc: 0.9612109744560076 loss: 0.5988529399037361 test_loss: 0.14764416217803955 test_acc: 0.9350490196078431\n",
      "epoch 794 total_train_acc: 0.9609744560075686 loss: 0.5752808451652527 test_loss: 0.4584519863128662 test_acc: 0.9325980392156863\n",
      "epoch 795 total_train_acc: 0.9571901608325449 loss: 0.5696321949362755 test_loss: 0.18061672151088715 test_acc: 0.9301470588235294\n",
      "epoch 796 total_train_acc: 0.9602649006622517 loss: 0.5504835918545723 test_loss: 0.09445466101169586 test_acc: 0.9325980392156863\n",
      "epoch 797 total_train_acc: 0.9619205298013245 loss: 0.5562060996890068 test_loss: 0.09616652131080627 test_acc: 0.9319852941176471\n",
      "epoch 798 total_train_acc: 0.9597918637653737 loss: 0.5636268332600594 test_loss: 0.0682588666677475 test_acc: 0.9301470588235294\n",
      "epoch 799 total_train_acc: 0.9614474929044465 loss: 0.5749686509370804 test_loss: 0.2604452669620514 test_acc: 0.9319852941176471\n",
      "epoch 800 total_train_acc: 0.9645222327341533 loss: 0.573492094874382 test_loss: 0.03862534835934639 test_acc: 0.9295343137254902\n",
      "epoch 801 total_train_acc: 0.9619205298013245 loss: 0.5580982267856598 test_loss: 0.1320473998785019 test_acc: 0.9307598039215687\n",
      "epoch 802 total_train_acc: 0.9595553453169348 loss: 0.5169022977352142 test_loss: 0.4910634756088257 test_acc: 0.9307598039215687\n",
      "epoch 803 total_train_acc: 0.9609744560075686 loss: 0.5769454091787338 test_loss: 0.27488967776298523 test_acc: 0.9332107843137255\n",
      "epoch 804 total_train_acc: 0.9621570482497634 loss: 0.5297811850905418 test_loss: 0.32751786708831787 test_acc: 0.9350490196078431\n",
      "epoch 805 total_train_acc: 0.9633396404919584 loss: 0.5365141108632088 test_loss: 0.16261647641658783 test_acc: 0.9356617647058824\n",
      "epoch 806 total_train_acc: 0.9600283822138127 loss: 0.5644506961107254 test_loss: 0.09463681280612946 test_acc: 0.9307598039215687\n",
      "epoch 807 total_train_acc: 0.9614474929044465 loss: 0.5319957956671715 test_loss: 0.22298067808151245 test_acc: 0.9344362745098039\n",
      "epoch 808 total_train_acc: 0.9640491958372753 loss: 0.5492550581693649 test_loss: 0.13982753455638885 test_acc: 0.9338235294117647\n",
      "epoch 809 total_train_acc: 0.956244087038789 loss: 0.5627610459923744 test_loss: 0.06021793559193611 test_acc: 0.9332107843137255\n",
      "epoch 810 total_train_acc: 0.9612109744560076 loss: 0.5462974607944489 test_loss: 0.1673993617296219 test_acc: 0.9295343137254902\n",
      "epoch 811 total_train_acc: 0.9600283822138127 loss: 0.5726143792271614 test_loss: 0.18650470674037933 test_acc: 0.9319852941176471\n",
      "epoch 812 total_train_acc: 0.9619205298013245 loss: 0.5702134147286415 test_loss: 0.36423459649086 test_acc: 0.9338235294117647\n",
      "epoch 813 total_train_acc: 0.9607379375591296 loss: 0.5333693474531174 test_loss: 0.4131906032562256 test_acc: 0.9301470588235294\n",
      "epoch 814 total_train_acc: 0.9614474929044465 loss: 0.5474633947014809 test_loss: 0.10790500044822693 test_acc: 0.9338235294117647\n",
      "epoch 815 total_train_acc: 0.9612109744560076 loss: 0.5367344319820404 test_loss: 0.32268401980400085 test_acc: 0.9332107843137255\n",
      "epoch 816 total_train_acc: 0.9583727530747398 loss: 0.5504102930426598 test_loss: 0.19516043365001678 test_acc: 0.9332107843137255\n",
      "epoch 817 total_train_acc: 0.9583727530747398 loss: 0.5250058099627495 test_loss: 0.19578585028648376 test_acc: 0.9332107843137255\n",
      "epoch 818 total_train_acc: 0.9595553453169348 loss: 0.5557441934943199 test_loss: 0.20458345115184784 test_acc: 0.9332107843137255\n",
      "epoch 819 total_train_acc: 0.9600283822138127 loss: 0.5503543764352798 test_loss: 0.14095626771450043 test_acc: 0.9332107843137255\n",
      "epoch 820 total_train_acc: 0.9638126773888364 loss: 0.5146473348140717 test_loss: 0.1508544236421585 test_acc: 0.9332107843137255\n",
      "epoch 821 total_train_acc: 0.9619205298013245 loss: 0.5068872720003128 test_loss: 0.061543677002191544 test_acc: 0.9325980392156863\n",
      "epoch 822 total_train_acc: 0.9621570482497634 loss: 0.5483091995120049 test_loss: 0.17216934263706207 test_acc: 0.9313725490196079\n",
      "epoch 823 total_train_acc: 0.9635761589403974 loss: 0.5693854987621307 test_loss: 0.16672611236572266 test_acc: 0.9313725490196079\n",
      "epoch 824 total_train_acc: 0.9597918637653737 loss: 0.584598995745182 test_loss: 0.16470664739608765 test_acc: 0.9319852941176471\n",
      "epoch 825 total_train_acc: 0.9595553453169348 loss: 0.5398064479231834 test_loss: 0.2725626826286316 test_acc: 0.9338235294117647\n",
      "epoch 826 total_train_acc: 0.9638126773888364 loss: 0.5584360510110855 test_loss: 0.15799136459827423 test_acc: 0.9325980392156863\n",
      "epoch 827 total_train_acc: 0.956244087038789 loss: 0.5790166035294533 test_loss: 0.22126257419586182 test_acc: 0.9295343137254902\n",
      "epoch 828 total_train_acc: 0.9574266792809839 loss: 0.5749127492308617 test_loss: 0.11187972873449326 test_acc: 0.9301470588235294\n",
      "epoch 829 total_train_acc: 0.9616840113528855 loss: 0.5414365604519844 test_loss: 0.40614256262779236 test_acc: 0.9307598039215687\n",
      "epoch 830 total_train_acc: 0.9626300851466414 loss: 0.5344992727041245 test_loss: 0.36154043674468994 test_acc: 0.9319852941176471\n",
      "epoch 831 total_train_acc: 0.9609744560075686 loss: 0.5288536176085472 test_loss: 0.4279033839702606 test_acc: 0.9332107843137255\n",
      "epoch 832 total_train_acc: 0.9635761589403974 loss: 0.5549968928098679 test_loss: 0.41609296202659607 test_acc: 0.9325980392156863\n",
      "epoch 833 total_train_acc: 0.9619205298013245 loss: 0.48664920404553413 test_loss: 0.37403517961502075 test_acc: 0.9356617647058824\n",
      "epoch 834 total_train_acc: 0.9619205298013245 loss: 0.5181367471814156 test_loss: 0.15209931135177612 test_acc: 0.9307598039215687\n",
      "epoch 835 total_train_acc: 0.9605014191106906 loss: 0.5202255621552467 test_loss: 0.07972247153520584 test_acc: 0.9301470588235294\n",
      "epoch 836 total_train_acc: 0.9621570482497634 loss: 0.5645709708333015 test_loss: 0.10499750077724457 test_acc: 0.9313725490196079\n",
      "epoch 837 total_train_acc: 0.9645222327341533 loss: 0.5598668456077576 test_loss: 0.27744951844215393 test_acc: 0.9301470588235294\n",
      "epoch 838 total_train_acc: 0.9614474929044465 loss: 0.5505146980285645 test_loss: 0.27857834100723267 test_acc: 0.9325980392156863\n",
      "epoch 839 total_train_acc: 0.9619205298013245 loss: 0.5505061224102974 test_loss: 0.3499913215637207 test_acc: 0.9319852941176471\n",
      "epoch 840 total_train_acc: 0.9593188268684958 loss: 0.5440923646092415 test_loss: 0.29095008969306946 test_acc: 0.9301470588235294\n",
      "epoch 841 total_train_acc: 0.9614474929044465 loss: 0.5928793027997017 test_loss: 0.07471979409456253 test_acc: 0.9313725490196079\n",
      "epoch 842 total_train_acc: 0.9626300851466414 loss: 0.5354672595858574 test_loss: 0.29928576946258545 test_acc: 0.9338235294117647\n",
      "epoch 843 total_train_acc: 0.9593188268684958 loss: 0.51069375872612 test_loss: 0.3804188370704651 test_acc: 0.9319852941176471\n",
      "epoch 844 total_train_acc: 0.9626300851466414 loss: 0.5242655202746391 test_loss: 0.34577512741088867 test_acc: 0.9276960784313726\n",
      "epoch 845 total_train_acc: 0.9581362346263008 loss: 0.5774902030825615 test_loss: 0.28664007782936096 test_acc: 0.9350490196078431\n",
      "epoch 846 total_train_acc: 0.9621570482497634 loss: 0.5343800410628319 test_loss: 0.2510254979133606 test_acc: 0.9356617647058824\n",
      "epoch 847 total_train_acc: 0.9600283822138127 loss: 0.5926693677902222 test_loss: 0.280779629945755 test_acc: 0.9307598039215687\n",
      "epoch 848 total_train_acc: 0.9621570482497634 loss: 0.602039061486721 test_loss: 0.2824668288230896 test_acc: 0.9313725490196079\n",
      "epoch 849 total_train_acc: 0.9649952696310312 loss: 0.5114932954311371 test_loss: 0.17396894097328186 test_acc: 0.9307598039215687\n",
      "epoch 850 total_train_acc: 0.966414380321665 loss: 0.5343530476093292 test_loss: 0.1385144144296646 test_acc: 0.9319852941176471\n",
      "epoch 851 total_train_acc: 0.9590823084200568 loss: 0.5601238012313843 test_loss: 0.1780095249414444 test_acc: 0.9344362745098039\n",
      "epoch 852 total_train_acc: 0.9612109744560076 loss: 0.5364701449871063 test_loss: 0.12341530621051788 test_acc: 0.9325980392156863\n",
      "epoch 853 total_train_acc: 0.9609744560075686 loss: 0.5551093891263008 test_loss: 0.16195382177829742 test_acc: 0.9313725490196079\n",
      "epoch 854 total_train_acc: 0.9638126773888364 loss: 0.5358948409557343 test_loss: 0.4398006796836853 test_acc: 0.9350490196078431\n",
      "epoch 855 total_train_acc: 0.9607379375591296 loss: 0.55691097676754 test_loss: 0.10220512747764587 test_acc: 0.9338235294117647\n",
      "epoch 856 total_train_acc: 0.9619205298013245 loss: 0.5350905954837799 test_loss: 0.0986529067158699 test_acc: 0.9350490196078431\n",
      "epoch 857 total_train_acc: 0.9621570482497634 loss: 0.5737314224243164 test_loss: 0.46967408061027527 test_acc: 0.9338235294117647\n",
      "epoch 858 total_train_acc: 0.9631031220435194 loss: 0.5403572246432304 test_loss: 0.16669195890426636 test_acc: 0.9307598039215687\n",
      "epoch 859 total_train_acc: 0.9614474929044465 loss: 0.5517786368727684 test_loss: 0.0885820984840393 test_acc: 0.9313725490196079\n",
      "epoch 860 total_train_acc: 0.9652317880794702 loss: 0.5421983301639557 test_loss: 0.29968351125717163 test_acc: 0.9338235294117647\n",
      "epoch 861 total_train_acc: 0.9626300851466414 loss: 0.5299877598881721 test_loss: 0.010719957761466503 test_acc: 0.9307598039215687\n",
      "epoch 862 total_train_acc: 0.9612109744560076 loss: 0.575326606631279 test_loss: 0.08264034241437912 test_acc: 0.9301470588235294\n",
      "epoch 863 total_train_acc: 0.9633396404919584 loss: 0.49797532707452774 test_loss: 0.4483908712863922 test_acc: 0.9338235294117647\n",
      "epoch 864 total_train_acc: 0.9609744560075686 loss: 0.5059138685464859 test_loss: 0.15748551487922668 test_acc: 0.9350490196078431\n",
      "epoch 865 total_train_acc: 0.9607379375591296 loss: 0.5246315523982048 test_loss: 0.19730545580387115 test_acc: 0.9356617647058824\n",
      "epoch 866 total_train_acc: 0.9623935666982024 loss: 0.5253939479589462 test_loss: 0.3563584089279175 test_acc: 0.9313725490196079\n",
      "epoch 867 total_train_acc: 0.9626300851466414 loss: 0.5280508771538734 test_loss: 0.3501039743423462 test_acc: 0.9295343137254902\n",
      "epoch 868 total_train_acc: 0.9600283822138127 loss: 0.5175655707716942 test_loss: 0.17074033617973328 test_acc: 0.9307598039215687\n",
      "epoch 869 total_train_acc: 0.9633396404919584 loss: 0.5577367097139359 test_loss: 0.08133313059806824 test_acc: 0.9362745098039216\n",
      "epoch 870 total_train_acc: 0.9616840113528855 loss: 0.5610193014144897 test_loss: 0.26248568296432495 test_acc: 0.9332107843137255\n",
      "epoch 871 total_train_acc: 0.9631031220435194 loss: 0.5375565886497498 test_loss: 0.20538444817066193 test_acc: 0.9319852941176471\n",
      "epoch 872 total_train_acc: 0.9614474929044465 loss: 0.5262282118201256 test_loss: 0.2745647728443146 test_acc: 0.9313725490196079\n",
      "epoch 873 total_train_acc: 0.9647587511825922 loss: 0.498539961874485 test_loss: 0.33842933177948 test_acc: 0.9344362745098039\n",
      "epoch 874 total_train_acc: 0.9616840113528855 loss: 0.5540685132145882 test_loss: 0.11941472440958023 test_acc: 0.9301470588235294\n",
      "epoch 875 total_train_acc: 0.9628666035950804 loss: 0.5247111544013023 test_loss: 0.07460744678974152 test_acc: 0.9283088235294118\n",
      "epoch 876 total_train_acc: 0.9657048249763481 loss: 0.5281785279512405 test_loss: 0.16072750091552734 test_acc: 0.9319852941176471\n",
      "epoch 877 total_train_acc: 0.9635761589403974 loss: 0.5225233733654022 test_loss: 0.3322855830192566 test_acc: 0.9307598039215687\n",
      "epoch 878 total_train_acc: 0.9607379375591296 loss: 0.562499925494194 test_loss: 0.13846582174301147 test_acc: 0.9295343137254902\n",
      "epoch 879 total_train_acc: 0.9642857142857143 loss: 0.5091298967599869 test_loss: 0.4292488992214203 test_acc: 0.9350490196078431\n",
      "epoch 880 total_train_acc: 0.9645222327341533 loss: 0.5118190720677376 test_loss: 0.3444334864616394 test_acc: 0.9350490196078431\n",
      "epoch 881 total_train_acc: 0.9600283822138127 loss: 0.5809119492769241 test_loss: 0.12622343003749847 test_acc: 0.9319852941176471\n",
      "epoch 882 total_train_acc: 0.9621570482497634 loss: 0.5466770231723785 test_loss: 0.41456952691078186 test_acc: 0.9338235294117647\n",
      "epoch 883 total_train_acc: 0.9661778618732261 loss: 0.551556795835495 test_loss: 0.23155349493026733 test_acc: 0.9344362745098039\n",
      "epoch 884 total_train_acc: 0.9621570482497634 loss: 0.5996341034770012 test_loss: 0.04742009937763214 test_acc: 0.9301470588235294\n",
      "epoch 885 total_train_acc: 0.9621570482497634 loss: 0.5463801994919777 test_loss: 0.09354382008314133 test_acc: 0.9264705882352942\n",
      "epoch 886 total_train_acc: 0.9626300851466414 loss: 0.5221521183848381 test_loss: 0.1308920532464981 test_acc: 0.9319852941176471\n",
      "epoch 887 total_train_acc: 0.9605014191106906 loss: 0.534159243106842 test_loss: 0.266578733921051 test_acc: 0.9325980392156863\n",
      "epoch 888 total_train_acc: 0.9621570482497634 loss: 0.5260214358568192 test_loss: 0.5246701240539551 test_acc: 0.9325980392156863\n",
      "epoch 889 total_train_acc: 0.9642857142857143 loss: 0.5253476500511169 test_loss: 0.12931795418262482 test_acc: 0.9307598039215687\n",
      "epoch 890 total_train_acc: 0.9647587511825922 loss: 0.529276467859745 test_loss: 0.5818244218826294 test_acc: 0.9313725490196079\n",
      "epoch 891 total_train_acc: 0.9600283822138127 loss: 0.5302779376506805 test_loss: 0.3623521625995636 test_acc: 0.9276960784313726\n",
      "epoch 892 total_train_acc: 0.9654683065279092 loss: 0.5256905853748322 test_loss: 0.19003279507160187 test_acc: 0.9338235294117647\n",
      "epoch 893 total_train_acc: 0.9640491958372753 loss: 0.5110660865902901 test_loss: 0.11013894528150558 test_acc: 0.9350490196078431\n",
      "epoch 894 total_train_acc: 0.9638126773888364 loss: 0.5176147222518921 test_loss: 0.29658982157707214 test_acc: 0.9319852941176471\n",
      "epoch 895 total_train_acc: 0.9614474929044465 loss: 0.534097395837307 test_loss: 0.1057608500123024 test_acc: 0.9301470588235294\n",
      "epoch 896 total_train_acc: 0.9633396404919584 loss: 0.4916829690337181 test_loss: 0.27497273683547974 test_acc: 0.9319852941176471\n",
      "epoch 897 total_train_acc: 0.966414380321665 loss: 0.4919235557317734 test_loss: 0.30891233682632446 test_acc: 0.9344362745098039\n",
      "epoch 898 total_train_acc: 0.9602649006622517 loss: 0.5483672246336937 test_loss: 0.20571516454219818 test_acc: 0.9325980392156863\n",
      "epoch 899 total_train_acc: 0.9635761589403974 loss: 0.5423218607902527 test_loss: 0.10118460655212402 test_acc: 0.9319852941176471\n",
      "epoch 900 total_train_acc: 0.9640491958372753 loss: 0.4959406554698944 test_loss: 0.06253938376903534 test_acc: 0.9313725490196079\n",
      "epoch 901 total_train_acc: 0.9621570482497634 loss: 0.49701012670993805 test_loss: 0.2565230131149292 test_acc: 0.9319852941176471\n",
      "epoch 902 total_train_acc: 0.9642857142857143 loss: 0.541762575507164 test_loss: 0.3679552376270294 test_acc: 0.9325980392156863\n",
      "epoch 903 total_train_acc: 0.9586092715231788 loss: 0.5317179635167122 test_loss: 0.20783819258213043 test_acc: 0.9338235294117647\n",
      "epoch 904 total_train_acc: 0.9640491958372753 loss: 0.5012095719575882 test_loss: 0.11441151797771454 test_acc: 0.9264705882352942\n",
      "epoch 905 total_train_acc: 0.9635761589403974 loss: 0.5459854602813721 test_loss: 0.23778767883777618 test_acc: 0.928921568627451\n",
      "epoch 906 total_train_acc: 0.9631031220435194 loss: 0.5112348645925522 test_loss: 0.3196219205856323 test_acc: 0.9325980392156863\n",
      "epoch 907 total_train_acc: 0.9631031220435194 loss: 0.5101142823696136 test_loss: 0.38485389947891235 test_acc: 0.9325980392156863\n",
      "epoch 908 total_train_acc: 0.9661778618732261 loss: 0.5397584736347198 test_loss: 0.13929840922355652 test_acc: 0.9313725490196079\n",
      "epoch 909 total_train_acc: 0.9657048249763481 loss: 0.4748144671320915 test_loss: 0.24393059313297272 test_acc: 0.9283088235294118\n",
      "epoch 910 total_train_acc: 0.9631031220435194 loss: 0.5572365000844002 test_loss: 0.24603727459907532 test_acc: 0.9344362745098039\n",
      "epoch 911 total_train_acc: 0.9588457899716177 loss: 0.528846949338913 test_loss: 0.20591869950294495 test_acc: 0.9307598039215687\n",
      "epoch 912 total_train_acc: 0.9635761589403974 loss: 0.5012253299355507 test_loss: 0.1898023933172226 test_acc: 0.9313725490196079\n",
      "epoch 913 total_train_acc: 0.9640491958372753 loss: 0.5251740366220474 test_loss: 0.23390798270702362 test_acc: 0.9307598039215687\n",
      "epoch 914 total_train_acc: 0.9642857142857143 loss: 0.4855996370315552 test_loss: 0.32772836089134216 test_acc: 0.9301470588235294\n",
      "epoch 915 total_train_acc: 0.9616840113528855 loss: 0.49424705654382706 test_loss: 0.3150571882724762 test_acc: 0.9350490196078431\n",
      "epoch 916 total_train_acc: 0.9626300851466414 loss: 0.47057267278432846 test_loss: 0.24261318147182465 test_acc: 0.9368872549019608\n",
      "epoch 917 total_train_acc: 0.9623935666982024 loss: 0.5146864503622055 test_loss: 0.3320358097553253 test_acc: 0.9356617647058824\n",
      "epoch 918 total_train_acc: 0.9619205298013245 loss: 0.5192963033914566 test_loss: 0.034431975334882736 test_acc: 0.9362745098039216\n",
      "epoch 919 total_train_acc: 0.9687795648060549 loss: 0.49506284296512604 test_loss: 0.1859143078327179 test_acc: 0.9362745098039216\n",
      "epoch 920 total_train_acc: 0.9671239356669821 loss: 0.4992731884121895 test_loss: 0.46061626076698303 test_acc: 0.9350490196078431\n",
      "epoch 921 total_train_acc: 0.9668874172185431 loss: 0.48235008120536804 test_loss: 0.13840676844120026 test_acc: 0.9313725490196079\n",
      "epoch 922 total_train_acc: 0.9642857142857143 loss: 0.5349498689174652 test_loss: 0.02181687019765377 test_acc: 0.9313725490196079\n",
      "epoch 923 total_train_acc: 0.9635761589403974 loss: 0.5141690373420715 test_loss: 0.1845807135105133 test_acc: 0.9332107843137255\n",
      "epoch 924 total_train_acc: 0.9647587511825922 loss: 0.48974842578172684 test_loss: 0.06121230870485306 test_acc: 0.9338235294117647\n",
      "epoch 925 total_train_acc: 0.966414380321665 loss: 0.5359406992793083 test_loss: 0.2283477634191513 test_acc: 0.9313725490196079\n",
      "epoch 926 total_train_acc: 0.9654683065279092 loss: 0.5133420750498772 test_loss: 0.2550179362297058 test_acc: 0.9319852941176471\n",
      "epoch 927 total_train_acc: 0.9628666035950804 loss: 0.5096536055207253 test_loss: 0.11213364452123642 test_acc: 0.9307598039215687\n",
      "epoch 928 total_train_acc: 0.9654683065279092 loss: 0.4920603856444359 test_loss: 0.4389931857585907 test_acc: 0.9307598039215687\n",
      "epoch 929 total_train_acc: 0.9607379375591296 loss: 0.4874946251511574 test_loss: 0.363695353269577 test_acc: 0.9332107843137255\n",
      "epoch 930 total_train_acc: 0.9638126773888364 loss: 0.4987175613641739 test_loss: 0.2829652428627014 test_acc: 0.9362745098039216\n",
      "epoch 931 total_train_acc: 0.9628666035950804 loss: 0.4859379976987839 test_loss: 0.16280221939086914 test_acc: 0.9325980392156863\n",
      "epoch 932 total_train_acc: 0.9633396404919584 loss: 0.5320688188076019 test_loss: 0.0791720449924469 test_acc: 0.9313725490196079\n",
      "epoch 933 total_train_acc: 0.9654683065279092 loss: 0.5184660330414772 test_loss: 0.07083410769701004 test_acc: 0.9313725490196079\n",
      "epoch 934 total_train_acc: 0.9638126773888364 loss: 0.5162883326411247 test_loss: 0.17151406407356262 test_acc: 0.9313725490196079\n",
      "epoch 935 total_train_acc: 0.9628666035950804 loss: 0.5255813375115395 test_loss: 0.3828805983066559 test_acc: 0.9313725490196079\n",
      "epoch 936 total_train_acc: 0.9616840113528855 loss: 0.4913913309574127 test_loss: 0.22187815606594086 test_acc: 0.9301470588235294\n",
      "epoch 937 total_train_acc: 0.966414380321665 loss: 0.4952552244067192 test_loss: 0.19880026578903198 test_acc: 0.9313725490196079\n",
      "epoch 938 total_train_acc: 0.9633396404919584 loss: 0.5181887149810791 test_loss: 0.07758819311857224 test_acc: 0.9338235294117647\n",
      "epoch 939 total_train_acc: 0.9635761589403974 loss: 0.5225798115134239 test_loss: 0.19878125190734863 test_acc: 0.9325980392156863\n",
      "epoch 940 total_train_acc: 0.967360454115421 loss: 0.5226933434605598 test_loss: 0.15926237404346466 test_acc: 0.9344362745098039\n",
      "epoch 941 total_train_acc: 0.9642857142857143 loss: 0.49525610357522964 test_loss: 0.0561964251101017 test_acc: 0.9368872549019608\n",
      "epoch 942 total_train_acc: 0.9628666035950804 loss: 0.4742082878947258 test_loss: 0.27385812997817993 test_acc: 0.9301470588235294\n",
      "epoch 943 total_train_acc: 0.9628666035950804 loss: 0.5113353729248047 test_loss: 0.16387420892715454 test_acc: 0.9307598039215687\n",
      "epoch 944 total_train_acc: 0.9659413434247871 loss: 0.468869224190712 test_loss: 0.10149089992046356 test_acc: 0.9332107843137255\n",
      "epoch 945 total_train_acc: 0.9640491958372753 loss: 0.4994542896747589 test_loss: 0.1042240709066391 test_acc: 0.9338235294117647\n",
      "epoch 946 total_train_acc: 0.9647587511825922 loss: 0.4879194125533104 test_loss: 0.3599550127983093 test_acc: 0.9350490196078431\n",
      "epoch 947 total_train_acc: 0.966414380321665 loss: 0.4746975526213646 test_loss: 0.28090688586235046 test_acc: 0.9295343137254902\n",
      "epoch 948 total_train_acc: 0.9659413434247871 loss: 0.48876482993364334 test_loss: 0.11940830945968628 test_acc: 0.928921568627451\n",
      "epoch 949 total_train_acc: 0.9652317880794702 loss: 0.5266955345869064 test_loss: 0.14366263151168823 test_acc: 0.9338235294117647\n",
      "epoch 950 total_train_acc: 0.9635761589403974 loss: 0.4799037203192711 test_loss: 0.3808262348175049 test_acc: 0.9319852941176471\n",
      "epoch 951 total_train_acc: 0.9628666035950804 loss: 0.5093602314591408 test_loss: 0.22781556844711304 test_acc: 0.9325980392156863\n",
      "epoch 952 total_train_acc: 0.9642857142857143 loss: 0.5078765973448753 test_loss: 0.20672807097434998 test_acc: 0.9319852941176471\n",
      "epoch 953 total_train_acc: 0.9671239356669821 loss: 0.4678180441260338 test_loss: 0.3581409752368927 test_acc: 0.9332107843137255\n",
      "epoch 954 total_train_acc: 0.9623935666982024 loss: 0.5379228740930557 test_loss: 0.031077027320861816 test_acc: 0.9319852941176471\n",
      "epoch 955 total_train_acc: 0.9621570482497634 loss: 0.491911917924881 test_loss: 0.23946987092494965 test_acc: 0.9325980392156863\n",
      "epoch 956 total_train_acc: 0.9645222327341533 loss: 0.5136407613754272 test_loss: 0.27057936787605286 test_acc: 0.9313725490196079\n",
      "epoch 957 total_train_acc: 0.9645222327341533 loss: 0.516213484108448 test_loss: 0.25277864933013916 test_acc: 0.9332107843137255\n",
      "epoch 958 total_train_acc: 0.9626300851466414 loss: 0.5098563805222511 test_loss: 0.25391900539398193 test_acc: 0.9344362745098039\n",
      "epoch 959 total_train_acc: 0.9657048249763481 loss: 0.5281712487339973 test_loss: 0.10944460332393646 test_acc: 0.9350490196078431\n",
      "epoch 960 total_train_acc: 0.9645222327341533 loss: 0.5184566080570221 test_loss: 0.02187427319586277 test_acc: 0.9319852941176471\n",
      "epoch 961 total_train_acc: 0.9645222327341533 loss: 0.47925421595573425 test_loss: 0.14651818573474884 test_acc: 0.9319852941176471\n",
      "epoch 962 total_train_acc: 0.9600283822138127 loss: 0.5169403403997421 test_loss: 0.20869672298431396 test_acc: 0.9319852941176471\n",
      "epoch 963 total_train_acc: 0.9619205298013245 loss: 0.49545968323946 test_loss: 0.16644635796546936 test_acc: 0.9295343137254902\n",
      "epoch 964 total_train_acc: 0.9661778618732261 loss: 0.4917144253849983 test_loss: 0.13162492215633392 test_acc: 0.9295343137254902\n",
      "epoch 965 total_train_acc: 0.9638126773888364 loss: 0.49819397926330566 test_loss: 0.3094537556171417 test_acc: 0.9350490196078431\n",
      "epoch 966 total_train_acc: 0.9621570482497634 loss: 0.5712929591536522 test_loss: 0.06983886659145355 test_acc: 0.9356617647058824\n",
      "epoch 967 total_train_acc: 0.9642857142857143 loss: 0.5940572842955589 test_loss: 0.4609133005142212 test_acc: 0.9332107843137255\n",
      "epoch 968 total_train_acc: 0.9671239356669821 loss: 0.49305953830480576 test_loss: 0.20510630309581757 test_acc: 0.9319852941176471\n",
      "epoch 969 total_train_acc: 0.9642857142857143 loss: 0.5043722465634346 test_loss: 0.17965877056121826 test_acc: 0.9332107843137255\n",
      "epoch 970 total_train_acc: 0.9654683065279092 loss: 0.49467360228300095 test_loss: 0.1776139736175537 test_acc: 0.9313725490196079\n",
      "epoch 971 total_train_acc: 0.9616840113528855 loss: 0.5699029415845871 test_loss: 0.21994000673294067 test_acc: 0.9313725490196079\n",
      "epoch 972 total_train_acc: 0.9642857142857143 loss: 0.5137020796537399 test_loss: 0.2968035042285919 test_acc: 0.9368872549019608\n",
      "epoch 973 total_train_acc: 0.9668874172185431 loss: 0.49679718166589737 test_loss: 0.1365675926208496 test_acc: 0.9362745098039216\n",
      "epoch 974 total_train_acc: 0.9638126773888364 loss: 0.5322783440351486 test_loss: 0.08945952355861664 test_acc: 0.9344362745098039\n",
      "epoch 975 total_train_acc: 0.9612109744560076 loss: 0.5074162185192108 test_loss: 0.014475381001830101 test_acc: 0.9313725490196079\n",
      "epoch 976 total_train_acc: 0.966414380321665 loss: 0.48201409727334976 test_loss: 0.3436497151851654 test_acc: 0.9338235294117647\n",
      "epoch 977 total_train_acc: 0.9626300851466414 loss: 0.5018132254481316 test_loss: 0.1276540458202362 test_acc: 0.9362745098039216\n",
      "epoch 978 total_train_acc: 0.9657048249763481 loss: 0.49358293414115906 test_loss: 0.2848222851753235 test_acc: 0.9332107843137255\n",
      "epoch 979 total_train_acc: 0.9635761589403974 loss: 0.5039053410291672 test_loss: 0.16712243854999542 test_acc: 0.9332107843137255\n",
      "epoch 980 total_train_acc: 0.9631031220435194 loss: 0.5120039507746696 test_loss: 0.1953440010547638 test_acc: 0.9319852941176471\n",
      "epoch 981 total_train_acc: 0.9652317880794702 loss: 0.49383846670389175 test_loss: 0.27380090951919556 test_acc: 0.9350490196078431\n",
      "epoch 982 total_train_acc: 0.9607379375591296 loss: 0.5083002895116806 test_loss: 0.2539726793766022 test_acc: 0.9356617647058824\n",
      "epoch 983 total_train_acc: 0.9647587511825922 loss: 0.5008984133601189 test_loss: 0.382933646440506 test_acc: 0.9350490196078431\n",
      "epoch 984 total_train_acc: 0.9638126773888364 loss: 0.494744174182415 test_loss: 0.4244076907634735 test_acc: 0.9350490196078431\n",
      "epoch 985 total_train_acc: 0.96759697256386 loss: 0.47160158306360245 test_loss: 0.16622300446033478 test_acc: 0.9332107843137255\n",
      "epoch 986 total_train_acc: 0.9633396404919584 loss: 0.5298833027482033 test_loss: 0.3357791006565094 test_acc: 0.9356617647058824\n",
      "epoch 987 total_train_acc: 0.9659413434247871 loss: 0.489385761320591 test_loss: 0.0792224332690239 test_acc: 0.9344362745098039\n",
      "epoch 988 total_train_acc: 0.9654683065279092 loss: 0.5322592854499817 test_loss: 0.5343760848045349 test_acc: 0.9350490196078431\n",
      "epoch 989 total_train_acc: 0.9638126773888364 loss: 0.4983968660235405 test_loss: 0.05604209378361702 test_acc: 0.9319852941176471\n",
      "epoch 990 total_train_acc: 0.9668874172185431 loss: 0.5007227957248688 test_loss: 0.16432718932628632 test_acc: 0.9332107843137255\n",
      "epoch 991 total_train_acc: 0.9659413434247871 loss: 0.5301404297351837 test_loss: 0.08407557755708694 test_acc: 0.9313725490196079\n",
      "epoch 992 total_train_acc: 0.9626300851466414 loss: 0.47972607612609863 test_loss: 0.265541672706604 test_acc: 0.9338235294117647\n",
      "epoch 993 total_train_acc: 0.9661778618732261 loss: 0.48210349678993225 test_loss: 0.29639920592308044 test_acc: 0.9338235294117647\n",
      "epoch 994 total_train_acc: 0.9612109744560076 loss: 0.5074864774942398 test_loss: 0.4981856942176819 test_acc: 0.9313725490196079\n",
      "epoch 995 total_train_acc: 0.9647587511825922 loss: 0.4982858672738075 test_loss: 0.20313365757465363 test_acc: 0.9307598039215687\n",
      "epoch 996 total_train_acc: 0.967360454115421 loss: 0.4692293033003807 test_loss: 0.07642686367034912 test_acc: 0.9344362745098039\n",
      "epoch 997 total_train_acc: 0.9668874172185431 loss: 0.4974416196346283 test_loss: 0.15026915073394775 test_acc: 0.9362745098039216\n",
      "epoch 998 total_train_acc: 0.9647587511825922 loss: 0.49028320610523224 test_loss: 0.28134819865226746 test_acc: 0.9344362745098039\n",
      "epoch 999 total_train_acc: 0.9619205298013245 loss: 0.4974692612886429 test_loss: 0.06784290820360184 test_acc: 0.9325980392156863\n",
      "epoch 1000 total_train_acc: 0.9642857142857143 loss: 0.5168521776795387 test_loss: 0.17293716967105865 test_acc: 0.9338235294117647\n",
      "epoch 1001 total_train_acc: 0.9623935666982024 loss: 0.5113702043890953 test_loss: 0.08648523688316345 test_acc: 0.9325980392156863\n",
      "epoch 1002 total_train_acc: 0.9621570482497634 loss: 0.5015677511692047 test_loss: 0.2058434635400772 test_acc: 0.9283088235294118\n",
      "epoch 1003 total_train_acc: 0.9645222327341533 loss: 0.47745998948812485 test_loss: 0.32974961400032043 test_acc: 0.9313725490196079\n",
      "epoch 1004 total_train_acc: 0.9638126773888364 loss: 0.49794620275497437 test_loss: 0.16495844721794128 test_acc: 0.9313725490196079\n",
      "epoch 1005 total_train_acc: 0.9671239356669821 loss: 0.48320814967155457 test_loss: 0.15724359452724457 test_acc: 0.9313725490196079\n",
      "epoch 1006 total_train_acc: 0.966414380321665 loss: 0.4633113816380501 test_loss: 0.0806872770190239 test_acc: 0.9338235294117647\n",
      "epoch 1007 total_train_acc: 0.966414380321665 loss: 0.5059966370463371 test_loss: 0.07004936784505844 test_acc: 0.9350490196078431\n",
      "epoch 1008 total_train_acc: 0.9659413434247871 loss: 0.48288821429014206 test_loss: 0.22696584463119507 test_acc: 0.9344362745098039\n",
      "epoch 1009 total_train_acc: 0.9640491958372753 loss: 0.5192379951477051 test_loss: 0.23400680720806122 test_acc: 0.9350490196078431\n",
      "epoch 1010 total_train_acc: 0.9638126773888364 loss: 0.5148798450827599 test_loss: 0.3706167936325073 test_acc: 0.9350490196078431\n",
      "epoch 1011 total_train_acc: 0.9647587511825922 loss: 0.4742750748991966 test_loss: 0.12343847006559372 test_acc: 0.9350490196078431\n",
      "epoch 1012 total_train_acc: 0.9616840113528855 loss: 0.5235859602689743 test_loss: 0.4713446795940399 test_acc: 0.9332107843137255\n",
      "epoch 1013 total_train_acc: 0.9671239356669821 loss: 0.464714452624321 test_loss: 0.1010332852602005 test_acc: 0.9325980392156863\n",
      "epoch 1014 total_train_acc: 0.9652317880794702 loss: 0.5550008565187454 test_loss: 0.11473893374204636 test_acc: 0.9344362745098039\n",
      "epoch 1015 total_train_acc: 0.9647587511825922 loss: 0.5263878926634789 test_loss: 0.18137812614440918 test_acc: 0.9332107843137255\n",
      "epoch 1016 total_train_acc: 0.9635761589403974 loss: 0.5167633891105652 test_loss: 0.04828372970223427 test_acc: 0.9307598039215687\n",
      "epoch 1017 total_train_acc: 0.9609744560075686 loss: 0.5260757207870483 test_loss: 0.19345496594905853 test_acc: 0.9307598039215687\n",
      "epoch 1018 total_train_acc: 0.96759697256386 loss: 0.4829083979129791 test_loss: 0.2399144172668457 test_acc: 0.9344362745098039\n",
      "epoch 1019 total_train_acc: 0.9654683065279092 loss: 0.4964617043733597 test_loss: 0.579161524772644 test_acc: 0.9362745098039216\n",
      "epoch 1020 total_train_acc: 0.9638126773888364 loss: 0.5109257102012634 test_loss: 0.18114763498306274 test_acc: 0.9325980392156863\n",
      "epoch 1021 total_train_acc: 0.9659413434247871 loss: 0.48773331195116043 test_loss: 0.1698664128780365 test_acc: 0.9325980392156863\n",
      "epoch 1022 total_train_acc: 0.9619205298013245 loss: 0.5004876777529716 test_loss: 0.1616656631231308 test_acc: 0.9344362745098039\n",
      "epoch 1023 total_train_acc: 0.9607379375591296 loss: 0.5082731917500496 test_loss: 0.1336863785982132 test_acc: 0.9350490196078431\n",
      "epoch 1024 total_train_acc: 0.9661778618732261 loss: 0.47308358550071716 test_loss: 0.26662907004356384 test_acc: 0.9338235294117647\n",
      "epoch 1025 total_train_acc: 0.966650898770104 loss: 0.48543211817741394 test_loss: 0.12500767409801483 test_acc: 0.9325980392156863\n",
      "epoch 1026 total_train_acc: 0.9661778618732261 loss: 0.45451470464468 test_loss: 0.09516008198261261 test_acc: 0.9344362745098039\n",
      "epoch 1027 total_train_acc: 0.9635761589403974 loss: 0.5088532865047455 test_loss: 0.11987637728452682 test_acc: 0.9350490196078431\n",
      "epoch 1028 total_train_acc: 0.9654683065279092 loss: 0.45622818917036057 test_loss: 0.45286983251571655 test_acc: 0.9338235294117647\n",
      "epoch 1029 total_train_acc: 0.9652317880794702 loss: 0.4802323132753372 test_loss: 0.4570833742618561 test_acc: 0.9301470588235294\n",
      "epoch 1030 total_train_acc: 0.9661778618732261 loss: 0.4614563211798668 test_loss: 0.2476092427968979 test_acc: 0.9313725490196079\n",
      "epoch 1031 total_train_acc: 0.9635761589403974 loss: 0.5416689142584801 test_loss: 0.3121269643306732 test_acc: 0.9356617647058824\n",
      "epoch 1032 total_train_acc: 0.9649952696310312 loss: 0.46972040086984634 test_loss: 0.07465063780546188 test_acc: 0.9307598039215687\n",
      "epoch 1033 total_train_acc: 0.9633396404919584 loss: 0.493326835334301 test_loss: 0.09333977848291397 test_acc: 0.9350490196078431\n",
      "epoch 1034 total_train_acc: 0.9647587511825922 loss: 0.46573059633374214 test_loss: 0.07771846652030945 test_acc: 0.9350490196078431\n",
      "epoch 1035 total_train_acc: 0.966414380321665 loss: 0.46478620916604996 test_loss: 0.29355165362358093 test_acc: 0.9325980392156863\n",
      "epoch 1036 total_train_acc: 0.9642857142857143 loss: 0.4981037974357605 test_loss: 0.11002663522958755 test_acc: 0.9307598039215687\n",
      "epoch 1037 total_train_acc: 0.9645222327341533 loss: 0.46559176594018936 test_loss: 0.30631136894226074 test_acc: 0.9319852941176471\n",
      "epoch 1038 total_train_acc: 0.968070009460738 loss: 0.424231119453907 test_loss: 0.1543593853712082 test_acc: 0.9344362745098039\n",
      "epoch 1039 total_train_acc: 0.9635761589403974 loss: 0.5252810940146446 test_loss: 0.22830559313297272 test_acc: 0.9350490196078431\n",
      "epoch 1040 total_train_acc: 0.9661778618732261 loss: 0.47962046414613724 test_loss: 0.14156541228294373 test_acc: 0.9295343137254902\n",
      "epoch 1041 total_train_acc: 0.966650898770104 loss: 0.46188103407621384 test_loss: 0.05960872396826744 test_acc: 0.9319852941176471\n",
      "epoch 1042 total_train_acc: 0.9654683065279092 loss: 0.48960788547992706 test_loss: 0.13140295445919037 test_acc: 0.9356617647058824\n",
      "epoch 1043 total_train_acc: 0.9659413434247871 loss: 0.5392795726656914 test_loss: 0.3543204665184021 test_acc: 0.9356617647058824\n",
      "epoch 1044 total_train_acc: 0.9683065279091769 loss: 0.496900275349617 test_loss: 0.07823491841554642 test_acc: 0.9325980392156863\n",
      "epoch 1045 total_train_acc: 0.9640491958372753 loss: 0.47132667899131775 test_loss: 0.1542472392320633 test_acc: 0.9313725490196079\n",
      "epoch 1046 total_train_acc: 0.9661778618732261 loss: 0.5084623321890831 test_loss: 0.15759384632110596 test_acc: 0.9325980392156863\n",
      "epoch 1047 total_train_acc: 0.9645222327341533 loss: 0.49780086427927017 test_loss: 0.034525785595178604 test_acc: 0.9375\n",
      "epoch 1048 total_train_acc: 0.9671239356669821 loss: 0.48039280623197556 test_loss: 0.16687530279159546 test_acc: 0.9350490196078431\n",
      "epoch 1049 total_train_acc: 0.9645222327341533 loss: 0.49946972727775574 test_loss: 0.27499058842658997 test_acc: 0.9332107843137255\n",
      "epoch 1050 total_train_acc: 0.9633396404919584 loss: 0.5217019841074944 test_loss: 0.07146508246660233 test_acc: 0.9307598039215687\n",
      "epoch 1051 total_train_acc: 0.9645222327341533 loss: 0.4957977831363678 test_loss: 0.4493407905101776 test_acc: 0.9338235294117647\n",
      "epoch 1052 total_train_acc: 0.9671239356669821 loss: 0.5192861035466194 test_loss: 0.39505869150161743 test_acc: 0.9356617647058824\n",
      "epoch 1053 total_train_acc: 0.9659413434247871 loss: 0.4730745926499367 test_loss: 0.32626184821128845 test_acc: 0.9319852941176471\n",
      "epoch 1054 total_train_acc: 0.9649952696310312 loss: 0.4859231635928154 test_loss: 0.2695440948009491 test_acc: 0.9313725490196079\n",
      "epoch 1055 total_train_acc: 0.96759697256386 loss: 0.49538661539554596 test_loss: 0.06127743050456047 test_acc: 0.9350490196078431\n",
      "epoch 1056 total_train_acc: 0.9671239356669821 loss: 0.46842119097709656 test_loss: 0.1658354103565216 test_acc: 0.9325980392156863\n",
      "epoch 1057 total_train_acc: 0.9638126773888364 loss: 0.45929065346717834 test_loss: 0.08500377088785172 test_acc: 0.9313725490196079\n",
      "epoch 1058 total_train_acc: 0.9671239356669821 loss: 0.46539362519979477 test_loss: 0.1148795634508133 test_acc: 0.9350490196078431\n",
      "epoch 1059 total_train_acc: 0.9626300851466414 loss: 0.4728526212275028 test_loss: 0.3367130756378174 test_acc: 0.9356617647058824\n",
      "epoch 1060 total_train_acc: 0.9649952696310312 loss: 0.49582141637802124 test_loss: 0.3438688814640045 test_acc: 0.9362745098039216\n",
      "epoch 1061 total_train_acc: 0.967360454115421 loss: 0.43836361914873123 test_loss: 0.21709126234054565 test_acc: 0.9307598039215687\n",
      "epoch 1062 total_train_acc: 0.966650898770104 loss: 0.5074010789394379 test_loss: 0.28940102458000183 test_acc: 0.9307598039215687\n",
      "epoch 1063 total_train_acc: 0.9640491958372753 loss: 0.4796704798936844 test_loss: 0.3094731867313385 test_acc: 0.9313725490196079\n",
      "epoch 1064 total_train_acc: 0.9642857142857143 loss: 0.5163705348968506 test_loss: 0.2724439203739166 test_acc: 0.9313725490196079\n",
      "epoch 1065 total_train_acc: 0.9671239356669821 loss: 0.46446453034877777 test_loss: 0.5842726826667786 test_acc: 0.9332107843137255\n",
      "epoch 1066 total_train_acc: 0.9640491958372753 loss: 0.4664725363254547 test_loss: 0.26440292596817017 test_acc: 0.9332107843137255\n",
      "epoch 1067 total_train_acc: 0.9661778618732261 loss: 0.45587126165628433 test_loss: 0.07126593589782715 test_acc: 0.9325980392156863\n",
      "epoch 1068 total_train_acc: 0.9640491958372753 loss: 0.5266219228506088 test_loss: 0.19247981905937195 test_acc: 0.9350490196078431\n",
      "epoch 1069 total_train_acc: 0.9640491958372753 loss: 0.5017397478222847 test_loss: 0.08566106110811234 test_acc: 0.9350490196078431\n",
      "epoch 1070 total_train_acc: 0.966414380321665 loss: 0.5193938985466957 test_loss: 0.3071244955062866 test_acc: 0.9332107843137255\n",
      "epoch 1071 total_train_acc: 0.9657048249763481 loss: 0.46617820113897324 test_loss: 0.10571934282779694 test_acc: 0.9344362745098039\n",
      "epoch 1072 total_train_acc: 0.966414380321665 loss: 0.46885671466588974 test_loss: 0.22766365110874176 test_acc: 0.9344362745098039\n",
      "epoch 1073 total_train_acc: 0.966650898770104 loss: 0.48293596506118774 test_loss: 0.03928382322192192 test_acc: 0.9313725490196079\n",
      "epoch 1074 total_train_acc: 0.967360454115421 loss: 0.4904230162501335 test_loss: 0.09851380437612534 test_acc: 0.9301470588235294\n",
      "epoch 1075 total_train_acc: 0.966650898770104 loss: 0.46754927933216095 test_loss: 0.12805329263210297 test_acc: 0.9344362745098039\n",
      "epoch 1076 total_train_acc: 0.967360454115421 loss: 0.47031693160533905 test_loss: 0.12239643186330795 test_acc: 0.9368872549019608\n",
      "epoch 1077 total_train_acc: 0.9709082308420057 loss: 0.43127766996622086 test_loss: 0.10464905202388763 test_acc: 0.9332107843137255\n",
      "epoch 1078 total_train_acc: 0.9685430463576159 loss: 0.488427072763443 test_loss: 0.180936798453331 test_acc: 0.9338235294117647\n",
      "epoch 1079 total_train_acc: 0.9687795648060549 loss: 0.4724609851837158 test_loss: 0.20417332649230957 test_acc: 0.9350490196078431\n",
      "epoch 1080 total_train_acc: 0.967833491012299 loss: 0.5037022233009338 test_loss: 0.14755071699619293 test_acc: 0.9350490196078431\n",
      "epoch 1081 total_train_acc: 0.9668874172185431 loss: 0.4644705131649971 test_loss: 0.10285679250955582 test_acc: 0.9338235294117647\n",
      "epoch 1082 total_train_acc: 0.9657048249763481 loss: 0.5168502405285835 test_loss: 0.3184998035430908 test_acc: 0.9350490196078431\n",
      "epoch 1083 total_train_acc: 0.9683065279091769 loss: 0.428330946713686 test_loss: 0.045292943716049194 test_acc: 0.9344362745098039\n",
      "epoch 1084 total_train_acc: 0.9659413434247871 loss: 0.4640592262148857 test_loss: 0.1310064047574997 test_acc: 0.9319852941176471\n",
      "epoch 1085 total_train_acc: 0.9683065279091769 loss: 0.5066410899162292 test_loss: 0.49321311712265015 test_acc: 0.9301470588235294\n",
      "epoch 1086 total_train_acc: 0.966650898770104 loss: 0.474255733191967 test_loss: 0.189585343003273 test_acc: 0.9319852941176471\n",
      "epoch 1087 total_train_acc: 0.9661778618732261 loss: 0.511433981359005 test_loss: 0.05992136895656586 test_acc: 0.9307598039215687\n",
      "epoch 1088 total_train_acc: 0.9642857142857143 loss: 0.500369630753994 test_loss: 0.11902608722448349 test_acc: 0.9319852941176471\n",
      "epoch 1089 total_train_acc: 0.966650898770104 loss: 0.4826447144150734 test_loss: 0.12080377340316772 test_acc: 0.9325980392156863\n",
      "epoch 1090 total_train_acc: 0.9635761589403974 loss: 0.5408932790160179 test_loss: 0.3222423791885376 test_acc: 0.9356617647058824\n",
      "epoch 1091 total_train_acc: 0.9657048249763481 loss: 0.4619143530726433 test_loss: 0.1215294748544693 test_acc: 0.9295343137254902\n",
      "epoch 1092 total_train_acc: 0.9683065279091769 loss: 0.44184839725494385 test_loss: 0.2121364027261734 test_acc: 0.9319852941176471\n",
      "epoch 1093 total_train_acc: 0.968070009460738 loss: 0.5168785750865936 test_loss: 0.23368331789970398 test_acc: 0.9338235294117647\n",
      "epoch 1094 total_train_acc: 0.9638126773888364 loss: 0.5227966979146004 test_loss: 0.1911832094192505 test_acc: 0.9338235294117647\n",
      "epoch 1095 total_train_acc: 0.966414380321665 loss: 0.4497779980301857 test_loss: 0.2624249756336212 test_acc: 0.9319852941176471\n",
      "epoch 1096 total_train_acc: 0.9659413434247871 loss: 0.4976718947291374 test_loss: 0.15740250051021576 test_acc: 0.9313725490196079\n",
      "epoch 1097 total_train_acc: 0.967833491012299 loss: 0.4724661633372307 test_loss: 0.043957047164440155 test_acc: 0.9356617647058824\n",
      "epoch 1098 total_train_acc: 0.9619205298013245 loss: 0.5221802890300751 test_loss: 0.14849483966827393 test_acc: 0.9344362745098039\n",
      "epoch 1099 total_train_acc: 0.9661778618732261 loss: 0.44016578048467636 test_loss: 0.06685759872198105 test_acc: 0.9325980392156863\n",
      "epoch 1100 total_train_acc: 0.966650898770104 loss: 0.480996273458004 test_loss: 0.15070490539073944 test_acc: 0.9313725490196079\n",
      "epoch 1101 total_train_acc: 0.9645222327341533 loss: 0.46557987481355667 test_loss: 0.13264687359333038 test_acc: 0.9301470588235294\n",
      "epoch 1102 total_train_acc: 0.9645222327341533 loss: 0.47061601281166077 test_loss: 0.28115949034690857 test_acc: 0.9356617647058824\n",
      "epoch 1103 total_train_acc: 0.9642857142857143 loss: 0.47438423335552216 test_loss: 0.20218950510025024 test_acc: 0.9332107843137255\n",
      "epoch 1104 total_train_acc: 0.9668874172185431 loss: 0.4348371624946594 test_loss: 0.2112407237291336 test_acc: 0.9295343137254902\n",
      "epoch 1105 total_train_acc: 0.9668874172185431 loss: 0.48841825872659683 test_loss: 0.012969844974577427 test_acc: 0.9319852941176471\n",
      "epoch 1106 total_train_acc: 0.9694891201513718 loss: 0.44065288454294205 test_loss: 0.18602214753627777 test_acc: 0.9344362745098039\n",
      "epoch 1107 total_train_acc: 0.9683065279091769 loss: 0.463952012360096 test_loss: 0.1905515342950821 test_acc: 0.9332107843137255\n",
      "epoch 1108 total_train_acc: 0.966414380321665 loss: 0.4670633226633072 test_loss: 0.14588622748851776 test_acc: 0.9344362745098039\n",
      "epoch 1109 total_train_acc: 0.9683065279091769 loss: 0.47379227727651596 test_loss: 0.3569749891757965 test_acc: 0.9319852941176471\n",
      "epoch 1110 total_train_acc: 0.967833491012299 loss: 0.4331922382116318 test_loss: 0.33251211047172546 test_acc: 0.9313725490196079\n",
      "epoch 1111 total_train_acc: 0.9685430463576159 loss: 0.4578833281993866 test_loss: 0.3866598308086395 test_acc: 0.9319852941176471\n",
      "epoch 1112 total_train_acc: 0.968070009460738 loss: 0.4537990316748619 test_loss: 0.15075701475143433 test_acc: 0.9332107843137255\n",
      "epoch 1113 total_train_acc: 0.9668874172185431 loss: 0.46681738644838333 test_loss: 0.10245305299758911 test_acc: 0.9344362745098039\n",
      "epoch 1114 total_train_acc: 0.9694891201513718 loss: 0.4259878620505333 test_loss: 0.32612305879592896 test_acc: 0.9325980392156863\n",
      "epoch 1115 total_train_acc: 0.9628666035950804 loss: 0.5233595743775368 test_loss: 0.0783492773771286 test_acc: 0.9325980392156863\n",
      "epoch 1116 total_train_acc: 0.9692526017029328 loss: 0.4663488604128361 test_loss: 0.12981392443180084 test_acc: 0.9338235294117647\n",
      "epoch 1117 total_train_acc: 0.9683065279091769 loss: 0.45042357593774796 test_loss: 0.3525547683238983 test_acc: 0.9319852941176471\n",
      "epoch 1118 total_train_acc: 0.9704351939451277 loss: 0.45942608267068863 test_loss: 0.40420591831207275 test_acc: 0.9313725490196079\n",
      "epoch 1119 total_train_acc: 0.966650898770104 loss: 0.4749698266386986 test_loss: 0.2900933623313904 test_acc: 0.9295343137254902\n",
      "epoch 1120 total_train_acc: 0.9638126773888364 loss: 0.5584286227822304 test_loss: 0.13019001483917236 test_acc: 0.9319852941176471\n",
      "epoch 1121 total_train_acc: 0.9716177861873226 loss: 0.4438697099685669 test_loss: 0.17360268533229828 test_acc: 0.9313725490196079\n",
      "epoch 1122 total_train_acc: 0.96759697256386 loss: 0.43714092671871185 test_loss: 0.11297690123319626 test_acc: 0.9319852941176471\n",
      "epoch 1123 total_train_acc: 0.966650898770104 loss: 0.4577917829155922 test_loss: 0.18163476884365082 test_acc: 0.9350490196078431\n",
      "epoch 1124 total_train_acc: 0.9626300851466414 loss: 0.46633825451135635 test_loss: 0.4900723397731781 test_acc: 0.9325980392156863\n",
      "epoch 1125 total_train_acc: 0.9652317880794702 loss: 0.4777878373861313 test_loss: 0.25881820917129517 test_acc: 0.9350490196078431\n",
      "epoch 1126 total_train_acc: 0.968070009460738 loss: 0.4828372374176979 test_loss: 0.3301151394844055 test_acc: 0.9344362745098039\n",
      "epoch 1127 total_train_acc: 0.967360454115421 loss: 0.473997563123703 test_loss: 0.30457910895347595 test_acc: 0.9332107843137255\n",
      "epoch 1128 total_train_acc: 0.9661778618732261 loss: 0.48035669326782227 test_loss: 0.15546318888664246 test_acc: 0.9313725490196079\n",
      "epoch 1129 total_train_acc: 0.9652317880794702 loss: 0.4481375887989998 test_loss: 0.3053662180900574 test_acc: 0.9319852941176471\n",
      "epoch 1130 total_train_acc: 0.9690160832544938 loss: 0.424153707921505 test_loss: 0.06625515222549438 test_acc: 0.9301470588235294\n",
      "epoch 1131 total_train_acc: 0.9621570482497634 loss: 0.48102642595767975 test_loss: 0.4237453043460846 test_acc: 0.9307598039215687\n",
      "epoch 1132 total_train_acc: 0.9652317880794702 loss: 0.4679592698812485 test_loss: 0.6856686472892761 test_acc: 0.9325980392156863\n",
      "epoch 1133 total_train_acc: 0.9694891201513718 loss: 0.4535495713353157 test_loss: 0.3874374032020569 test_acc: 0.9356617647058824\n",
      "epoch 1134 total_train_acc: 0.9685430463576159 loss: 0.43612226098775864 test_loss: 0.2733805179595947 test_acc: 0.9344362745098039\n",
      "epoch 1135 total_train_acc: 0.9687795648060549 loss: 0.4596264287829399 test_loss: 0.4526917636394501 test_acc: 0.9319852941176471\n",
      "epoch 1136 total_train_acc: 0.9683065279091769 loss: 0.4311346784234047 test_loss: 0.03861035779118538 test_acc: 0.9307598039215687\n",
      "epoch 1137 total_train_acc: 0.9692526017029328 loss: 0.44659458100795746 test_loss: 0.08256467431783676 test_acc: 0.9313725490196079\n",
      "epoch 1138 total_train_acc: 0.9687795648060549 loss: 0.48542359471321106 test_loss: 0.33781951665878296 test_acc: 0.9313725490196079\n",
      "epoch 1139 total_train_acc: 0.9661778618732261 loss: 0.4644078016281128 test_loss: 0.06844110041856766 test_acc: 0.9313725490196079\n",
      "epoch 1140 total_train_acc: 0.96759697256386 loss: 0.4710230678319931 test_loss: 0.28564560413360596 test_acc: 0.9338235294117647\n",
      "epoch 1141 total_train_acc: 0.968070009460738 loss: 0.4676222428679466 test_loss: 0.2883490324020386 test_acc: 0.9344362745098039\n",
      "epoch 1142 total_train_acc: 0.968070009460738 loss: 0.4531727358698845 test_loss: 0.42624956369400024 test_acc: 0.9350490196078431\n",
      "epoch 1143 total_train_acc: 0.9671239356669821 loss: 0.4613829255104065 test_loss: 0.17263230681419373 test_acc: 0.9313725490196079\n",
      "epoch 1144 total_train_acc: 0.967833491012299 loss: 0.4383722022175789 test_loss: 0.15526877343654633 test_acc: 0.9307598039215687\n",
      "epoch 1145 total_train_acc: 0.9697256385998108 loss: 0.44133806973695755 test_loss: 0.21348698437213898 test_acc: 0.9313725490196079\n",
      "epoch 1146 total_train_acc: 0.9687795648060549 loss: 0.47542744874954224 test_loss: 0.12705086171627045 test_acc: 0.9338235294117647\n",
      "epoch 1147 total_train_acc: 0.967833491012299 loss: 0.4419144317507744 test_loss: 0.20986440777778625 test_acc: 0.9319852941176471\n",
      "epoch 1148 total_train_acc: 0.966414380321665 loss: 0.45752231031656265 test_loss: 0.16757330298423767 test_acc: 0.9283088235294118\n",
      "epoch 1149 total_train_acc: 0.9685430463576159 loss: 0.4547959789633751 test_loss: 0.28534334897994995 test_acc: 0.9283088235294118\n",
      "epoch 1150 total_train_acc: 0.9668874172185431 loss: 0.46690134704113007 test_loss: 0.49593934416770935 test_acc: 0.9325980392156863\n",
      "epoch 1151 total_train_acc: 0.9699621570482497 loss: 0.4715357795357704 test_loss: 0.11184260994195938 test_acc: 0.9338235294117647\n",
      "epoch 1152 total_train_acc: 0.968070009460738 loss: 0.427909679710865 test_loss: 0.2822243869304657 test_acc: 0.9319852941176471\n",
      "epoch 1153 total_train_acc: 0.9661778618732261 loss: 0.5088896378874779 test_loss: 0.3912820518016815 test_acc: 0.9325980392156863\n",
      "epoch 1154 total_train_acc: 0.9671239356669821 loss: 0.4639127254486084 test_loss: 0.12697753310203552 test_acc: 0.9319852941176471\n",
      "epoch 1155 total_train_acc: 0.9668874172185431 loss: 0.44223717600107193 test_loss: 0.18679706752300262 test_acc: 0.9301470588235294\n",
      "epoch 1156 total_train_acc: 0.9725638599810785 loss: 0.39166661351919174 test_loss: 0.14487271010875702 test_acc: 0.9307598039215687\n",
      "epoch 1157 total_train_acc: 0.9706717123935666 loss: 0.449211947619915 test_loss: 0.2467367798089981 test_acc: 0.9344362745098039\n",
      "epoch 1158 total_train_acc: 0.9635761589403974 loss: 0.4721563309431076 test_loss: 0.08021413534879684 test_acc: 0.9307598039215687\n",
      "epoch 1159 total_train_acc: 0.967833491012299 loss: 0.4655236527323723 test_loss: 0.15635661780834198 test_acc: 0.9332107843137255\n",
      "epoch 1160 total_train_acc: 0.9668874172185431 loss: 0.4425560459494591 test_loss: 0.22347210347652435 test_acc: 0.9319852941176471\n",
      "epoch 1161 total_train_acc: 0.9685430463576159 loss: 0.4608369842171669 test_loss: 0.028077661991119385 test_acc: 0.9301470588235294\n",
      "epoch 1162 total_train_acc: 0.9654683065279092 loss: 0.4494079723954201 test_loss: 0.46116408705711365 test_acc: 0.9301470588235294\n",
      "epoch 1163 total_train_acc: 0.966414380321665 loss: 0.43846650421619415 test_loss: 0.17685575783252716 test_acc: 0.9295343137254902\n",
      "epoch 1164 total_train_acc: 0.9704351939451277 loss: 0.42261917889118195 test_loss: 0.17498569190502167 test_acc: 0.9295343137254902\n",
      "epoch 1165 total_train_acc: 0.9659413434247871 loss: 0.4547305479645729 test_loss: 0.3335515856742859 test_acc: 0.9283088235294118\n",
      "epoch 1166 total_train_acc: 0.9645222327341533 loss: 0.4716585502028465 test_loss: 0.13512875139713287 test_acc: 0.928921568627451\n",
      "epoch 1167 total_train_acc: 0.9716177861873226 loss: 0.42462362349033356 test_loss: 0.29731014370918274 test_acc: 0.9325980392156863\n",
      "epoch 1168 total_train_acc: 0.9697256385998108 loss: 0.42037151753902435 test_loss: 0.037679947912693024 test_acc: 0.9319852941176471\n",
      "epoch 1169 total_train_acc: 0.9654683065279092 loss: 0.44141682982444763 test_loss: 0.24937815964221954 test_acc: 0.9307598039215687\n",
      "epoch 1170 total_train_acc: 0.96759697256386 loss: 0.45842745155096054 test_loss: 0.11627137660980225 test_acc: 0.9332107843137255\n",
      "epoch 1171 total_train_acc: 0.9685430463576159 loss: 0.4528254121541977 test_loss: 0.6196839213371277 test_acc: 0.9350490196078431\n",
      "epoch 1172 total_train_acc: 0.9683065279091769 loss: 0.4247424155473709 test_loss: 0.09431253373622894 test_acc: 0.9313725490196079\n",
      "epoch 1173 total_train_acc: 0.9716177861873226 loss: 0.4266752749681473 test_loss: 0.1964535117149353 test_acc: 0.9307598039215687\n",
      "epoch 1174 total_train_acc: 0.9687795648060549 loss: 0.4726158156991005 test_loss: 0.09145888686180115 test_acc: 0.9313725490196079\n",
      "epoch 1175 total_train_acc: 0.9649952696310312 loss: 0.4718231111764908 test_loss: 0.024215443059802055 test_acc: 0.9307598039215687\n",
      "epoch 1176 total_train_acc: 0.967833491012299 loss: 0.4626348242163658 test_loss: 0.07567187398672104 test_acc: 0.9325980392156863\n",
      "epoch 1177 total_train_acc: 0.9668874172185431 loss: 0.4702325761318207 test_loss: 0.10881847143173218 test_acc: 0.928921568627451\n",
      "epoch 1178 total_train_acc: 0.9687795648060549 loss: 0.44675251096487045 test_loss: 0.3229689300060272 test_acc: 0.9313725490196079\n",
      "epoch 1179 total_train_acc: 0.9706717123935666 loss: 0.43586698174476624 test_loss: 0.2258535772562027 test_acc: 0.9332107843137255\n",
      "epoch 1180 total_train_acc: 0.9668874172185431 loss: 0.45481186360120773 test_loss: 0.2795488238334656 test_acc: 0.9313725490196079\n",
      "epoch 1181 total_train_acc: 0.9713812677388837 loss: 0.40719471499323845 test_loss: 0.05775611847639084 test_acc: 0.9356617647058824\n",
      "epoch 1182 total_train_acc: 0.9657048249763481 loss: 0.4774143546819687 test_loss: 0.2130364626646042 test_acc: 0.9344362745098039\n",
      "epoch 1183 total_train_acc: 0.9671239356669821 loss: 0.5006878077983856 test_loss: 0.058930058032274246 test_acc: 0.9313725490196079\n",
      "epoch 1184 total_train_acc: 0.9671239356669821 loss: 0.4941128268837929 test_loss: 0.1529519259929657 test_acc: 0.9362745098039216\n",
      "epoch 1185 total_train_acc: 0.9657048249763481 loss: 0.44135212898254395 test_loss: 0.17197565734386444 test_acc: 0.9344362745098039\n",
      "epoch 1186 total_train_acc: 0.9671239356669821 loss: 0.4160227067768574 test_loss: 0.15265241265296936 test_acc: 0.9344362745098039\n",
      "epoch 1187 total_train_acc: 0.9711447492904447 loss: 0.42457688599824905 test_loss: 0.32465600967407227 test_acc: 0.9338235294117647\n",
      "epoch 1188 total_train_acc: 0.9701986754966887 loss: 0.43714017421007156 test_loss: 0.15530820190906525 test_acc: 0.9344362745098039\n",
      "epoch 1189 total_train_acc: 0.9694891201513718 loss: 0.4387664645910263 test_loss: 0.25506943464279175 test_acc: 0.9325980392156863\n",
      "epoch 1190 total_train_acc: 0.967360454115421 loss: 0.4492187201976776 test_loss: 0.4553444981575012 test_acc: 0.9307598039215687\n",
      "epoch 1191 total_train_acc: 0.966650898770104 loss: 0.46592843532562256 test_loss: 0.24426501989364624 test_acc: 0.9295343137254902\n",
      "epoch 1192 total_train_acc: 0.96759697256386 loss: 0.4482113942503929 test_loss: 0.19515974819660187 test_acc: 0.9350490196078431\n",
      "epoch 1193 total_train_acc: 0.9718543046357616 loss: 0.45489010214805603 test_loss: 0.23049072921276093 test_acc: 0.9325980392156863\n",
      "epoch 1194 total_train_acc: 0.9690160832544938 loss: 0.40584272146224976 test_loss: 0.14976397156715393 test_acc: 0.9319852941176471\n",
      "epoch 1195 total_train_acc: 0.9633396404919584 loss: 0.4685553312301636 test_loss: 0.029963934794068336 test_acc: 0.9325980392156863\n",
      "epoch 1196 total_train_acc: 0.9716177861873226 loss: 0.43455325812101364 test_loss: 0.1452372819185257 test_acc: 0.9350490196078431\n",
      "epoch 1197 total_train_acc: 0.9694891201513718 loss: 0.4680502638220787 test_loss: 0.1545429676771164 test_acc: 0.9319852941176471\n",
      "epoch 1198 total_train_acc: 0.9701986754966887 loss: 0.4349789023399353 test_loss: 0.05278981477022171 test_acc: 0.9325980392156863\n",
      "epoch 1199 total_train_acc: 0.9649952696310312 loss: 0.4626665785908699 test_loss: 0.09183262288570404 test_acc: 0.9338235294117647\n",
      "epoch 1200 total_train_acc: 0.9671239356669821 loss: 0.42589566856622696 test_loss: 0.03974124789237976 test_acc: 0.9325980392156863\n",
      "epoch 1201 total_train_acc: 0.9687795648060549 loss: 0.46848296374082565 test_loss: 0.3584262430667877 test_acc: 0.9319852941176471\n",
      "epoch 1202 total_train_acc: 0.9718543046357616 loss: 0.39485954493284225 test_loss: 0.08261030912399292 test_acc: 0.9319852941176471\n",
      "epoch 1203 total_train_acc: 0.9668874172185431 loss: 0.4767165705561638 test_loss: 0.10731195658445358 test_acc: 0.9332107843137255\n",
      "epoch 1204 total_train_acc: 0.9723273415326396 loss: 0.4114166498184204 test_loss: 0.02862878143787384 test_acc: 0.9350490196078431\n",
      "epoch 1205 total_train_acc: 0.96759697256386 loss: 0.4256839156150818 test_loss: 0.2820796072483063 test_acc: 0.9338235294117647\n",
      "epoch 1206 total_train_acc: 0.9687795648060549 loss: 0.41334987059235573 test_loss: 0.4286670386791229 test_acc: 0.9319852941176471\n",
      "epoch 1207 total_train_acc: 0.966414380321665 loss: 0.4296604320406914 test_loss: 0.6344885230064392 test_acc: 0.9338235294117647\n",
      "epoch 1208 total_train_acc: 0.9683065279091769 loss: 0.43116313964128494 test_loss: 0.18029499053955078 test_acc: 0.9338235294117647\n",
      "epoch 1209 total_train_acc: 0.966650898770104 loss: 0.43929851800203323 test_loss: 0.25475966930389404 test_acc: 0.9332107843137255\n",
      "epoch 1210 total_train_acc: 0.9687795648060549 loss: 0.46999146044254303 test_loss: 0.1602185070514679 test_acc: 0.9325980392156863\n",
      "epoch 1211 total_train_acc: 0.9699621570482497 loss: 0.43136289715766907 test_loss: 0.07096521556377411 test_acc: 0.9313725490196079\n",
      "epoch 1212 total_train_acc: 0.9671239356669821 loss: 0.4626499116420746 test_loss: 0.1805780977010727 test_acc: 0.9338235294117647\n",
      "epoch 1213 total_train_acc: 0.967833491012299 loss: 0.43029485642910004 test_loss: 0.33644214272499084 test_acc: 0.9338235294117647\n",
      "epoch 1214 total_train_acc: 0.96759697256386 loss: 0.4491501525044441 test_loss: 0.2736603915691376 test_acc: 0.9332107843137255\n",
      "epoch 1215 total_train_acc: 0.967833491012299 loss: 0.4402158558368683 test_loss: 0.1113305538892746 test_acc: 0.9338235294117647\n",
      "epoch 1216 total_train_acc: 0.9701986754966887 loss: 0.42619726061820984 test_loss: 0.1772628277540207 test_acc: 0.9325980392156863\n",
      "epoch 1217 total_train_acc: 0.9723273415326396 loss: 0.43910234421491623 test_loss: 0.1320984661579132 test_acc: 0.9319852941176471\n",
      "epoch 1218 total_train_acc: 0.9690160832544938 loss: 0.42109087854623795 test_loss: 0.22884520888328552 test_acc: 0.9344362745098039\n",
      "epoch 1219 total_train_acc: 0.9683065279091769 loss: 0.4399494230747223 test_loss: 0.06716831773519516 test_acc: 0.9356617647058824\n",
      "epoch 1220 total_train_acc: 0.9685430463576159 loss: 0.45575200766324997 test_loss: 0.2127460539340973 test_acc: 0.9362745098039216\n",
      "epoch 1221 total_train_acc: 0.9709082308420057 loss: 0.42387334257364273 test_loss: 0.12592044472694397 test_acc: 0.9350490196078431\n",
      "epoch 1222 total_train_acc: 0.9685430463576159 loss: 0.42422403395175934 test_loss: 0.30390775203704834 test_acc: 0.9338235294117647\n",
      "epoch 1223 total_train_acc: 0.9718543046357616 loss: 0.4220477193593979 test_loss: 0.17297430336475372 test_acc: 0.9319852941176471\n",
      "epoch 1224 total_train_acc: 0.9697256385998108 loss: 0.4549454301595688 test_loss: 0.10612326860427856 test_acc: 0.9313725490196079\n",
      "epoch 1225 total_train_acc: 0.9685430463576159 loss: 0.4690258055925369 test_loss: 0.17466184496879578 test_acc: 0.9319852941176471\n",
      "epoch 1226 total_train_acc: 0.9744560075685903 loss: 0.42843323945999146 test_loss: 0.31430020928382874 test_acc: 0.9356617647058824\n",
      "epoch 1227 total_train_acc: 0.9647587511825922 loss: 0.44005242735147476 test_loss: 0.06973587721586227 test_acc: 0.9350490196078431\n",
      "epoch 1228 total_train_acc: 0.9690160832544938 loss: 0.4629915654659271 test_loss: 0.052643757313489914 test_acc: 0.9344362745098039\n",
      "epoch 1229 total_train_acc: 0.9692526017029328 loss: 0.44187386333942413 test_loss: 0.25206258893013 test_acc: 0.9325980392156863\n",
      "epoch 1230 total_train_acc: 0.9690160832544938 loss: 0.4352278783917427 test_loss: 0.11948475986719131 test_acc: 0.9344362745098039\n",
      "epoch 1231 total_train_acc: 0.9716177861873226 loss: 0.4361484944820404 test_loss: 0.21605205535888672 test_acc: 0.9319852941176471\n",
      "epoch 1232 total_train_acc: 0.9661778618732261 loss: 0.41665268689393997 test_loss: 0.2505488693714142 test_acc: 0.9344362745098039\n",
      "epoch 1233 total_train_acc: 0.9671239356669821 loss: 0.4284577816724777 test_loss: 0.0955279991030693 test_acc: 0.9332107843137255\n",
      "epoch 1234 total_train_acc: 0.9694891201513718 loss: 0.44569406658411026 test_loss: 0.3297235071659088 test_acc: 0.9325980392156863\n",
      "epoch 1235 total_train_acc: 0.9671239356669821 loss: 0.4591763764619827 test_loss: 0.16311314702033997 test_acc: 0.9350490196078431\n",
      "epoch 1236 total_train_acc: 0.9687795648060549 loss: 0.4274456351995468 test_loss: 0.3920190632343292 test_acc: 0.9368872549019608\n",
      "epoch 1237 total_train_acc: 0.9699621570482497 loss: 0.42782098799943924 test_loss: 0.12540219724178314 test_acc: 0.9344362745098039\n",
      "epoch 1238 total_train_acc: 0.9718543046357616 loss: 0.4079994782805443 test_loss: 0.22880646586418152 test_acc: 0.9332107843137255\n",
      "epoch 1239 total_train_acc: 0.9697256385998108 loss: 0.4158327579498291 test_loss: 0.4788585901260376 test_acc: 0.9313725490196079\n",
      "epoch 1240 total_train_acc: 0.9709082308420057 loss: 0.43123818188905716 test_loss: 0.16076041758060455 test_acc: 0.9313725490196079\n",
      "epoch 1241 total_train_acc: 0.9713812677388837 loss: 0.4054027944803238 test_loss: 0.16725532710552216 test_acc: 0.9325980392156863\n",
      "epoch 1242 total_train_acc: 0.9692526017029328 loss: 0.43931473791599274 test_loss: 0.024795738980174065 test_acc: 0.9332107843137255\n",
      "epoch 1243 total_train_acc: 0.9699621570482497 loss: 0.4243156909942627 test_loss: 0.22937937080860138 test_acc: 0.9338235294117647\n",
      "epoch 1244 total_train_acc: 0.9737464522232734 loss: 0.4089340716600418 test_loss: 0.06833434849977493 test_acc: 0.9350490196078431\n",
      "epoch 1245 total_train_acc: 0.967360454115421 loss: 0.46764059364795685 test_loss: 0.06886444240808487 test_acc: 0.9344362745098039\n",
      "epoch 1246 total_train_acc: 0.9739829706717124 loss: 0.40394478291273117 test_loss: 0.17207065224647522 test_acc: 0.9344362745098039\n",
      "epoch 1247 total_train_acc: 0.9654683065279092 loss: 0.4612097889184952 test_loss: 0.29704082012176514 test_acc: 0.9301470588235294\n",
      "epoch 1248 total_train_acc: 0.9704351939451277 loss: 0.4421171396970749 test_loss: 0.22060298919677734 test_acc: 0.9313725490196079\n",
      "epoch 1249 total_train_acc: 0.9706717123935666 loss: 0.4346441701054573 test_loss: 0.24237333238124847 test_acc: 0.9338235294117647\n",
      "epoch 1250 total_train_acc: 0.9718543046357616 loss: 0.42653363943099976 test_loss: 0.5266802906990051 test_acc: 0.9350490196078431\n",
      "epoch 1251 total_train_acc: 0.9683065279091769 loss: 0.4473937898874283 test_loss: 0.1343073546886444 test_acc: 0.9338235294117647\n",
      "epoch 1252 total_train_acc: 0.9718543046357616 loss: 0.4030991569161415 test_loss: 0.08529751002788544 test_acc: 0.9350490196078431\n",
      "epoch 1253 total_train_acc: 0.9694891201513718 loss: 0.42030396312475204 test_loss: 0.07048126310110092 test_acc: 0.9332107843137255\n",
      "epoch 1254 total_train_acc: 0.9683065279091769 loss: 0.3977728746831417 test_loss: 0.3093545138835907 test_acc: 0.9350490196078431\n",
      "epoch 1255 total_train_acc: 0.9697256385998108 loss: 0.41079677641391754 test_loss: 0.2117251604795456 test_acc: 0.9344362745098039\n",
      "epoch 1256 total_train_acc: 0.9661778618732261 loss: 0.46520333737134933 test_loss: 0.29072320461273193 test_acc: 0.9313725490196079\n",
      "epoch 1257 total_train_acc: 0.9711447492904447 loss: 0.41214174032211304 test_loss: 0.2238588184118271 test_acc: 0.9338235294117647\n",
      "epoch 1258 total_train_acc: 0.9699621570482497 loss: 0.449750192463398 test_loss: 0.4796195328235626 test_acc: 0.9344362745098039\n",
      "epoch 1259 total_train_acc: 0.9701986754966887 loss: 0.42353900521993637 test_loss: 0.2997989356517792 test_acc: 0.9338235294117647\n",
      "epoch 1260 total_train_acc: 0.9685430463576159 loss: 0.45848941802978516 test_loss: 0.14334017038345337 test_acc: 0.9319852941176471\n",
      "epoch 1261 total_train_acc: 0.9711447492904447 loss: 0.4097812697291374 test_loss: 0.2716093063354492 test_acc: 0.9332107843137255\n",
      "epoch 1262 total_train_acc: 0.9699621570482497 loss: 0.40213145315647125 test_loss: 0.15559211373329163 test_acc: 0.9332107843137255\n",
      "epoch 1263 total_train_acc: 0.9645222327341533 loss: 0.4794337972998619 test_loss: 0.5895412564277649 test_acc: 0.9344362745098039\n",
      "epoch 1264 total_train_acc: 0.9709082308420057 loss: 0.40838436782360077 test_loss: 0.10236044228076935 test_acc: 0.9338235294117647\n",
      "epoch 1265 total_train_acc: 0.9685430463576159 loss: 0.4634694382548332 test_loss: 0.355871319770813 test_acc: 0.9362745098039216\n",
      "epoch 1266 total_train_acc: 0.9713812677388837 loss: 0.4351101815700531 test_loss: 0.06903497129678726 test_acc: 0.9362745098039216\n",
      "epoch 1267 total_train_acc: 0.9706717123935666 loss: 0.4499036967754364 test_loss: 0.03703320026397705 test_acc: 0.9344362745098039\n",
      "epoch 1268 total_train_acc: 0.9701986754966887 loss: 0.409564271569252 test_loss: 0.06729117780923843 test_acc: 0.9325980392156863\n",
      "epoch 1269 total_train_acc: 0.9716177861873226 loss: 0.40314147621393204 test_loss: 0.4967891573905945 test_acc: 0.9313725490196079\n",
      "epoch 1270 total_train_acc: 0.9709082308420057 loss: 0.4296841099858284 test_loss: 0.23998308181762695 test_acc: 0.9332107843137255\n",
      "epoch 1271 total_train_acc: 0.96759697256386 loss: 0.46319765597581863 test_loss: 0.11825675517320633 test_acc: 0.9350490196078431\n",
      "epoch 1272 total_train_acc: 0.9687795648060549 loss: 0.46373992413282394 test_loss: 0.19915533065795898 test_acc: 0.9350490196078431\n",
      "epoch 1273 total_train_acc: 0.966414380321665 loss: 0.4430195465683937 test_loss: 0.1602003276348114 test_acc: 0.9319852941176471\n",
      "epoch 1274 total_train_acc: 0.9697256385998108 loss: 0.4208722561597824 test_loss: 0.25075384974479675 test_acc: 0.9344362745098039\n",
      "epoch 1275 total_train_acc: 0.9723273415326396 loss: 0.38864120095968246 test_loss: 0.14282214641571045 test_acc: 0.9319852941176471\n",
      "epoch 1276 total_train_acc: 0.9744560075685903 loss: 0.4473326951265335 test_loss: 0.22952544689178467 test_acc: 0.9319852941176471\n",
      "epoch 1277 total_train_acc: 0.968070009460738 loss: 0.4577939957380295 test_loss: 0.29709622263908386 test_acc: 0.9332107843137255\n",
      "epoch 1278 total_train_acc: 0.9713812677388837 loss: 0.40467438846826553 test_loss: 0.049108199775218964 test_acc: 0.9344362745098039\n",
      "epoch 1279 total_train_acc: 0.9720908230842006 loss: 0.409824974834919 test_loss: 0.3269798457622528 test_acc: 0.9344362745098039\n",
      "epoch 1280 total_train_acc: 0.9690160832544938 loss: 0.4251759350299835 test_loss: 0.18033699691295624 test_acc: 0.9350490196078431\n",
      "epoch 1281 total_train_acc: 0.9692526017029328 loss: 0.4137071743607521 test_loss: 0.059252671897411346 test_acc: 0.9325980392156863\n",
      "epoch 1282 total_train_acc: 0.9716177861873226 loss: 0.41647740453481674 test_loss: 0.21329669654369354 test_acc: 0.9313725490196079\n",
      "epoch 1283 total_train_acc: 0.9692526017029328 loss: 0.41453422233462334 test_loss: 0.0476393885910511 test_acc: 0.9307598039215687\n",
      "epoch 1284 total_train_acc: 0.9685430463576159 loss: 0.41223080456256866 test_loss: 0.10298836976289749 test_acc: 0.9313725490196079\n",
      "epoch 1285 total_train_acc: 0.9706717123935666 loss: 0.4847390577197075 test_loss: 0.17139579355716705 test_acc: 0.9325980392156863\n",
      "epoch 1286 total_train_acc: 0.9687795648060549 loss: 0.4259338974952698 test_loss: 0.3920733630657196 test_acc: 0.9350490196078431\n",
      "epoch 1287 total_train_acc: 0.9732734153263954 loss: 0.41740240901708603 test_loss: 0.31991443037986755 test_acc: 0.9338235294117647\n",
      "epoch 1288 total_train_acc: 0.9723273415326396 loss: 0.4187277853488922 test_loss: 0.22291108965873718 test_acc: 0.9319852941176471\n",
      "epoch 1289 total_train_acc: 0.9701986754966887 loss: 0.4618111774325371 test_loss: 0.6150802373886108 test_acc: 0.9319852941176471\n",
      "epoch 1290 total_train_acc: 0.9699621570482497 loss: 0.41618945449590683 test_loss: 0.047719717025756836 test_acc: 0.9313725490196079\n",
      "epoch 1291 total_train_acc: 0.9701986754966887 loss: 0.4141053035855293 test_loss: 0.19753716886043549 test_acc: 0.9338235294117647\n",
      "epoch 1292 total_train_acc: 0.9746925260170294 loss: 0.4219728261232376 test_loss: 0.02375490963459015 test_acc: 0.9338235294117647\n",
      "epoch 1293 total_train_acc: 0.9692526017029328 loss: 0.4211718216538429 test_loss: 0.40049540996551514 test_acc: 0.9338235294117647\n",
      "epoch 1294 total_train_acc: 0.967833491012299 loss: 0.426187664270401 test_loss: 0.20911306142807007 test_acc: 0.9356617647058824\n",
      "epoch 1295 total_train_acc: 0.9699621570482497 loss: 0.4312273785471916 test_loss: 0.08614589273929596 test_acc: 0.9362745098039216\n",
      "epoch 1296 total_train_acc: 0.9683065279091769 loss: 0.40916886925697327 test_loss: 0.12847445905208588 test_acc: 0.9344362745098039\n",
      "epoch 1297 total_train_acc: 0.9699621570482497 loss: 0.435266338288784 test_loss: 0.4518399238586426 test_acc: 0.9338235294117647\n",
      "epoch 1298 total_train_acc: 0.9709082308420057 loss: 0.4318380355834961 test_loss: 0.1917768120765686 test_acc: 0.9325980392156863\n",
      "epoch 1299 total_train_acc: 0.9716177861873226 loss: 0.4388233944773674 test_loss: 0.19320067763328552 test_acc: 0.9319852941176471\n",
      "epoch 1300 total_train_acc: 0.9701986754966887 loss: 0.42949507385492325 test_loss: 0.39362457394599915 test_acc: 0.9338235294117647\n",
      "epoch 1301 total_train_acc: 0.9683065279091769 loss: 0.4382871463894844 test_loss: 0.19839788973331451 test_acc: 0.9338235294117647\n",
      "epoch 1302 total_train_acc: 0.9704351939451277 loss: 0.42031077295541763 test_loss: 0.1278184950351715 test_acc: 0.9332107843137255\n",
      "epoch 1303 total_train_acc: 0.9654683065279092 loss: 0.4407070279121399 test_loss: 0.1819857656955719 test_acc: 0.9332107843137255\n",
      "epoch 1304 total_train_acc: 0.9725638599810785 loss: 0.4356757253408432 test_loss: 0.47629114985466003 test_acc: 0.9319852941176471\n",
      "epoch 1305 total_train_acc: 0.9685430463576159 loss: 0.41483305394649506 test_loss: 0.08047648519277573 test_acc: 0.9344362745098039\n",
      "epoch 1306 total_train_acc: 0.9704351939451277 loss: 0.43837468326091766 test_loss: 0.16044889390468597 test_acc: 0.9319852941176471\n",
      "epoch 1307 total_train_acc: 0.9713812677388837 loss: 0.4222875237464905 test_loss: 0.4820874035358429 test_acc: 0.9338235294117647\n",
      "epoch 1308 total_train_acc: 0.9683065279091769 loss: 0.4382051080465317 test_loss: 0.1949012279510498 test_acc: 0.9332107843137255\n",
      "epoch 1309 total_train_acc: 0.9706717123935666 loss: 0.4301375448703766 test_loss: 0.30555567145347595 test_acc: 0.9350490196078431\n",
      "epoch 1310 total_train_acc: 0.9709082308420057 loss: 0.4256473183631897 test_loss: 0.1673876792192459 test_acc: 0.9344362745098039\n",
      "epoch 1311 total_train_acc: 0.9720908230842006 loss: 0.3756193034350872 test_loss: 0.22512759268283844 test_acc: 0.9313725490196079\n",
      "epoch 1312 total_train_acc: 0.9699621570482497 loss: 0.41948380321264267 test_loss: 0.5405961275100708 test_acc: 0.9313725490196079\n",
      "epoch 1313 total_train_acc: 0.9706717123935666 loss: 0.3958585448563099 test_loss: 0.010599754750728607 test_acc: 0.9332107843137255\n",
      "epoch 1314 total_train_acc: 0.9685430463576159 loss: 0.4221954457461834 test_loss: 0.231972798705101 test_acc: 0.9356617647058824\n",
      "epoch 1315 total_train_acc: 0.9694891201513718 loss: 0.42244110256433487 test_loss: 0.25100624561309814 test_acc: 0.9344362745098039\n",
      "epoch 1316 total_train_acc: 0.9704351939451277 loss: 0.4100734740495682 test_loss: 0.25736916065216064 test_acc: 0.9344362745098039\n",
      "epoch 1317 total_train_acc: 0.9713812677388837 loss: 0.4324207082390785 test_loss: 0.2527008056640625 test_acc: 0.9325980392156863\n",
      "epoch 1318 total_train_acc: 0.9671239356669821 loss: 0.4227297082543373 test_loss: 0.3481731712818146 test_acc: 0.9319852941176471\n",
      "epoch 1319 total_train_acc: 0.96759697256386 loss: 0.4331904724240303 test_loss: 0.12929843366146088 test_acc: 0.9332107843137255\n",
      "epoch 1320 total_train_acc: 0.9737464522232734 loss: 0.4177471995353699 test_loss: 0.35055363178253174 test_acc: 0.9344362745098039\n",
      "epoch 1321 total_train_acc: 0.968070009460738 loss: 0.4422079473733902 test_loss: 0.33513951301574707 test_acc: 0.9344362745098039\n",
      "epoch 1322 total_train_acc: 0.9711447492904447 loss: 0.44085876643657684 test_loss: 0.1056758314371109 test_acc: 0.9313725490196079\n",
      "epoch 1323 total_train_acc: 0.9697256385998108 loss: 0.4320080876350403 test_loss: 0.31494033336639404 test_acc: 0.9325980392156863\n",
      "epoch 1324 total_train_acc: 0.9716177861873226 loss: 0.4232650548219681 test_loss: 0.31998884677886963 test_acc: 0.9344362745098039\n",
      "epoch 1325 total_train_acc: 0.9687795648060549 loss: 0.44666338711977005 test_loss: 0.2089443802833557 test_acc: 0.9362745098039216\n",
      "epoch 1326 total_train_acc: 0.9716177861873226 loss: 0.4341934621334076 test_loss: 0.27113327383995056 test_acc: 0.9325980392156863\n",
      "epoch 1327 total_train_acc: 0.9683065279091769 loss: 0.43263813853263855 test_loss: 0.11586734652519226 test_acc: 0.9332107843137255\n",
      "epoch 1328 total_train_acc: 0.9685430463576159 loss: 0.45099204778671265 test_loss: 0.2543008029460907 test_acc: 0.9319852941176471\n",
      "epoch 1329 total_train_acc: 0.9668874172185431 loss: 0.39874646440148354 test_loss: 0.341910719871521 test_acc: 0.9325980392156863\n",
      "epoch 1330 total_train_acc: 0.967360454115421 loss: 0.45231305062770844 test_loss: 0.07514693588018417 test_acc: 0.9350490196078431\n",
      "epoch 1331 total_train_acc: 0.9716177861873226 loss: 0.4178946390748024 test_loss: 0.2530899941921234 test_acc: 0.9356617647058824\n",
      "epoch 1332 total_train_acc: 0.9704351939451277 loss: 0.41315730661153793 test_loss: 0.22047217190265656 test_acc: 0.9344362745098039\n",
      "epoch 1333 total_train_acc: 0.9713812677388837 loss: 0.44224631786346436 test_loss: 0.19776196777820587 test_acc: 0.9332107843137255\n",
      "epoch 1334 total_train_acc: 0.9697256385998108 loss: 0.4203592464327812 test_loss: 0.03478628769516945 test_acc: 0.9362745098039216\n",
      "epoch 1335 total_train_acc: 0.9706717123935666 loss: 0.4444617033004761 test_loss: 0.0262012779712677 test_acc: 0.9356617647058824\n",
      "epoch 1336 total_train_acc: 0.9671239356669821 loss: 0.4230494573712349 test_loss: 0.23710516095161438 test_acc: 0.9325980392156863\n",
      "epoch 1337 total_train_acc: 0.9697256385998108 loss: 0.40191061049699783 test_loss: 0.2727232873439789 test_acc: 0.9325980392156863\n",
      "epoch 1338 total_train_acc: 0.9716177861873226 loss: 0.4327612966299057 test_loss: 0.19440577924251556 test_acc: 0.9332107843137255\n",
      "epoch 1339 total_train_acc: 0.9694891201513718 loss: 0.404579296708107 test_loss: 0.20226924121379852 test_acc: 0.9319852941176471\n",
      "epoch 1340 total_train_acc: 0.9709082308420057 loss: 0.4158724620938301 test_loss: 0.1425461769104004 test_acc: 0.9307598039215687\n",
      "epoch 1341 total_train_acc: 0.968070009460738 loss: 0.4318661242723465 test_loss: 0.2577002942562103 test_acc: 0.9295343137254902\n",
      "epoch 1342 total_train_acc: 0.9732734153263954 loss: 0.4207111597061157 test_loss: 0.09340330958366394 test_acc: 0.9313725490196079\n",
      "epoch 1343 total_train_acc: 0.9657048249763481 loss: 0.48645226657390594 test_loss: 0.21471017599105835 test_acc: 0.9344362745098039\n",
      "epoch 1344 total_train_acc: 0.9725638599810785 loss: 0.4161210134625435 test_loss: 0.47402405738830566 test_acc: 0.9350490196078431\n",
      "epoch 1345 total_train_acc: 0.9737464522232734 loss: 0.4044213593006134 test_loss: 0.31558912992477417 test_acc: 0.9338235294117647\n",
      "epoch 1346 total_train_acc: 0.9716177861873226 loss: 0.4378368481993675 test_loss: 0.14847803115844727 test_acc: 0.9325980392156863\n",
      "epoch 1347 total_train_acc: 0.9704351939451277 loss: 0.41077394783496857 test_loss: 0.24178637564182281 test_acc: 0.9332107843137255\n",
      "epoch 1348 total_train_acc: 0.9723273415326396 loss: 0.4350692182779312 test_loss: 0.07430548965930939 test_acc: 0.9332107843137255\n",
      "epoch 1349 total_train_acc: 0.9716177861873226 loss: 0.4133724942803383 test_loss: 0.35455867648124695 test_acc: 0.9332107843137255\n",
      "epoch 1350 total_train_acc: 0.9706717123935666 loss: 0.4433613121509552 test_loss: 0.15393680334091187 test_acc: 0.9338235294117647\n",
      "epoch 1351 total_train_acc: 0.9699621570482497 loss: 0.3800687864422798 test_loss: 0.051257576793432236 test_acc: 0.9313725490196079\n",
      "epoch 1352 total_train_acc: 0.9713812677388837 loss: 0.4212277755141258 test_loss: 0.24130664765834808 test_acc: 0.9325980392156863\n",
      "epoch 1353 total_train_acc: 0.9690160832544938 loss: 0.42155715078115463 test_loss: 0.11182557791471481 test_acc: 0.9332107843137255\n",
      "epoch 1354 total_train_acc: 0.9723273415326396 loss: 0.3868996202945709 test_loss: 0.4660876393318176 test_acc: 0.9350490196078431\n",
      "epoch 1355 total_train_acc: 0.9725638599810785 loss: 0.4188619405031204 test_loss: 0.08405350893735886 test_acc: 0.9368872549019608\n",
      "epoch 1356 total_train_acc: 0.9699621570482497 loss: 0.4385695531964302 test_loss: 0.22892487049102783 test_acc: 0.9356617647058824\n",
      "epoch 1357 total_train_acc: 0.9723273415326396 loss: 0.42048027366399765 test_loss: 0.19030842185020447 test_acc: 0.9338235294117647\n",
      "epoch 1358 total_train_acc: 0.967833491012299 loss: 0.41458582133054733 test_loss: 0.08584176748991013 test_acc: 0.9307598039215687\n",
      "epoch 1359 total_train_acc: 0.9683065279091769 loss: 0.42908181995153427 test_loss: 0.4112982153892517 test_acc: 0.9325980392156863\n",
      "epoch 1360 total_train_acc: 0.9720908230842006 loss: 0.40306924656033516 test_loss: 0.36614206433296204 test_acc: 0.9313725490196079\n",
      "epoch 1361 total_train_acc: 0.9713812677388837 loss: 0.4146483317017555 test_loss: 0.3476482927799225 test_acc: 0.9332107843137255\n",
      "epoch 1362 total_train_acc: 0.9687795648060549 loss: 0.4398779571056366 test_loss: 0.26361072063446045 test_acc: 0.9344362745098039\n",
      "epoch 1363 total_train_acc: 0.9737464522232734 loss: 0.42002013325691223 test_loss: 0.2959018349647522 test_acc: 0.9350490196078431\n",
      "epoch 1364 total_train_acc: 0.9692526017029328 loss: 0.4607340171933174 test_loss: 0.3221680521965027 test_acc: 0.9344362745098039\n",
      "epoch 1365 total_train_acc: 0.9713812677388837 loss: 0.44092389941215515 test_loss: 0.13515815138816833 test_acc: 0.9350490196078431\n",
      "epoch 1366 total_train_acc: 0.9728003784295175 loss: 0.42053207755088806 test_loss: 0.0790034607052803 test_acc: 0.9313725490196079\n",
      "epoch 1367 total_train_acc: 0.9690160832544938 loss: 0.4062393754720688 test_loss: 0.22420759499073029 test_acc: 0.9301470588235294\n",
      "epoch 1368 total_train_acc: 0.9692526017029328 loss: 0.4155049957334995 test_loss: 0.09111727774143219 test_acc: 0.9332107843137255\n",
      "epoch 1369 total_train_acc: 0.9694891201513718 loss: 0.43731992691755295 test_loss: 0.1600968986749649 test_acc: 0.9325980392156863\n",
      "epoch 1370 total_train_acc: 0.9718543046357616 loss: 0.40528853237628937 test_loss: 0.2968159317970276 test_acc: 0.9344362745098039\n",
      "epoch 1371 total_train_acc: 0.9723273415326396 loss: 0.38364288210868835 test_loss: 0.2552605867385864 test_acc: 0.9356617647058824\n",
      "epoch 1372 total_train_acc: 0.9704351939451277 loss: 0.45237475633621216 test_loss: 0.11440061777830124 test_acc: 0.9356617647058824\n",
      "epoch 1373 total_train_acc: 0.9704351939451277 loss: 0.42658931761980057 test_loss: 0.3822995722293854 test_acc: 0.9344362745098039\n",
      "epoch 1374 total_train_acc: 0.9697256385998108 loss: 0.44315753877162933 test_loss: 0.04461752250790596 test_acc: 0.9332107843137255\n",
      "epoch 1375 total_train_acc: 0.9690160832544938 loss: 0.4453835263848305 test_loss: 0.38624534010887146 test_acc: 0.9319852941176471\n",
      "epoch 1376 total_train_acc: 0.9730368968779565 loss: 0.44377361983060837 test_loss: 0.2396211475133896 test_acc: 0.9362745098039216\n",
      "epoch 1377 total_train_acc: 0.967833491012299 loss: 0.41593611240386963 test_loss: 0.30672842264175415 test_acc: 0.9344362745098039\n",
      "epoch 1378 total_train_acc: 0.9718543046357616 loss: 0.4317886680364609 test_loss: 0.08224676549434662 test_acc: 0.9350490196078431\n",
      "epoch 1379 total_train_acc: 0.9730368968779565 loss: 0.4304414987564087 test_loss: 0.03958601504564285 test_acc: 0.9344362745098039\n",
      "epoch 1380 total_train_acc: 0.9718543046357616 loss: 0.4294220730662346 test_loss: 0.22640734910964966 test_acc: 0.9344362745098039\n",
      "epoch 1381 total_train_acc: 0.9709082308420057 loss: 0.42360542714595795 test_loss: 0.44931918382644653 test_acc: 0.9319852941176471\n",
      "epoch 1382 total_train_acc: 0.9716177861873226 loss: 0.3937208540737629 test_loss: 0.4468398988246918 test_acc: 0.9332107843137255\n",
      "epoch 1383 total_train_acc: 0.9711447492904447 loss: 0.40856216847896576 test_loss: 0.22877100110054016 test_acc: 0.9344362745098039\n",
      "epoch 1384 total_train_acc: 0.9730368968779565 loss: 0.4065444767475128 test_loss: 0.15919135510921478 test_acc: 0.9332107843137255\n",
      "epoch 1385 total_train_acc: 0.9716177861873226 loss: 0.394596841186285 test_loss: 0.21956691145896912 test_acc: 0.9325980392156863\n",
      "epoch 1386 total_train_acc: 0.9692526017029328 loss: 0.4259880483150482 test_loss: 0.3994652330875397 test_acc: 0.9344362745098039\n",
      "epoch 1387 total_train_acc: 0.9685430463576159 loss: 0.44806164503097534 test_loss: 0.04321575537323952 test_acc: 0.9356617647058824\n",
      "epoch 1388 total_train_acc: 0.9732734153263954 loss: 0.42615868151187897 test_loss: 0.37486377358436584 test_acc: 0.9356617647058824\n",
      "epoch 1389 total_train_acc: 0.9685430463576159 loss: 0.4163033440709114 test_loss: 0.1665373593568802 test_acc: 0.9332107843137255\n",
      "epoch 1390 total_train_acc: 0.9723273415326396 loss: 0.41510146856307983 test_loss: 0.10351303964853287 test_acc: 0.9332107843137255\n",
      "epoch 1391 total_train_acc: 0.9701986754966887 loss: 0.427070215344429 test_loss: 0.06043139472603798 test_acc: 0.9332107843137255\n",
      "epoch 1392 total_train_acc: 0.9716177861873226 loss: 0.4149479940533638 test_loss: 0.291151762008667 test_acc: 0.9325980392156863\n",
      "epoch 1393 total_train_acc: 0.9718543046357616 loss: 0.4073459580540657 test_loss: 0.17593097686767578 test_acc: 0.9332107843137255\n",
      "epoch 1394 total_train_acc: 0.9706717123935666 loss: 0.4356949105858803 test_loss: 0.11305797100067139 test_acc: 0.9350490196078431\n",
      "epoch 1395 total_train_acc: 0.9687795648060549 loss: 0.41349728778004646 test_loss: 0.1458166539669037 test_acc: 0.9332107843137255\n",
      "epoch 1396 total_train_acc: 0.9699621570482497 loss: 0.40849798917770386 test_loss: 0.11722911894321442 test_acc: 0.9350490196078431\n",
      "epoch 1397 total_train_acc: 0.9739829706717124 loss: 0.40228933840990067 test_loss: 0.42187005281448364 test_acc: 0.9350490196078431\n",
      "epoch 1398 total_train_acc: 0.9690160832544938 loss: 0.4183550775051117 test_loss: 0.1324463039636612 test_acc: 0.9368872549019608\n",
      "epoch 1399 total_train_acc: 0.9737464522232734 loss: 0.4062735512852669 test_loss: 0.1592685580253601 test_acc: 0.9338235294117647\n",
      "epoch 1400 total_train_acc: 0.966414380321665 loss: 0.4217880368232727 test_loss: 0.15070009231567383 test_acc: 0.9325980392156863\n",
      "epoch 1401 total_train_acc: 0.9690160832544938 loss: 0.4306904748082161 test_loss: 0.28190532326698303 test_acc: 0.9313725490196079\n",
      "epoch 1402 total_train_acc: 0.9706717123935666 loss: 0.4041399210691452 test_loss: 0.3076910972595215 test_acc: 0.9319852941176471\n",
      "epoch 1403 total_train_acc: 0.9709082308420057 loss: 0.40837234258651733 test_loss: 0.35375967621803284 test_acc: 0.9325980392156863\n",
      "epoch 1404 total_train_acc: 0.96759697256386 loss: 0.4588593877851963 test_loss: 0.07726084440946579 test_acc: 0.9319852941176471\n",
      "epoch 1405 total_train_acc: 0.9737464522232734 loss: 0.4214613363146782 test_loss: 0.0831274464726448 test_acc: 0.9350490196078431\n",
      "epoch 1406 total_train_acc: 0.9690160832544938 loss: 0.42141325771808624 test_loss: 0.3308834731578827 test_acc: 0.9356617647058824\n",
      "epoch 1407 total_train_acc: 0.9706717123935666 loss: 0.4732968285679817 test_loss: 0.17266146838665009 test_acc: 0.9362745098039216\n",
      "epoch 1408 total_train_acc: 0.9739829706717124 loss: 0.39249762892723083 test_loss: 0.28929370641708374 test_acc: 0.9368872549019608\n",
      "epoch 1409 total_train_acc: 0.9744560075685903 loss: 0.3853615112602711 test_loss: 0.1865629255771637 test_acc: 0.9325980392156863\n",
      "epoch 1410 total_train_acc: 0.9694891201513718 loss: 0.42901914566755295 test_loss: 0.07174406200647354 test_acc: 0.9338235294117647\n",
      "epoch 1411 total_train_acc: 0.9694891201513718 loss: 0.40080611780285835 test_loss: 0.14695867896080017 test_acc: 0.9325980392156863\n",
      "epoch 1412 total_train_acc: 0.9737464522232734 loss: 0.3860544338822365 test_loss: 0.06955497711896896 test_acc: 0.9344362745098039\n",
      "epoch 1413 total_train_acc: 0.9683065279091769 loss: 0.47007977962493896 test_loss: 0.18609033524990082 test_acc: 0.9350490196078431\n",
      "epoch 1414 total_train_acc: 0.9694891201513718 loss: 0.4023853987455368 test_loss: 0.13559900224208832 test_acc: 0.9344362745098039\n",
      "epoch 1415 total_train_acc: 0.9690160832544938 loss: 0.4319770410656929 test_loss: 0.11359275877475739 test_acc: 0.9350490196078431\n",
      "epoch 1416 total_train_acc: 0.9709082308420057 loss: 0.4454284682869911 test_loss: 0.05247482657432556 test_acc: 0.9368872549019608\n",
      "epoch 1417 total_train_acc: 0.9709082308420057 loss: 0.4066196158528328 test_loss: 0.35095763206481934 test_acc: 0.9344362745098039\n",
      "epoch 1418 total_train_acc: 0.9737464522232734 loss: 0.42497627437114716 test_loss: 0.03694218024611473 test_acc: 0.9332107843137255\n",
      "epoch 1419 total_train_acc: 0.9685430463576159 loss: 0.42121320217847824 test_loss: 0.0393223837018013 test_acc: 0.9344362745098039\n",
      "epoch 1420 total_train_acc: 0.9687795648060549 loss: 0.42392879724502563 test_loss: 0.3401745855808258 test_acc: 0.9356617647058824\n",
      "epoch 1421 total_train_acc: 0.9723273415326396 loss: 0.3945918492972851 test_loss: 0.17535653710365295 test_acc: 0.9325980392156863\n",
      "epoch 1422 total_train_acc: 0.9728003784295175 loss: 0.3911472335457802 test_loss: 0.23158352077007294 test_acc: 0.9325980392156863\n",
      "epoch 1423 total_train_acc: 0.9718543046357616 loss: 0.4330366775393486 test_loss: 0.212227001786232 test_acc: 0.9356617647058824\n",
      "epoch 1424 total_train_acc: 0.9701986754966887 loss: 0.42410165071487427 test_loss: 0.21666967868804932 test_acc: 0.9356617647058824\n",
      "epoch 1425 total_train_acc: 0.9697256385998108 loss: 0.447459876537323 test_loss: 0.3384104371070862 test_acc: 0.9362745098039216\n",
      "epoch 1426 total_train_acc: 0.9723273415326396 loss: 0.41635414212942123 test_loss: 0.20003114640712738 test_acc: 0.9356617647058824\n",
      "epoch 1427 total_train_acc: 0.9704351939451277 loss: 0.40140365809202194 test_loss: 0.1767280399799347 test_acc: 0.9332107843137255\n",
      "epoch 1428 total_train_acc: 0.9690160832544938 loss: 0.42136672884225845 test_loss: 0.09116457402706146 test_acc: 0.9325980392156863\n",
      "epoch 1429 total_train_acc: 0.9683065279091769 loss: 0.4137215316295624 test_loss: 0.1923196017742157 test_acc: 0.9319852941176471\n",
      "epoch 1430 total_train_acc: 0.9713812677388837 loss: 0.38678213953971863 test_loss: 0.06666421890258789 test_acc: 0.9344362745098039\n",
      "epoch 1431 total_train_acc: 0.9709082308420057 loss: 0.44928689301013947 test_loss: 0.16508100926876068 test_acc: 0.9350490196078431\n",
      "epoch 1432 total_train_acc: 0.9728003784295175 loss: 0.403099000453949 test_loss: 0.18048062920570374 test_acc: 0.9332107843137255\n",
      "epoch 1433 total_train_acc: 0.9730368968779565 loss: 0.4166398048400879 test_loss: 0.15526413917541504 test_acc: 0.9313725490196079\n",
      "epoch 1434 total_train_acc: 0.9687795648060549 loss: 0.4091775044798851 test_loss: 0.15409500896930695 test_acc: 0.9319852941176471\n",
      "epoch 1435 total_train_acc: 0.9751655629139073 loss: 0.37301599606871605 test_loss: 0.4875802993774414 test_acc: 0.9338235294117647\n",
      "epoch 1436 total_train_acc: 0.9716177861873226 loss: 0.37931346148252487 test_loss: 0.28257593512535095 test_acc: 0.9344362745098039\n",
      "epoch 1437 total_train_acc: 0.9718543046357616 loss: 0.400136549025774 test_loss: 0.145705908536911 test_acc: 0.9338235294117647\n",
      "epoch 1438 total_train_acc: 0.9711447492904447 loss: 0.40612590312957764 test_loss: 0.10516461730003357 test_acc: 0.9325980392156863\n",
      "epoch 1439 total_train_acc: 0.96759697256386 loss: 0.4264793023467064 test_loss: 0.3849179744720459 test_acc: 0.9319852941176471\n",
      "epoch 1440 total_train_acc: 0.9725638599810785 loss: 0.40116405487060547 test_loss: 0.09492164105176926 test_acc: 0.9319852941176471\n",
      "epoch 1441 total_train_acc: 0.9761116367076632 loss: 0.4039546400308609 test_loss: 0.054761096835136414 test_acc: 0.9313725490196079\n",
      "epoch 1442 total_train_acc: 0.9728003784295175 loss: 0.40520453453063965 test_loss: 0.14141294360160828 test_acc: 0.9338235294117647\n",
      "epoch 1443 total_train_acc: 0.9709082308420057 loss: 0.431933656334877 test_loss: 0.22988088428974152 test_acc: 0.9344362745098039\n",
      "epoch 1444 total_train_acc: 0.9725638599810785 loss: 0.4417208880186081 test_loss: 0.1069391667842865 test_acc: 0.9356617647058824\n",
      "epoch 1445 total_train_acc: 0.9730368968779565 loss: 0.4351521283388138 test_loss: 0.134556844830513 test_acc: 0.9313725490196079\n",
      "epoch 1446 total_train_acc: 0.9694891201513718 loss: 0.427677221596241 test_loss: 0.2713090777397156 test_acc: 0.9332107843137255\n",
      "epoch 1447 total_train_acc: 0.9701986754966887 loss: 0.4216161295771599 test_loss: 0.044980235397815704 test_acc: 0.9325980392156863\n",
      "epoch 1448 total_train_acc: 0.9701986754966887 loss: 0.4115815758705139 test_loss: 0.13864511251449585 test_acc: 0.9338235294117647\n",
      "epoch 1449 total_train_acc: 0.9690160832544938 loss: 0.40976759791374207 test_loss: 0.032239388674497604 test_acc: 0.9319852941176471\n",
      "epoch 1450 total_train_acc: 0.9661778618732261 loss: 0.42226286232471466 test_loss: 0.2134387344121933 test_acc: 0.9344362745098039\n",
      "epoch 1451 total_train_acc: 0.9732734153263954 loss: 0.44692865014076233 test_loss: 0.1833585500717163 test_acc: 0.9344362745098039\n",
      "epoch 1452 total_train_acc: 0.9701986754966887 loss: 0.4211174175143242 test_loss: 0.22223837673664093 test_acc: 0.9362745098039216\n",
      "epoch 1453 total_train_acc: 0.9709082308420057 loss: 0.4379568174481392 test_loss: 0.25567105412483215 test_acc: 0.9356617647058824\n",
      "epoch 1454 total_train_acc: 0.9718543046357616 loss: 0.3937533125281334 test_loss: 0.2190079689025879 test_acc: 0.9332107843137255\n",
      "epoch 1455 total_train_acc: 0.9683065279091769 loss: 0.44805121421813965 test_loss: 0.20902661979198456 test_acc: 0.9338235294117647\n",
      "epoch 1456 total_train_acc: 0.9728003784295175 loss: 0.4012153372168541 test_loss: 0.6598238348960876 test_acc: 0.9338235294117647\n",
      "epoch 1457 total_train_acc: 0.9725638599810785 loss: 0.4229081869125366 test_loss: 0.3976689279079437 test_acc: 0.9356617647058824\n",
      "epoch 1458 total_train_acc: 0.9735099337748344 loss: 0.41900934278964996 test_loss: 0.2590946853160858 test_acc: 0.9356617647058824\n",
      "epoch 1459 total_train_acc: 0.9716177861873226 loss: 0.41669924557209015 test_loss: 0.038188908249139786 test_acc: 0.9350490196078431\n",
      "epoch 1460 total_train_acc: 0.9709082308420057 loss: 0.44992945343255997 test_loss: 0.13678856194019318 test_acc: 0.9356617647058824\n",
      "epoch 1461 total_train_acc: 0.9704351939451277 loss: 0.4079228937625885 test_loss: 0.1762908399105072 test_acc: 0.9356617647058824\n",
      "epoch 1462 total_train_acc: 0.9716177861873226 loss: 0.40049055963754654 test_loss: 0.4014042615890503 test_acc: 0.9362745098039216\n",
      "epoch 1463 total_train_acc: 0.9739829706717124 loss: 0.4102839529514313 test_loss: 0.2824757397174835 test_acc: 0.9344362745098039\n",
      "epoch 1464 total_train_acc: 0.9744560075685903 loss: 0.38340239971876144 test_loss: 0.1565181016921997 test_acc: 0.9344362745098039\n",
      "epoch 1465 total_train_acc: 0.9706717123935666 loss: 0.3690146282315254 test_loss: 0.156891867518425 test_acc: 0.9332107843137255\n",
      "epoch 1466 total_train_acc: 0.9687795648060549 loss: 0.4090399816632271 test_loss: 0.06004224717617035 test_acc: 0.9350490196078431\n",
      "epoch 1467 total_train_acc: 0.9720908230842006 loss: 0.4146144613623619 test_loss: 0.11070651561021805 test_acc: 0.9344362745098039\n",
      "epoch 1468 total_train_acc: 0.9704351939451277 loss: 0.3840026222169399 test_loss: 0.28863778710365295 test_acc: 0.9362745098039216\n",
      "epoch 1469 total_train_acc: 0.9699621570482497 loss: 0.4363720268011093 test_loss: 0.1992071270942688 test_acc: 0.9356617647058824\n",
      "epoch 1470 total_train_acc: 0.9725638599810785 loss: 0.42111921310424805 test_loss: 0.5041905045509338 test_acc: 0.9356617647058824\n",
      "epoch 1471 total_train_acc: 0.9730368968779565 loss: 0.41034605354070663 test_loss: 0.30314767360687256 test_acc: 0.9350490196078431\n",
      "epoch 1472 total_train_acc: 0.9711447492904447 loss: 0.40250708907842636 test_loss: 0.6535671949386597 test_acc: 0.9338235294117647\n",
      "epoch 1473 total_train_acc: 0.9709082308420057 loss: 0.44216030091047287 test_loss: 0.10056093335151672 test_acc: 0.9356617647058824\n",
      "epoch 1474 total_train_acc: 0.9720908230842006 loss: 0.42410507798194885 test_loss: 0.1253628432750702 test_acc: 0.9338235294117647\n",
      "epoch 1475 total_train_acc: 0.9690160832544938 loss: 0.4261733368039131 test_loss: 0.0093916654586792 test_acc: 0.9332107843137255\n",
      "epoch 1476 total_train_acc: 0.9711447492904447 loss: 0.4042201340198517 test_loss: 0.4107862710952759 test_acc: 0.9319852941176471\n",
      "epoch 1477 total_train_acc: 0.967360454115421 loss: 0.4249376133084297 test_loss: 0.20258256793022156 test_acc: 0.9325980392156863\n",
      "epoch 1478 total_train_acc: 0.9701986754966887 loss: 0.418963260948658 test_loss: 0.15208332240581512 test_acc: 0.9350490196078431\n",
      "epoch 1479 total_train_acc: 0.9690160832544938 loss: 0.39910631626844406 test_loss: 0.295138955116272 test_acc: 0.9362745098039216\n",
      "epoch 1480 total_train_acc: 0.9716177861873226 loss: 0.4123387783765793 test_loss: 0.26532620191574097 test_acc: 0.9350490196078431\n",
      "epoch 1481 total_train_acc: 0.9725638599810785 loss: 0.4259282425045967 test_loss: 0.2144993543624878 test_acc: 0.9319852941176471\n",
      "epoch 1482 total_train_acc: 0.9704351939451277 loss: 0.4174323081970215 test_loss: 0.1596415787935257 test_acc: 0.9325980392156863\n",
      "epoch 1483 total_train_acc: 0.9746925260170294 loss: 0.4005589485168457 test_loss: 0.2992151975631714 test_acc: 0.9325980392156863\n",
      "epoch 1484 total_train_acc: 0.9699621570482497 loss: 0.4115471765398979 test_loss: 0.08114327490329742 test_acc: 0.9356617647058824\n",
      "epoch 1485 total_train_acc: 0.9685430463576159 loss: 0.4405020549893379 test_loss: 0.2671012878417969 test_acc: 0.9350490196078431\n",
      "epoch 1486 total_train_acc: 0.9718543046357616 loss: 0.38371138274669647 test_loss: 0.1286734640598297 test_acc: 0.9332107843137255\n",
      "epoch 1487 total_train_acc: 0.9723273415326396 loss: 0.40384966880083084 test_loss: 0.07258135080337524 test_acc: 0.9325980392156863\n",
      "epoch 1488 total_train_acc: 0.9723273415326396 loss: 0.3938501551747322 test_loss: 0.16809192299842834 test_acc: 0.9338235294117647\n",
      "epoch 1489 total_train_acc: 0.9694891201513718 loss: 0.4232488349080086 test_loss: 0.031255196779966354 test_acc: 0.9350490196078431\n",
      "epoch 1490 total_train_acc: 0.9758751182592242 loss: 0.36857694387435913 test_loss: 0.30613973736763 test_acc: 0.9325980392156863\n",
      "epoch 1491 total_train_acc: 0.9725638599810785 loss: 0.37201154232025146 test_loss: 0.1338869035243988 test_acc: 0.9325980392156863\n",
      "epoch 1492 total_train_acc: 0.9732734153263954 loss: 0.4019688069820404 test_loss: 0.37684082984924316 test_acc: 0.9325980392156863\n",
      "epoch 1493 total_train_acc: 0.9692526017029328 loss: 0.39301057904958725 test_loss: 0.1560070514678955 test_acc: 0.9332107843137255\n",
      "epoch 1494 total_train_acc: 0.9723273415326396 loss: 0.40779972821474075 test_loss: 0.11033160984516144 test_acc: 0.9338235294117647\n",
      "epoch 1495 total_train_acc: 0.9732734153263954 loss: 0.38056571409106255 test_loss: 0.12921705842018127 test_acc: 0.9332107843137255\n",
      "epoch 1496 total_train_acc: 0.9713812677388837 loss: 0.40406909584999084 test_loss: 0.0279336329549551 test_acc: 0.9319852941176471\n",
      "epoch 1497 total_train_acc: 0.9711447492904447 loss: 0.3971576765179634 test_loss: 0.2678147852420807 test_acc: 0.9338235294117647\n",
      "epoch 1498 total_train_acc: 0.9706717123935666 loss: 0.4133378341794014 test_loss: 0.350507527589798 test_acc: 0.9332107843137255\n",
      "epoch 1499 total_train_acc: 0.9728003784295175 loss: 0.3905141204595566 test_loss: 0.3256857991218567 test_acc: 0.9338235294117647\n",
      "epoch 1500 total_train_acc: 0.9711447492904447 loss: 0.4057271182537079 test_loss: 0.1092531830072403 test_acc: 0.9332107843137255\n",
      "epoch 1501 total_train_acc: 0.9697256385998108 loss: 0.3879573568701744 test_loss: 0.5804967284202576 test_acc: 0.9313725490196079\n",
      "epoch 1502 total_train_acc: 0.9723273415326396 loss: 0.40425805747509 test_loss: 0.3792725205421448 test_acc: 0.9325980392156863\n",
      "epoch 1503 total_train_acc: 0.968070009460738 loss: 0.41728439927101135 test_loss: 0.23795612156391144 test_acc: 0.9313725490196079\n",
      "epoch 1504 total_train_acc: 0.9690160832544938 loss: 0.408392496407032 test_loss: 0.40353044867515564 test_acc: 0.9344362745098039\n",
      "epoch 1505 total_train_acc: 0.9739829706717124 loss: 0.3902515545487404 test_loss: 0.030344786122441292 test_acc: 0.9338235294117647\n",
      "epoch 1506 total_train_acc: 0.9739829706717124 loss: 0.38038480281829834 test_loss: 0.042830031365156174 test_acc: 0.9344362745098039\n",
      "epoch 1507 total_train_acc: 0.9671239356669821 loss: 0.39936941489577293 test_loss: 0.029242850840091705 test_acc: 0.9350490196078431\n",
      "epoch 1508 total_train_acc: 0.9716177861873226 loss: 0.4782370775938034 test_loss: 0.2905025780200958 test_acc: 0.9350490196078431\n",
      "epoch 1509 total_train_acc: 0.9751655629139073 loss: 0.36876609921455383 test_loss: 0.14741666615009308 test_acc: 0.9325980392156863\n",
      "epoch 1510 total_train_acc: 0.9735099337748344 loss: 0.3783728778362274 test_loss: 0.1612497717142105 test_acc: 0.9332107843137255\n",
      "epoch 1511 total_train_acc: 0.9742194891201513 loss: 0.38002561032772064 test_loss: 0.182297483086586 test_acc: 0.9338235294117647\n",
      "epoch 1512 total_train_acc: 0.9732734153263954 loss: 0.409615233540535 test_loss: 0.22264908254146576 test_acc: 0.9325980392156863\n",
      "epoch 1513 total_train_acc: 0.9735099337748344 loss: 0.37273724749684334 test_loss: 0.26482343673706055 test_acc: 0.9338235294117647\n",
      "epoch 1514 total_train_acc: 0.9709082308420057 loss: 0.40700262039899826 test_loss: 0.42010724544525146 test_acc: 0.9332107843137255\n",
      "epoch 1515 total_train_acc: 0.9711447492904447 loss: 0.3888007290661335 test_loss: 0.0806322768330574 test_acc: 0.9325980392156863\n",
      "epoch 1516 total_train_acc: 0.9699621570482497 loss: 0.41243717819452286 test_loss: 0.6320623755455017 test_acc: 0.9338235294117647\n",
      "epoch 1517 total_train_acc: 0.9709082308420057 loss: 0.3958294168114662 test_loss: 0.283203125 test_acc: 0.9381127450980392\n",
      "epoch 1518 total_train_acc: 0.9694891201513718 loss: 0.4675231873989105 test_loss: 0.5067651867866516 test_acc: 0.9381127450980392\n",
      "epoch 1519 total_train_acc: 0.9749290444654684 loss: 0.38366132229566574 test_loss: 0.31447601318359375 test_acc: 0.9362745098039216\n",
      "epoch 1520 total_train_acc: 0.9716177861873226 loss: 0.4111102893948555 test_loss: 0.018920348957180977 test_acc: 0.9344362745098039\n",
      "epoch 1521 total_train_acc: 0.9730368968779565 loss: 0.41598033905029297 test_loss: 0.08710763603448868 test_acc: 0.9350490196078431\n",
      "epoch 1522 total_train_acc: 0.9735099337748344 loss: 0.3991841822862625 test_loss: 0.02862098626792431 test_acc: 0.9344362745098039\n",
      "epoch 1523 total_train_acc: 0.9737464522232734 loss: 0.4169303849339485 test_loss: 0.11760453879833221 test_acc: 0.9356617647058824\n",
      "epoch 1524 total_train_acc: 0.9711447492904447 loss: 0.42535679042339325 test_loss: 0.21067094802856445 test_acc: 0.9350490196078431\n",
      "epoch 1525 total_train_acc: 0.9754020813623463 loss: 0.40444234013557434 test_loss: 0.32081979513168335 test_acc: 0.9381127450980392\n",
      "epoch 1526 total_train_acc: 0.9716177861873226 loss: 0.38275037333369255 test_loss: 0.04785783588886261 test_acc: 0.9362745098039216\n",
      "epoch 1527 total_train_acc: 0.9713812677388837 loss: 0.42735622078180313 test_loss: 0.1525983065366745 test_acc: 0.9307598039215687\n",
      "epoch 1528 total_train_acc: 0.9728003784295175 loss: 0.38727597892284393 test_loss: 0.8413549661636353 test_acc: 0.9313725490196079\n",
      "epoch 1529 total_train_acc: 0.9758751182592242 loss: 0.38262349367141724 test_loss: 0.24733766913414001 test_acc: 0.9301470588235294\n",
      "epoch 1530 total_train_acc: 0.9723273415326396 loss: 0.40632522851228714 test_loss: 0.08326832205057144 test_acc: 0.9313725490196079\n",
      "epoch 1531 total_train_acc: 0.9711447492904447 loss: 0.39849793910980225 test_loss: 0.23462072014808655 test_acc: 0.9338235294117647\n",
      "epoch 1532 total_train_acc: 0.9723273415326396 loss: 0.3862964138388634 test_loss: 0.38388127088546753 test_acc: 0.9344362745098039\n",
      "epoch 1533 total_train_acc: 0.9720908230842006 loss: 0.39327121526002884 test_loss: 0.39653441309928894 test_acc: 0.9325980392156863\n",
      "epoch 1534 total_train_acc: 0.9720908230842006 loss: 0.3986010178923607 test_loss: 0.6044702529907227 test_acc: 0.9344362745098039\n",
      "epoch 1535 total_train_acc: 0.9746925260170294 loss: 0.3881972059607506 test_loss: 0.14329349994659424 test_acc: 0.9338235294117647\n",
      "epoch 1536 total_train_acc: 0.9725638599810785 loss: 0.4460822641849518 test_loss: 0.7578960061073303 test_acc: 0.9338235294117647\n",
      "epoch 1537 total_train_acc: 0.9749290444654684 loss: 0.3942166343331337 test_loss: 0.1415420025587082 test_acc: 0.9325980392156863\n",
      "epoch 1538 total_train_acc: 0.9699621570482497 loss: 0.3976004347205162 test_loss: 0.13850827515125275 test_acc: 0.9332107843137255\n",
      "epoch 1539 total_train_acc: 0.9711447492904447 loss: 0.42218615859746933 test_loss: 0.2448338121175766 test_acc: 0.9307598039215687\n",
      "epoch 1540 total_train_acc: 0.9718543046357616 loss: 0.42339611053466797 test_loss: 0.07369985431432724 test_acc: 0.9332107843137255\n",
      "epoch 1541 total_train_acc: 0.9749290444654684 loss: 0.3540998362004757 test_loss: 0.0848642960190773 test_acc: 0.9338235294117647\n",
      "epoch 1542 total_train_acc: 0.9728003784295175 loss: 0.3673539273440838 test_loss: 0.7670850157737732 test_acc: 0.9325980392156863\n",
      "epoch 1543 total_train_acc: 0.9739829706717124 loss: 0.38459157571196556 test_loss: 0.07842963933944702 test_acc: 0.9325980392156863\n",
      "epoch 1544 total_train_acc: 0.9728003784295175 loss: 0.36939020454883575 test_loss: 0.06245090439915657 test_acc: 0.9344362745098039\n",
      "epoch 1545 total_train_acc: 0.9732734153263954 loss: 0.3904774561524391 test_loss: 0.0760655328631401 test_acc: 0.9325980392156863\n",
      "epoch 1546 total_train_acc: 0.9730368968779565 loss: 0.3884604685008526 test_loss: 0.15485158562660217 test_acc: 0.9350490196078431\n",
      "epoch 1547 total_train_acc: 0.9737464522232734 loss: 0.3747025765478611 test_loss: 0.15721438825130463 test_acc: 0.9362745098039216\n",
      "epoch 1548 total_train_acc: 0.9706717123935666 loss: 0.40448934584856033 test_loss: 0.46016746759414673 test_acc: 0.9362745098039216\n",
      "epoch 1549 total_train_acc: 0.9730368968779565 loss: 0.39842788875102997 test_loss: 0.14666438102722168 test_acc: 0.9344362745098039\n",
      "epoch 1550 total_train_acc: 0.9735099337748344 loss: 0.39450883865356445 test_loss: 0.15836113691329956 test_acc: 0.9313725490196079\n",
      "epoch 1551 total_train_acc: 0.9704351939451277 loss: 0.43982864171266556 test_loss: 0.07624533772468567 test_acc: 0.9319852941176471\n",
      "epoch 1552 total_train_acc: 0.9711447492904447 loss: 0.3790808990597725 test_loss: 0.2450082153081894 test_acc: 0.9350490196078431\n",
      "epoch 1553 total_train_acc: 0.9742194891201513 loss: 0.37006639316678047 test_loss: 0.4805091321468353 test_acc: 0.9338235294117647\n",
      "epoch 1554 total_train_acc: 0.9728003784295175 loss: 0.39329810440540314 test_loss: 0.19639411568641663 test_acc: 0.9344362745098039\n",
      "epoch 1555 total_train_acc: 0.9704351939451277 loss: 0.4018160179257393 test_loss: 0.1723906397819519 test_acc: 0.9350490196078431\n",
      "epoch 1556 total_train_acc: 0.9732734153263954 loss: 0.39501985162496567 test_loss: 0.364638090133667 test_acc: 0.9356617647058824\n",
      "epoch 1557 total_train_acc: 0.9713812677388837 loss: 0.4034225791692734 test_loss: 0.19039122760295868 test_acc: 0.9344362745098039\n",
      "epoch 1558 total_train_acc: 0.9718543046357616 loss: 0.3926878944039345 test_loss: 0.15674075484275818 test_acc: 0.9338235294117647\n",
      "epoch 1559 total_train_acc: 0.9739829706717124 loss: 0.396819107234478 test_loss: 0.4158988296985626 test_acc: 0.9338235294117647\n",
      "epoch 1560 total_train_acc: 0.9723273415326396 loss: 0.407977931201458 test_loss: 0.11199633777141571 test_acc: 0.9356617647058824\n",
      "epoch 1561 total_train_acc: 0.9720908230842006 loss: 0.3783477917313576 test_loss: 0.12853650748729706 test_acc: 0.9356617647058824\n",
      "epoch 1562 total_train_acc: 0.9730368968779565 loss: 0.4133997708559036 test_loss: 0.23994046449661255 test_acc: 0.9350490196078431\n",
      "epoch 1563 total_train_acc: 0.9716177861873226 loss: 0.3900974690914154 test_loss: 0.0876181498169899 test_acc: 0.9368872549019608\n",
      "epoch 1564 total_train_acc: 0.9758751182592242 loss: 0.4003280699253082 test_loss: 0.06581477075815201 test_acc: 0.9350490196078431\n",
      "epoch 1565 total_train_acc: 0.9718543046357616 loss: 0.39240966737270355 test_loss: 0.4617912769317627 test_acc: 0.9350490196078431\n",
      "epoch 1566 total_train_acc: 0.9704351939451277 loss: 0.4223807454109192 test_loss: 0.3047495484352112 test_acc: 0.9344362745098039\n",
      "epoch 1567 total_train_acc: 0.9720908230842006 loss: 0.41355710476636887 test_loss: 0.3007820248603821 test_acc: 0.9338235294117647\n",
      "epoch 1568 total_train_acc: 0.9737464522232734 loss: 0.3617522120475769 test_loss: 0.07153018563985825 test_acc: 0.9344362745098039\n",
      "epoch 1569 total_train_acc: 0.9728003784295175 loss: 0.3879529908299446 test_loss: 0.04220634698867798 test_acc: 0.9332107843137255\n",
      "epoch 1570 total_train_acc: 0.9692526017029328 loss: 0.4133547842502594 test_loss: 0.12292955815792084 test_acc: 0.9338235294117647\n",
      "epoch 1571 total_train_acc: 0.9735099337748344 loss: 0.4060233533382416 test_loss: 0.04286503791809082 test_acc: 0.9338235294117647\n",
      "epoch 1572 total_train_acc: 0.9725638599810785 loss: 0.3930596485733986 test_loss: 0.14890597760677338 test_acc: 0.9325980392156863\n",
      "epoch 1573 total_train_acc: 0.9706717123935666 loss: 0.39835669845342636 test_loss: 0.47807496786117554 test_acc: 0.9338235294117647\n",
      "epoch 1574 total_train_acc: 0.9754020813623463 loss: 0.40917523950338364 test_loss: 0.15546083450317383 test_acc: 0.9325980392156863\n",
      "epoch 1575 total_train_acc: 0.9735099337748344 loss: 0.3794362097978592 test_loss: 0.42550188302993774 test_acc: 0.9356617647058824\n",
      "epoch 1576 total_train_acc: 0.9728003784295175 loss: 0.3750252388417721 test_loss: 0.26210761070251465 test_acc: 0.9344362745098039\n",
      "epoch 1577 total_train_acc: 0.9709082308420057 loss: 0.41337618976831436 test_loss: 0.20780201256275177 test_acc: 0.9350490196078431\n",
      "epoch 1578 total_train_acc: 0.9732734153263954 loss: 0.36919405683875084 test_loss: 0.05012085661292076 test_acc: 0.9356617647058824\n",
      "epoch 1579 total_train_acc: 0.9716177861873226 loss: 0.408630333840847 test_loss: 0.025024987757205963 test_acc: 0.9344362745098039\n",
      "epoch 1580 total_train_acc: 0.9728003784295175 loss: 0.40926211327314377 test_loss: 0.01982518658041954 test_acc: 0.9350490196078431\n",
      "epoch 1581 total_train_acc: 0.9711447492904447 loss: 0.39130672067403793 test_loss: 0.8063782453536987 test_acc: 0.9332107843137255\n",
      "epoch 1582 total_train_acc: 0.9709082308420057 loss: 0.38759928941726685 test_loss: 0.25320157408714294 test_acc: 0.9338235294117647\n",
      "epoch 1583 total_train_acc: 0.9756385998107853 loss: 0.36145589873194695 test_loss: 0.22315934300422668 test_acc: 0.9338235294117647\n",
      "epoch 1584 total_train_acc: 0.9709082308420057 loss: 0.39153952896595 test_loss: 0.4536711573600769 test_acc: 0.9338235294117647\n",
      "epoch 1585 total_train_acc: 0.9716177861873226 loss: 0.40224216133356094 test_loss: 0.21641360223293304 test_acc: 0.9338235294117647\n",
      "epoch 1586 total_train_acc: 0.9772942289498581 loss: 0.37555093318223953 test_loss: 0.0512382909655571 test_acc: 0.9319852941176471\n",
      "epoch 1587 total_train_acc: 0.9732734153263954 loss: 0.3931497856974602 test_loss: 0.12624317407608032 test_acc: 0.9319852941176471\n",
      "epoch 1588 total_train_acc: 0.9718543046357616 loss: 0.39264532923698425 test_loss: 0.11257763206958771 test_acc: 0.9362745098039216\n",
      "epoch 1589 total_train_acc: 0.9716177861873226 loss: 0.37591737508773804 test_loss: 0.26625576615333557 test_acc: 0.9362745098039216\n",
      "epoch 1590 total_train_acc: 0.9746925260170294 loss: 0.41013485938310623 test_loss: 0.06068292260169983 test_acc: 0.9338235294117647\n",
      "epoch 1591 total_train_acc: 0.9713812677388837 loss: 0.40319572389125824 test_loss: 0.1221412643790245 test_acc: 0.9313725490196079\n",
      "epoch 1592 total_train_acc: 0.9709082308420057 loss: 0.4024273604154587 test_loss: 0.42190811038017273 test_acc: 0.9313725490196079\n",
      "epoch 1593 total_train_acc: 0.9737464522232734 loss: 0.38526683300733566 test_loss: 0.22193807363510132 test_acc: 0.9350490196078431\n",
      "epoch 1594 total_train_acc: 0.9690160832544938 loss: 0.44628119468688965 test_loss: 0.2096225768327713 test_acc: 0.9350490196078431\n",
      "epoch 1595 total_train_acc: 0.9730368968779565 loss: 0.38545387983322144 test_loss: 0.2986886203289032 test_acc: 0.9350490196078431\n",
      "epoch 1596 total_train_acc: 0.9746925260170294 loss: 0.38623344898223877 test_loss: 0.05420423299074173 test_acc: 0.9319852941176471\n",
      "epoch 1597 total_train_acc: 0.9718543046357616 loss: 0.40239978581666946 test_loss: 0.323598176240921 test_acc: 0.9325980392156863\n",
      "epoch 1598 total_train_acc: 0.9730368968779565 loss: 0.37048518657684326 test_loss: 0.27274516224861145 test_acc: 0.9307598039215687\n",
      "epoch 1599 total_train_acc: 0.9720908230842006 loss: 0.4156888872385025 test_loss: 0.3107204735279083 test_acc: 0.9338235294117647\n",
      "epoch 1600 total_train_acc: 0.9716177861873226 loss: 0.3905012160539627 test_loss: 0.8696244359016418 test_acc: 0.9338235294117647\n",
      "epoch 1601 total_train_acc: 0.9709082308420057 loss: 0.42542123794555664 test_loss: 0.027407264336943626 test_acc: 0.9350490196078431\n",
      "epoch 1602 total_train_acc: 0.9725638599810785 loss: 0.40840424597263336 test_loss: 0.014972416684031487 test_acc: 0.9350490196078431\n",
      "epoch 1603 total_train_acc: 0.9728003784295175 loss: 0.3805245943367481 test_loss: 0.18613994121551514 test_acc: 0.9350490196078431\n",
      "epoch 1604 total_train_acc: 0.9706717123935666 loss: 0.40198031067848206 test_loss: 0.02914242260158062 test_acc: 0.9338235294117647\n",
      "epoch 1605 total_train_acc: 0.9709082308420057 loss: 0.39193469285964966 test_loss: 0.30889755487442017 test_acc: 0.9338235294117647\n",
      "epoch 1606 total_train_acc: 0.9701986754966887 loss: 0.4117090627551079 test_loss: 0.14806491136550903 test_acc: 0.9325980392156863\n",
      "epoch 1607 total_train_acc: 0.9749290444654684 loss: 0.3875502273440361 test_loss: 0.29845505952835083 test_acc: 0.9338235294117647\n",
      "epoch 1608 total_train_acc: 0.9754020813623463 loss: 0.38359686732292175 test_loss: 0.35501328110694885 test_acc: 0.9332107843137255\n",
      "epoch 1609 total_train_acc: 0.9716177861873226 loss: 0.3780379258096218 test_loss: 0.2483164370059967 test_acc: 0.9338235294117647\n",
      "epoch 1610 total_train_acc: 0.9728003784295175 loss: 0.385341577231884 test_loss: 0.11299069970846176 test_acc: 0.9325980392156863\n",
      "epoch 1611 total_train_acc: 0.9723273415326396 loss: 0.3896380886435509 test_loss: 0.34361037611961365 test_acc: 0.9332107843137255\n",
      "epoch 1612 total_train_acc: 0.9744560075685903 loss: 0.38643259555101395 test_loss: 0.19496463239192963 test_acc: 0.9350490196078431\n",
      "epoch 1613 total_train_acc: 0.9716177861873226 loss: 0.40284086763858795 test_loss: 0.07570579648017883 test_acc: 0.9338235294117647\n",
      "epoch 1614 total_train_acc: 0.9761116367076632 loss: 0.36401785910129547 test_loss: 0.35425907373428345 test_acc: 0.9344362745098039\n",
      "epoch 1615 total_train_acc: 0.9730368968779565 loss: 0.38555818051099777 test_loss: 0.2715238034725189 test_acc: 0.9325980392156863\n",
      "epoch 1616 total_train_acc: 0.9737464522232734 loss: 0.3746800273656845 test_loss: 0.1676187962293625 test_acc: 0.9332107843137255\n",
      "epoch 1617 total_train_acc: 0.9711447492904447 loss: 0.39314455538988113 test_loss: 0.1034359410405159 test_acc: 0.9344362745098039\n",
      "epoch 1618 total_train_acc: 0.9706717123935666 loss: 0.43212907761335373 test_loss: 0.6287457346916199 test_acc: 0.9344362745098039\n",
      "epoch 1619 total_train_acc: 0.9735099337748344 loss: 0.40981772541999817 test_loss: 0.17183904349803925 test_acc: 0.9344362745098039\n",
      "epoch 1620 total_train_acc: 0.9701986754966887 loss: 0.436459518969059 test_loss: 0.06631709635257721 test_acc: 0.9344362745098039\n",
      "epoch 1621 total_train_acc: 0.9701986754966887 loss: 0.3720588684082031 test_loss: 0.20770776271820068 test_acc: 0.9344362745098039\n",
      "epoch 1622 total_train_acc: 0.9728003784295175 loss: 0.3840247467160225 test_loss: 0.13271503150463104 test_acc: 0.9338235294117647\n",
      "epoch 1623 total_train_acc: 0.9704351939451277 loss: 0.38881032168865204 test_loss: 0.06324237585067749 test_acc: 0.9319852941176471\n",
      "epoch 1624 total_train_acc: 0.9749290444654684 loss: 0.3644852228462696 test_loss: 0.1378898024559021 test_acc: 0.9325980392156863\n",
      "epoch 1625 total_train_acc: 0.9720908230842006 loss: 0.4091342017054558 test_loss: 0.03682232275605202 test_acc: 0.9344362745098039\n",
      "epoch 1626 total_train_acc: 0.9739829706717124 loss: 0.3834126256406307 test_loss: 0.28529927134513855 test_acc: 0.9338235294117647\n",
      "epoch 1627 total_train_acc: 0.9728003784295175 loss: 0.3724794127047062 test_loss: 0.04787994921207428 test_acc: 0.9338235294117647\n",
      "epoch 1628 total_train_acc: 0.9713812677388837 loss: 0.4030187055468559 test_loss: 0.3035954236984253 test_acc: 0.9338235294117647\n",
      "epoch 1629 total_train_acc: 0.9744560075685903 loss: 0.3908214196562767 test_loss: 0.47280630469322205 test_acc: 0.9338235294117647\n",
      "epoch 1630 total_train_acc: 0.9697256385998108 loss: 0.4207707345485687 test_loss: 0.5049266815185547 test_acc: 0.9332107843137255\n",
      "epoch 1631 total_train_acc: 0.9723273415326396 loss: 0.40387681126594543 test_loss: 0.16306906938552856 test_acc: 0.9325980392156863\n",
      "epoch 1632 total_train_acc: 0.9706717123935666 loss: 0.38234010338783264 test_loss: 0.13511252403259277 test_acc: 0.9325980392156863\n",
      "epoch 1633 total_train_acc: 0.9732734153263954 loss: 0.3930748850107193 test_loss: 0.13801628351211548 test_acc: 0.9338235294117647\n",
      "epoch 1634 total_train_acc: 0.9730368968779565 loss: 0.37185341492295265 test_loss: 0.4666261672973633 test_acc: 0.9344362745098039\n",
      "epoch 1635 total_train_acc: 0.9723273415326396 loss: 0.41621048748493195 test_loss: 0.10981352627277374 test_acc: 0.9325980392156863\n",
      "epoch 1636 total_train_acc: 0.9730368968779565 loss: 0.37324386462569237 test_loss: 0.1556137651205063 test_acc: 0.9332107843137255\n",
      "epoch 1637 total_train_acc: 0.9746925260170294 loss: 0.3709816075861454 test_loss: 0.23615513741970062 test_acc: 0.9313725490196079\n",
      "epoch 1638 total_train_acc: 0.9746925260170294 loss: 0.36633873730897903 test_loss: 0.14363393187522888 test_acc: 0.9325980392156863\n",
      "epoch 1639 total_train_acc: 0.9744560075685903 loss: 0.38057803362607956 test_loss: 0.12888309359550476 test_acc: 0.9338235294117647\n",
      "epoch 1640 total_train_acc: 0.9765846736045412 loss: 0.36821865662932396 test_loss: 0.1560620665550232 test_acc: 0.9344362745098039\n",
      "epoch 1641 total_train_acc: 0.9744560075685903 loss: 0.3915207274258137 test_loss: 0.2727310359477997 test_acc: 0.9350490196078431\n",
      "epoch 1642 total_train_acc: 0.9758751182592242 loss: 0.405069962143898 test_loss: 0.21834762394428253 test_acc: 0.9350490196078431\n",
      "epoch 1643 total_train_acc: 0.9744560075685903 loss: 0.3883834406733513 test_loss: 0.06144912168383598 test_acc: 0.9344362745098039\n",
      "epoch 1644 total_train_acc: 0.9720908230842006 loss: 0.4138185940682888 test_loss: 0.12551170587539673 test_acc: 0.9338235294117647\n",
      "epoch 1645 total_train_acc: 0.9737464522232734 loss: 0.3959873169660568 test_loss: 0.29731589555740356 test_acc: 0.9338235294117647\n",
      "epoch 1646 total_train_acc: 0.9699621570482497 loss: 0.39618413895368576 test_loss: 0.005089538171887398 test_acc: 0.9344362745098039\n",
      "epoch 1647 total_train_acc: 0.9706717123935666 loss: 0.382837887853384 test_loss: 0.2942124307155609 test_acc: 0.9350490196078431\n",
      "epoch 1648 total_train_acc: 0.9723273415326396 loss: 0.39560820907354355 test_loss: 0.5517021417617798 test_acc: 0.9332107843137255\n",
      "epoch 1649 total_train_acc: 0.9754020813623463 loss: 0.39382797479629517 test_loss: 0.27588918805122375 test_acc: 0.9338235294117647\n",
      "epoch 1650 total_train_acc: 0.9720908230842006 loss: 0.3758665584027767 test_loss: 0.18819089233875275 test_acc: 0.9325980392156863\n",
      "epoch 1651 total_train_acc: 0.9754020813623463 loss: 0.38300473615527153 test_loss: 0.18808376789093018 test_acc: 0.9325980392156863\n",
      "epoch 1652 total_train_acc: 0.9739829706717124 loss: 0.3880975767970085 test_loss: 0.5224891304969788 test_acc: 0.9344362745098039\n",
      "epoch 1653 total_train_acc: 0.9720908230842006 loss: 0.42387480288743973 test_loss: 0.04961426183581352 test_acc: 0.9338235294117647\n",
      "epoch 1654 total_train_acc: 0.9725638599810785 loss: 0.4216283932328224 test_loss: 0.08811967074871063 test_acc: 0.9338235294117647\n",
      "epoch 1655 total_train_acc: 0.9732734153263954 loss: 0.3723140209913254 test_loss: 0.10318367928266525 test_acc: 0.9356617647058824\n",
      "epoch 1656 total_train_acc: 0.9744560075685903 loss: 0.38729672878980637 test_loss: 0.33541351556777954 test_acc: 0.9362745098039216\n",
      "epoch 1657 total_train_acc: 0.9723273415326396 loss: 0.39999065548181534 test_loss: 0.1737169623374939 test_acc: 0.9344362745098039\n",
      "epoch 1658 total_train_acc: 0.9697256385998108 loss: 0.37602290511131287 test_loss: 0.16145430505275726 test_acc: 0.9338235294117647\n",
      "epoch 1659 total_train_acc: 0.9732734153263954 loss: 0.396132230758667 test_loss: 0.17009153962135315 test_acc: 0.9332107843137255\n",
      "epoch 1660 total_train_acc: 0.9713812677388837 loss: 0.4093017727136612 test_loss: 0.25885945558547974 test_acc: 0.9307598039215687\n",
      "epoch 1661 total_train_acc: 0.9723273415326396 loss: 0.3926396518945694 test_loss: 0.0448743961751461 test_acc: 0.9319852941176471\n",
      "epoch 1662 total_train_acc: 0.9739829706717124 loss: 0.38231299072504044 test_loss: 0.1654922515153885 test_acc: 0.9332107843137255\n",
      "epoch 1663 total_train_acc: 0.9735099337748344 loss: 0.35494663938879967 test_loss: 0.044976606965065 test_acc: 0.9338235294117647\n",
      "epoch 1664 total_train_acc: 0.9742194891201513 loss: 0.36458366736769676 test_loss: 0.5227130055427551 test_acc: 0.9338235294117647\n",
      "epoch 1665 total_train_acc: 0.9713812677388837 loss: 0.38664352148771286 test_loss: 0.484681099653244 test_acc: 0.9332107843137255\n",
      "epoch 1666 total_train_acc: 0.9718543046357616 loss: 0.38139764219522476 test_loss: 0.2596563398838043 test_acc: 0.9319852941176471\n",
      "epoch 1667 total_train_acc: 0.9742194891201513 loss: 0.38871897757053375 test_loss: 0.2304985523223877 test_acc: 0.9325980392156863\n",
      "epoch 1668 total_train_acc: 0.9704351939451277 loss: 0.39551452547311783 test_loss: 0.4177606999874115 test_acc: 0.9332107843137255\n",
      "epoch 1669 total_train_acc: 0.9763481551561022 loss: 0.35165077820420265 test_loss: 0.09856926649808884 test_acc: 0.9338235294117647\n",
      "epoch 1670 total_train_acc: 0.9728003784295175 loss: 0.3961409032344818 test_loss: 0.05959998816251755 test_acc: 0.9338235294117647\n",
      "epoch 1671 total_train_acc: 0.9751655629139073 loss: 0.40182051062583923 test_loss: 0.3809341788291931 test_acc: 0.9338235294117647\n",
      "epoch 1672 total_train_acc: 0.9718543046357616 loss: 0.41846515983343124 test_loss: 0.20875144004821777 test_acc: 0.9350490196078431\n",
      "epoch 1673 total_train_acc: 0.9709082308420057 loss: 0.4132693111896515 test_loss: 0.12598347663879395 test_acc: 0.9338235294117647\n",
      "epoch 1674 total_train_acc: 0.9713812677388837 loss: 0.3848692700266838 test_loss: 0.09634508937597275 test_acc: 0.9338235294117647\n",
      "epoch 1675 total_train_acc: 0.9758751182592242 loss: 0.37279876694083214 test_loss: 0.06854798644781113 test_acc: 0.9344362745098039\n",
      "epoch 1676 total_train_acc: 0.9728003784295175 loss: 0.4179261475801468 test_loss: 0.11537373811006546 test_acc: 0.9332107843137255\n",
      "epoch 1677 total_train_acc: 0.9709082308420057 loss: 0.3997572660446167 test_loss: 0.4040079414844513 test_acc: 0.9332107843137255\n",
      "epoch 1678 total_train_acc: 0.9725638599810785 loss: 0.3988586738705635 test_loss: 0.18296290934085846 test_acc: 0.9325980392156863\n",
      "epoch 1679 total_train_acc: 0.9758751182592242 loss: 0.3713950589299202 test_loss: 0.17440499365329742 test_acc: 0.9344362745098039\n",
      "epoch 1680 total_train_acc: 0.9720908230842006 loss: 0.3841875419020653 test_loss: 0.09953590482473373 test_acc: 0.9344362745098039\n",
      "epoch 1681 total_train_acc: 0.9732734153263954 loss: 0.3637012355029583 test_loss: 0.06709987670183182 test_acc: 0.9338235294117647\n",
      "epoch 1682 total_train_acc: 0.9723273415326396 loss: 0.3905579671263695 test_loss: 0.006874220911413431 test_acc: 0.9338235294117647\n",
      "epoch 1683 total_train_acc: 0.9735099337748344 loss: 0.35730020329356194 test_loss: 0.1473757028579712 test_acc: 0.9344362745098039\n",
      "epoch 1684 total_train_acc: 0.9737464522232734 loss: 0.3444853201508522 test_loss: 0.07933475822210312 test_acc: 0.9319852941176471\n",
      "epoch 1685 total_train_acc: 0.9732734153263954 loss: 0.3883770480751991 test_loss: 0.05787709727883339 test_acc: 0.9313725490196079\n",
      "epoch 1686 total_train_acc: 0.9709082308420057 loss: 0.40338391810655594 test_loss: 0.01701529510319233 test_acc: 0.9325980392156863\n",
      "epoch 1687 total_train_acc: 0.9756385998107853 loss: 0.3849107399582863 test_loss: 0.2335427701473236 test_acc: 0.9332107843137255\n",
      "epoch 1688 total_train_acc: 0.9723273415326396 loss: 0.39167284220457077 test_loss: 0.15180635452270508 test_acc: 0.9332107843137255\n",
      "epoch 1689 total_train_acc: 0.9716177861873226 loss: 0.3646544814109802 test_loss: 0.009799289517104626 test_acc: 0.9325980392156863\n",
      "epoch 1690 total_train_acc: 0.9749290444654684 loss: 0.36931681632995605 test_loss: 0.23181962966918945 test_acc: 0.9325980392156863\n",
      "epoch 1691 total_train_acc: 0.9730368968779565 loss: 0.37326793745160103 test_loss: 0.3097620904445648 test_acc: 0.9325980392156863\n",
      "epoch 1692 total_train_acc: 0.9723273415326396 loss: 0.39938823133707047 test_loss: 0.14080578088760376 test_acc: 0.9332107843137255\n",
      "epoch 1693 total_train_acc: 0.9723273415326396 loss: 0.42461302876472473 test_loss: 0.05748986080288887 test_acc: 0.9350490196078431\n",
      "epoch 1694 total_train_acc: 0.9749290444654684 loss: 0.3687993474304676 test_loss: 0.05763651430606842 test_acc: 0.9350490196078431\n",
      "epoch 1695 total_train_acc: 0.9716177861873226 loss: 0.4085840880870819 test_loss: 0.11777576804161072 test_acc: 0.9356617647058824\n",
      "epoch 1696 total_train_acc: 0.9768211920529801 loss: 0.3668924905359745 test_loss: 0.33161500096321106 test_acc: 0.9338235294117647\n",
      "epoch 1697 total_train_acc: 0.9751655629139073 loss: 0.39689718186855316 test_loss: 0.22530531883239746 test_acc: 0.9350490196078431\n",
      "epoch 1698 total_train_acc: 0.9728003784295175 loss: 0.37247299775481224 test_loss: 0.10178463160991669 test_acc: 0.9338235294117647\n",
      "epoch 1699 total_train_acc: 0.9730368968779565 loss: 0.39309488236904144 test_loss: 0.023426493629813194 test_acc: 0.9332107843137255\n",
      "epoch 1700 total_train_acc: 0.9723273415326396 loss: 0.36160271242260933 test_loss: 0.170378178358078 test_acc: 0.9319852941176471\n",
      "epoch 1701 total_train_acc: 0.9711447492904447 loss: 0.38039005547761917 test_loss: 0.10087867081165314 test_acc: 0.9301470588235294\n",
      "epoch 1702 total_train_acc: 0.9749290444654684 loss: 0.3663463741540909 test_loss: 0.013011902570724487 test_acc: 0.9301470588235294\n",
      "epoch 1703 total_train_acc: 0.9742194891201513 loss: 0.35523809492588043 test_loss: 0.29603877663612366 test_acc: 0.9307598039215687\n",
      "epoch 1704 total_train_acc: 0.9709082308420057 loss: 0.4071541726589203 test_loss: 0.4040239453315735 test_acc: 0.9301470588235294\n",
      "epoch 1705 total_train_acc: 0.9739829706717124 loss: 0.3709735833108425 test_loss: 0.2899836599826813 test_acc: 0.9325980392156863\n",
      "epoch 1706 total_train_acc: 0.9718543046357616 loss: 0.4364015907049179 test_loss: 0.10579969733953476 test_acc: 0.9325980392156863\n",
      "epoch 1707 total_train_acc: 0.9716177861873226 loss: 0.3829413503408432 test_loss: 0.2750754952430725 test_acc: 0.9338235294117647\n",
      "epoch 1708 total_train_acc: 0.9739829706717124 loss: 0.37291962653398514 test_loss: 0.5925926566123962 test_acc: 0.9338235294117647\n",
      "epoch 1709 total_train_acc: 0.9756385998107853 loss: 0.358747161924839 test_loss: 0.055720631033182144 test_acc: 0.9332107843137255\n",
      "epoch 1710 total_train_acc: 0.9720908230842006 loss: 0.36817002296447754 test_loss: 0.2185840904712677 test_acc: 0.9325980392156863\n",
      "epoch 1711 total_train_acc: 0.9720908230842006 loss: 0.37336740642786026 test_loss: 0.08839745819568634 test_acc: 0.9325980392156863\n",
      "epoch 1712 total_train_acc: 0.9763481551561022 loss: 0.37766724079847336 test_loss: 0.43320539593696594 test_acc: 0.9313725490196079\n",
      "epoch 1713 total_train_acc: 0.9739829706717124 loss: 0.39244063943624496 test_loss: 0.14254999160766602 test_acc: 0.9307598039215687\n",
      "epoch 1714 total_train_acc: 0.9713812677388837 loss: 0.36238378658890724 test_loss: 0.19746339321136475 test_acc: 0.9332107843137255\n",
      "epoch 1715 total_train_acc: 0.9706717123935666 loss: 0.4538459777832031 test_loss: 0.8364390134811401 test_acc: 0.9344362745098039\n",
      "epoch 1716 total_train_acc: 0.9763481551561022 loss: 0.36449140310287476 test_loss: 0.12272214889526367 test_acc: 0.9362745098039216\n",
      "epoch 1717 total_train_acc: 0.9725638599810785 loss: 0.3939516246318817 test_loss: 0.13698996603488922 test_acc: 0.9350490196078431\n",
      "epoch 1718 total_train_acc: 0.9739829706717124 loss: 0.3596036694943905 test_loss: 0.365318238735199 test_acc: 0.9344362745098039\n",
      "epoch 1719 total_train_acc: 0.9716177861873226 loss: 0.38260646164417267 test_loss: 0.20491492748260498 test_acc: 0.9344362745098039\n",
      "epoch 1720 total_train_acc: 0.9735099337748344 loss: 0.37625752389431 test_loss: 0.17925915122032166 test_acc: 0.9338235294117647\n",
      "epoch 1721 total_train_acc: 0.9713812677388837 loss: 0.38693099096417427 test_loss: 0.26301324367523193 test_acc: 0.9338235294117647\n",
      "epoch 1722 total_train_acc: 0.9720908230842006 loss: 0.368825551122427 test_loss: 0.16228748857975006 test_acc: 0.9325980392156863\n",
      "epoch 1723 total_train_acc: 0.9718543046357616 loss: 0.3967916667461395 test_loss: 0.14747931063175201 test_acc: 0.9325980392156863\n",
      "epoch 1724 total_train_acc: 0.9739829706717124 loss: 0.3386522587388754 test_loss: 0.5331613421440125 test_acc: 0.9344362745098039\n",
      "epoch 1725 total_train_acc: 0.9739829706717124 loss: 0.3992408737540245 test_loss: 0.2704380452632904 test_acc: 0.9350490196078431\n",
      "epoch 1726 total_train_acc: 0.9697256385998108 loss: 0.39743877574801445 test_loss: 0.5400338172912598 test_acc: 0.9332107843137255\n",
      "epoch 1727 total_train_acc: 0.9735099337748344 loss: 0.3953673094511032 test_loss: 0.15108264982700348 test_acc: 0.9332107843137255\n",
      "epoch 1728 total_train_acc: 0.9699621570482497 loss: 0.3935113698244095 test_loss: 0.12439285218715668 test_acc: 0.9338235294117647\n",
      "epoch 1729 total_train_acc: 0.9713812677388837 loss: 0.3656557649374008 test_loss: 0.0842859148979187 test_acc: 0.9338235294117647\n",
      "epoch 1730 total_train_acc: 0.9706717123935666 loss: 0.39226730912923813 test_loss: 0.1269206553697586 test_acc: 0.9344362745098039\n",
      "epoch 1731 total_train_acc: 0.9732734153263954 loss: 0.36591916531324387 test_loss: 0.33631497621536255 test_acc: 0.9344362745098039\n",
      "epoch 1732 total_train_acc: 0.9725638599810785 loss: 0.39093000441789627 test_loss: 0.09963969886302948 test_acc: 0.9338235294117647\n",
      "epoch 1733 total_train_acc: 0.9761116367076632 loss: 0.3549588918685913 test_loss: 0.22639797627925873 test_acc: 0.9332107843137255\n",
      "epoch 1734 total_train_acc: 0.9763481551561022 loss: 0.347573209553957 test_loss: 0.061101268976926804 test_acc: 0.9332107843137255\n",
      "epoch 1735 total_train_acc: 0.9723273415326396 loss: 0.40248189866542816 test_loss: 0.3063220977783203 test_acc: 0.9325980392156863\n",
      "epoch 1736 total_train_acc: 0.9692526017029328 loss: 0.4300294518470764 test_loss: 0.24143049120903015 test_acc: 0.9344362745098039\n",
      "epoch 1737 total_train_acc: 0.9711447492904447 loss: 0.3907191902399063 test_loss: 0.3469497859477997 test_acc: 0.9332107843137255\n",
      "epoch 1738 total_train_acc: 0.9699621570482497 loss: 0.3784707449376583 test_loss: 0.07260848581790924 test_acc: 0.9338235294117647\n",
      "epoch 1739 total_train_acc: 0.9723273415326396 loss: 0.4076233133673668 test_loss: 0.1417880654335022 test_acc: 0.9325980392156863\n",
      "epoch 1740 total_train_acc: 0.9706717123935666 loss: 0.37270117551088333 test_loss: 0.4412356913089752 test_acc: 0.9325980392156863\n",
      "epoch 1741 total_train_acc: 0.9772942289498581 loss: 0.3557935357093811 test_loss: 0.14556840062141418 test_acc: 0.9325980392156863\n",
      "epoch 1742 total_train_acc: 0.9728003784295175 loss: 0.36578523740172386 test_loss: 0.09814810007810593 test_acc: 0.9344362745098039\n",
      "epoch 1743 total_train_acc: 0.9713812677388837 loss: 0.3554001711308956 test_loss: 0.20036573708057404 test_acc: 0.9344362745098039\n",
      "epoch 1744 total_train_acc: 0.9739829706717124 loss: 0.36154573783278465 test_loss: 0.2417331039905548 test_acc: 0.9344362745098039\n",
      "epoch 1745 total_train_acc: 0.9746925260170294 loss: 0.38120390474796295 test_loss: 0.13774575293064117 test_acc: 0.9344362745098039\n",
      "epoch 1746 total_train_acc: 0.9720908230842006 loss: 0.3920031562447548 test_loss: 0.3113240599632263 test_acc: 0.9356617647058824\n",
      "epoch 1747 total_train_acc: 0.9716177861873226 loss: 0.37750307470560074 test_loss: 0.6507214903831482 test_acc: 0.9338235294117647\n",
      "epoch 1748 total_train_acc: 0.9720908230842006 loss: 0.3913398012518883 test_loss: 0.31636354327201843 test_acc: 0.9356617647058824\n",
      "epoch 1749 total_train_acc: 0.9737464522232734 loss: 0.3835940286517143 test_loss: 0.035216134041547775 test_acc: 0.9350490196078431\n",
      "epoch 1750 total_train_acc: 0.9739829706717124 loss: 0.3898472189903259 test_loss: 0.3608110845088959 test_acc: 0.9350490196078431\n",
      "epoch 1751 total_train_acc: 0.9720908230842006 loss: 0.4514753371477127 test_loss: 0.2715618312358856 test_acc: 0.9350490196078431\n",
      "epoch 1752 total_train_acc: 0.9699621570482497 loss: 0.3780893348157406 test_loss: 0.5371086597442627 test_acc: 0.9350490196078431\n",
      "epoch 1753 total_train_acc: 0.9728003784295175 loss: 0.41541142389178276 test_loss: 0.26894500851631165 test_acc: 0.9356617647058824\n",
      "epoch 1754 total_train_acc: 0.9737464522232734 loss: 0.36108747869729996 test_loss: 0.3331299126148224 test_acc: 0.9350490196078431\n",
      "epoch 1755 total_train_acc: 0.9730368968779565 loss: 0.37007541954517365 test_loss: 0.12408078461885452 test_acc: 0.9350490196078431\n",
      "epoch 1756 total_train_acc: 0.9746925260170294 loss: 0.3684491626918316 test_loss: 0.13769736886024475 test_acc: 0.9325980392156863\n",
      "epoch 1757 total_train_acc: 0.9754020813623463 loss: 0.3517550639808178 test_loss: 0.2757127285003662 test_acc: 0.9338235294117647\n",
      "epoch 1758 total_train_acc: 0.9723273415326396 loss: 0.43401728570461273 test_loss: 0.02042149193584919 test_acc: 0.9325980392156863\n",
      "epoch 1759 total_train_acc: 0.9720908230842006 loss: 0.3738764338195324 test_loss: 0.03926417976617813 test_acc: 0.9338235294117647\n",
      "epoch 1760 total_train_acc: 0.9728003784295175 loss: 0.3625337742269039 test_loss: 0.48977750539779663 test_acc: 0.9332107843137255\n",
      "epoch 1761 total_train_acc: 0.9742194891201513 loss: 0.3720859885215759 test_loss: 0.26790085434913635 test_acc: 0.9332107843137255\n",
      "epoch 1762 total_train_acc: 0.9728003784295175 loss: 0.36645515263080597 test_loss: 0.43680980801582336 test_acc: 0.9338235294117647\n",
      "epoch 1763 total_train_acc: 0.9713812677388837 loss: 0.39479105174541473 test_loss: 0.037367649376392365 test_acc: 0.9350490196078431\n",
      "epoch 1764 total_train_acc: 0.9728003784295175 loss: 0.38224954903125763 test_loss: 0.34632089734077454 test_acc: 0.9368872549019608\n",
      "epoch 1765 total_train_acc: 0.9751655629139073 loss: 0.36611779779195786 test_loss: 0.051021646708250046 test_acc: 0.9362745098039216\n",
      "epoch 1766 total_train_acc: 0.9735099337748344 loss: 0.41188953071832657 test_loss: 0.41300758719444275 test_acc: 0.9356617647058824\n",
      "epoch 1767 total_train_acc: 0.9711447492904447 loss: 0.40527020394802094 test_loss: 0.26492929458618164 test_acc: 0.9362745098039216\n",
      "epoch 1768 total_train_acc: 0.9737464522232734 loss: 0.39103972166776657 test_loss: 0.09363763779401779 test_acc: 0.9332107843137255\n",
      "epoch 1769 total_train_acc: 0.9744560075685903 loss: 0.3995170406997204 test_loss: 0.18347257375717163 test_acc: 0.9338235294117647\n",
      "epoch 1770 total_train_acc: 0.9751655629139073 loss: 0.37093422561883926 test_loss: 0.13739630579948425 test_acc: 0.9344362745098039\n",
      "epoch 1771 total_train_acc: 0.9728003784295175 loss: 0.38007257878780365 test_loss: 0.24655750393867493 test_acc: 0.9350490196078431\n",
      "epoch 1772 total_train_acc: 0.9732734153263954 loss: 0.3908601179718971 test_loss: 0.19850507378578186 test_acc: 0.9350490196078431\n",
      "epoch 1773 total_train_acc: 0.9718543046357616 loss: 0.38545913249254227 test_loss: 0.08298744261264801 test_acc: 0.9356617647058824\n",
      "epoch 1774 total_train_acc: 0.9732734153263954 loss: 0.40737760066986084 test_loss: 0.18855184316635132 test_acc: 0.9356617647058824\n",
      "epoch 1775 total_train_acc: 0.9725638599810785 loss: 0.3869820013642311 test_loss: 0.14820505678653717 test_acc: 0.9350490196078431\n",
      "epoch 1776 total_train_acc: 0.9735099337748344 loss: 0.3767010495066643 test_loss: 0.24604609608650208 test_acc: 0.9356617647058824\n",
      "epoch 1777 total_train_acc: 0.9751655629139073 loss: 0.37584660947322845 test_loss: 0.17987816035747528 test_acc: 0.9319852941176471\n",
      "epoch 1778 total_train_acc: 0.9718543046357616 loss: 0.38045138493180275 test_loss: 0.041714128106832504 test_acc: 0.9313725490196079\n",
      "epoch 1779 total_train_acc: 0.9737464522232734 loss: 0.3758302591741085 test_loss: 0.25560882687568665 test_acc: 0.9338235294117647\n",
      "epoch 1780 total_train_acc: 0.9730368968779565 loss: 0.4197424426674843 test_loss: 0.5315160155296326 test_acc: 0.9344362745098039\n",
      "epoch 1781 total_train_acc: 0.9735099337748344 loss: 0.3648649677634239 test_loss: 0.2389000803232193 test_acc: 0.9350490196078431\n",
      "epoch 1782 total_train_acc: 0.9706717123935666 loss: 0.4295835420489311 test_loss: 0.07744793593883514 test_acc: 0.9350490196078431\n",
      "epoch 1783 total_train_acc: 0.9730368968779565 loss: 0.3781522735953331 test_loss: 0.11313154548406601 test_acc: 0.9338235294117647\n",
      "epoch 1784 total_train_acc: 0.9692526017029328 loss: 0.42491239309310913 test_loss: 0.41409990191459656 test_acc: 0.9344362745098039\n",
      "epoch 1785 total_train_acc: 0.9739829706717124 loss: 0.3641126863658428 test_loss: 0.47364553809165955 test_acc: 0.9356617647058824\n",
      "epoch 1786 total_train_acc: 0.9713812677388837 loss: 0.3898223862051964 test_loss: 0.2706280052661896 test_acc: 0.9350490196078431\n",
      "epoch 1787 total_train_acc: 0.9735099337748344 loss: 0.37140852212905884 test_loss: 0.34203118085861206 test_acc: 0.9338235294117647\n",
      "epoch 1788 total_train_acc: 0.9739829706717124 loss: 0.38403312116861343 test_loss: 0.07618040591478348 test_acc: 0.9344362745098039\n",
      "epoch 1789 total_train_acc: 0.9725638599810785 loss: 0.37828442081809044 test_loss: 0.12097883969545364 test_acc: 0.9350490196078431\n",
      "epoch 1790 total_train_acc: 0.9739829706717124 loss: 0.3917170315980911 test_loss: 0.1085275188088417 test_acc: 0.9350490196078431\n",
      "epoch 1791 total_train_acc: 0.9716177861873226 loss: 0.4293069541454315 test_loss: 0.25202521681785583 test_acc: 0.9350490196078431\n",
      "epoch 1792 total_train_acc: 0.9716177861873226 loss: 0.4211287945508957 test_loss: 0.3206329345703125 test_acc: 0.9344362745098039\n",
      "epoch 1793 total_train_acc: 0.9758751182592242 loss: 0.36313824355602264 test_loss: 0.40626218914985657 test_acc: 0.9350490196078431\n",
      "epoch 1794 total_train_acc: 0.9739829706717124 loss: 0.3989783823490143 test_loss: 0.3775808811187744 test_acc: 0.9344362745098039\n",
      "epoch 1795 total_train_acc: 0.9761116367076632 loss: 0.40589912235736847 test_loss: 0.17741651833057404 test_acc: 0.9325980392156863\n",
      "epoch 1796 total_train_acc: 0.9742194891201513 loss: 0.36744991317391396 test_loss: 0.14990492165088654 test_acc: 0.9325980392156863\n",
      "epoch 1797 total_train_acc: 0.9725638599810785 loss: 0.3827072009444237 test_loss: 0.05343605577945709 test_acc: 0.9332107843137255\n",
      "epoch 1798 total_train_acc: 0.9751655629139073 loss: 0.34129566699266434 test_loss: 0.17704038321971893 test_acc: 0.9325980392156863\n",
      "epoch 1799 total_train_acc: 0.9751655629139073 loss: 0.38580433279275894 test_loss: 0.7734401226043701 test_acc: 0.9338235294117647\n",
      "epoch 1800 total_train_acc: 0.9723273415326396 loss: 0.3891066163778305 test_loss: 0.059887949377298355 test_acc: 0.9332107843137255\n",
      "epoch 1801 total_train_acc: 0.9751655629139073 loss: 0.3971305191516876 test_loss: 0.45629432797431946 test_acc: 0.9319852941176471\n",
      "epoch 1802 total_train_acc: 0.9730368968779565 loss: 0.3700553700327873 test_loss: 0.1349519044160843 test_acc: 0.9332107843137255\n",
      "epoch 1803 total_train_acc: 0.9758751182592242 loss: 0.37019598484039307 test_loss: 0.0640607550740242 test_acc: 0.9338235294117647\n",
      "epoch 1804 total_train_acc: 0.9742194891201513 loss: 0.41206515580415726 test_loss: 0.2368103414773941 test_acc: 0.9332107843137255\n",
      "epoch 1805 total_train_acc: 0.9730368968779565 loss: 0.3808370493352413 test_loss: 0.14977803826332092 test_acc: 0.9356617647058824\n",
      "epoch 1806 total_train_acc: 0.9746925260170294 loss: 0.3916965574026108 test_loss: 0.16904006898403168 test_acc: 0.9332107843137255\n",
      "epoch 1807 total_train_acc: 0.9770577105014191 loss: 0.4019889831542969 test_loss: 0.28602150082588196 test_acc: 0.9338235294117647\n",
      "epoch 1808 total_train_acc: 0.9728003784295175 loss: 0.42346782237291336 test_loss: 0.22898124158382416 test_acc: 0.9338235294117647\n",
      "epoch 1809 total_train_acc: 0.9725638599810785 loss: 0.3834080100059509 test_loss: 0.12514494359493256 test_acc: 0.9344362745098039\n",
      "epoch 1810 total_train_acc: 0.9744560075685903 loss: 0.37953243032097816 test_loss: 0.047238387167453766 test_acc: 0.9344362745098039\n",
      "epoch 1811 total_train_acc: 0.9758751182592242 loss: 0.4086219221353531 test_loss: 0.39001065492630005 test_acc: 0.9338235294117647\n",
      "epoch 1812 total_train_acc: 0.9732734153263954 loss: 0.39104101806879044 test_loss: 0.4208349585533142 test_acc: 0.9350490196078431\n",
      "epoch 1813 total_train_acc: 0.9737464522232734 loss: 0.3737919107079506 test_loss: 0.018529700115323067 test_acc: 0.9350490196078431\n",
      "epoch 1814 total_train_acc: 0.9725638599810785 loss: 0.3949151709675789 test_loss: 0.4228508472442627 test_acc: 0.9344362745098039\n",
      "epoch 1815 total_train_acc: 0.9742194891201513 loss: 0.3867141008377075 test_loss: 0.18258298933506012 test_acc: 0.9338235294117647\n",
      "epoch 1816 total_train_acc: 0.9723273415326396 loss: 0.4068760350346565 test_loss: 0.27421829104423523 test_acc: 0.9338235294117647\n",
      "epoch 1817 total_train_acc: 0.9744560075685903 loss: 0.37261293083429337 test_loss: 0.3149172365665436 test_acc: 0.9332107843137255\n",
      "epoch 1818 total_train_acc: 0.9761116367076632 loss: 0.3535464443266392 test_loss: 0.32317036390304565 test_acc: 0.9338235294117647\n",
      "epoch 1819 total_train_acc: 0.9737464522232734 loss: 0.36595800146460533 test_loss: 0.20828327536582947 test_acc: 0.9338235294117647\n",
      "epoch 1820 total_train_acc: 0.9770577105014191 loss: 0.35068998858332634 test_loss: 0.12155599892139435 test_acc: 0.9350490196078431\n",
      "epoch 1821 total_train_acc: 0.9742194891201513 loss: 0.37189971655607224 test_loss: 0.04933376982808113 test_acc: 0.9344362745098039\n",
      "epoch 1822 total_train_acc: 0.978713339640492 loss: 0.3648844175040722 test_loss: 0.3228267729282379 test_acc: 0.9344362745098039\n",
      "epoch 1823 total_train_acc: 0.9746925260170294 loss: 0.39976320415735245 test_loss: 0.05524831637740135 test_acc: 0.9338235294117647\n",
      "epoch 1824 total_train_acc: 0.9749290444654684 loss: 0.36101941764354706 test_loss: 0.12591470777988434 test_acc: 0.9356617647058824\n",
      "epoch 1825 total_train_acc: 0.9735099337748344 loss: 0.35885027796030045 test_loss: 0.37873226404190063 test_acc: 0.9356617647058824\n",
      "epoch 1826 total_train_acc: 0.9765846736045412 loss: 0.35043787583708763 test_loss: 0.086571104824543 test_acc: 0.9338235294117647\n",
      "epoch 1827 total_train_acc: 0.9737464522232734 loss: 0.3662388026714325 test_loss: 0.17886361479759216 test_acc: 0.9332107843137255\n",
      "epoch 1828 total_train_acc: 0.9720908230842006 loss: 0.4072195068001747 test_loss: 0.12614470720291138 test_acc: 0.9350490196078431\n",
      "epoch 1829 total_train_acc: 0.9735099337748344 loss: 0.3973199278116226 test_loss: 0.30901017785072327 test_acc: 0.9344362745098039\n",
      "epoch 1830 total_train_acc: 0.9720908230842006 loss: 0.3823756091296673 test_loss: 0.18702378869056702 test_acc: 0.9350490196078431\n",
      "epoch 1831 total_train_acc: 0.9742194891201513 loss: 0.3871431425213814 test_loss: 0.5917689204216003 test_acc: 0.9350490196078431\n",
      "epoch 1832 total_train_acc: 0.9713812677388837 loss: 0.43535906821489334 test_loss: 0.3834691643714905 test_acc: 0.9356617647058824\n",
      "epoch 1833 total_train_acc: 0.9742194891201513 loss: 0.37687835842370987 test_loss: 0.19105085730552673 test_acc: 0.9350490196078431\n",
      "epoch 1834 total_train_acc: 0.9725638599810785 loss: 0.38767847418785095 test_loss: 0.11638442426919937 test_acc: 0.9356617647058824\n",
      "epoch 1835 total_train_acc: 0.9728003784295175 loss: 0.3756771422922611 test_loss: 0.12997324764728546 test_acc: 0.9344362745098039\n",
      "epoch 1836 total_train_acc: 0.9735099337748344 loss: 0.3642613887786865 test_loss: 0.19240409135818481 test_acc: 0.9344362745098039\n",
      "epoch 1837 total_train_acc: 0.9725638599810785 loss: 0.3844776973128319 test_loss: 0.2447868287563324 test_acc: 0.9332107843137255\n",
      "epoch 1838 total_train_acc: 0.9732734153263954 loss: 0.3457255996763706 test_loss: 0.22719120979309082 test_acc: 0.9338235294117647\n",
      "epoch 1839 total_train_acc: 0.9723273415326396 loss: 0.378561407327652 test_loss: 0.4472106993198395 test_acc: 0.9350490196078431\n",
      "epoch 1840 total_train_acc: 0.9756385998107853 loss: 0.38225995749235153 test_loss: 0.18940380215644836 test_acc: 0.9338235294117647\n",
      "epoch 1841 total_train_acc: 0.9730368968779565 loss: 0.35617778450250626 test_loss: 0.04843069985508919 test_acc: 0.9338235294117647\n",
      "epoch 1842 total_train_acc: 0.9732734153263954 loss: 0.38990187644958496 test_loss: 0.20909684896469116 test_acc: 0.9332107843137255\n",
      "epoch 1843 total_train_acc: 0.9716177861873226 loss: 0.3985001668334007 test_loss: 0.102320596575737 test_acc: 0.9338235294117647\n",
      "epoch 1844 total_train_acc: 0.9725638599810785 loss: 0.39522096514701843 test_loss: 0.3583562970161438 test_acc: 0.9344362745098039\n",
      "epoch 1845 total_train_acc: 0.9744560075685903 loss: 0.3327937088906765 test_loss: 0.4197201728820801 test_acc: 0.9350490196078431\n",
      "epoch 1846 total_train_acc: 0.9754020813623463 loss: 0.3685704246163368 test_loss: 0.3085772395133972 test_acc: 0.9344362745098039\n",
      "epoch 1847 total_train_acc: 0.9761116367076632 loss: 0.3657703101634979 test_loss: 0.19918762147426605 test_acc: 0.9344362745098039\n",
      "epoch 1848 total_train_acc: 0.9728003784295175 loss: 0.40540145337581635 test_loss: 0.7022786140441895 test_acc: 0.9350490196078431\n",
      "epoch 1849 total_train_acc: 0.9758751182592242 loss: 0.390640988945961 test_loss: 0.07083244621753693 test_acc: 0.9338235294117647\n",
      "epoch 1850 total_train_acc: 0.9744560075685903 loss: 0.38091613352298737 test_loss: 0.18963254988193512 test_acc: 0.9332107843137255\n",
      "epoch 1851 total_train_acc: 0.9737464522232734 loss: 0.3772483989596367 test_loss: 0.08596137166023254 test_acc: 0.9332107843137255\n",
      "epoch 1852 total_train_acc: 0.9709082308420057 loss: 0.43492262065410614 test_loss: 0.2587164640426636 test_acc: 0.9332107843137255\n",
      "epoch 1853 total_train_acc: 0.9704351939451277 loss: 0.3924004137516022 test_loss: 0.06684193015098572 test_acc: 0.9338235294117647\n",
      "epoch 1854 total_train_acc: 0.9749290444654684 loss: 0.38179903477430344 test_loss: 0.14481210708618164 test_acc: 0.9338235294117647\n",
      "epoch 1855 total_train_acc: 0.9720908230842006 loss: 0.382502056658268 test_loss: 0.08556950092315674 test_acc: 0.9350490196078431\n",
      "epoch 1856 total_train_acc: 0.9723273415326396 loss: 0.36170270293951035 test_loss: 0.10192529857158661 test_acc: 0.9325980392156863\n",
      "epoch 1857 total_train_acc: 0.9749290444654684 loss: 0.3650420159101486 test_loss: 0.10161325335502625 test_acc: 0.9325980392156863\n",
      "epoch 1858 total_train_acc: 0.9737464522232734 loss: 0.3917732238769531 test_loss: 0.021680472418665886 test_acc: 0.9344362745098039\n",
      "epoch 1859 total_train_acc: 0.9728003784295175 loss: 0.39492782950401306 test_loss: 0.2482050061225891 test_acc: 0.9338235294117647\n",
      "epoch 1860 total_train_acc: 0.9749290444654684 loss: 0.3682968094944954 test_loss: 0.2713068425655365 test_acc: 0.9332107843137255\n",
      "epoch 1861 total_train_acc: 0.9735099337748344 loss: 0.35647357255220413 test_loss: 0.03114909492433071 test_acc: 0.9332107843137255\n",
      "epoch 1862 total_train_acc: 0.9746925260170294 loss: 0.37330956757068634 test_loss: 0.24892908334732056 test_acc: 0.9338235294117647\n",
      "epoch 1863 total_train_acc: 0.9744560075685903 loss: 0.3775618299841881 test_loss: 0.1618318408727646 test_acc: 0.9332107843137255\n",
      "epoch 1864 total_train_acc: 0.9723273415326396 loss: 0.42105118185281754 test_loss: 0.3877975344657898 test_acc: 0.9332107843137255\n",
      "epoch 1865 total_train_acc: 0.9751655629139073 loss: 0.39946528524160385 test_loss: 0.24471153318881989 test_acc: 0.9344362745098039\n",
      "epoch 1866 total_train_acc: 0.9725638599810785 loss: 0.38899918645620346 test_loss: 0.13702593743801117 test_acc: 0.9356617647058824\n",
      "epoch 1867 total_train_acc: 0.9720908230842006 loss: 0.36002594977617264 test_loss: 0.14688241481781006 test_acc: 0.9356617647058824\n",
      "epoch 1868 total_train_acc: 0.9758751182592242 loss: 0.4046407416462898 test_loss: 0.17228251695632935 test_acc: 0.9344362745098039\n",
      "epoch 1869 total_train_acc: 0.9768211920529801 loss: 0.35009830445051193 test_loss: 0.029095469042658806 test_acc: 0.9344362745098039\n",
      "epoch 1870 total_train_acc: 0.9763481551561022 loss: 0.36629264056682587 test_loss: 0.09424040466547012 test_acc: 0.9344362745098039\n",
      "epoch 1871 total_train_acc: 0.9737464522232734 loss: 0.3931339308619499 test_loss: 0.1240067183971405 test_acc: 0.9332107843137255\n",
      "epoch 1872 total_train_acc: 0.9744560075685903 loss: 0.38235774636268616 test_loss: 0.28796860575675964 test_acc: 0.9332107843137255\n",
      "epoch 1873 total_train_acc: 0.9735099337748344 loss: 0.39200010150671005 test_loss: 0.17912442982196808 test_acc: 0.9344362745098039\n",
      "epoch 1874 total_train_acc: 0.9735099337748344 loss: 0.3745111897587776 test_loss: 0.04824931174516678 test_acc: 0.9332107843137255\n",
      "epoch 1875 total_train_acc: 0.9756385998107853 loss: 0.35304100066423416 test_loss: 0.15412603318691254 test_acc: 0.9344362745098039\n",
      "epoch 1876 total_train_acc: 0.9756385998107853 loss: 0.3657901883125305 test_loss: 0.10408028215169907 test_acc: 0.9350490196078431\n",
      "epoch 1877 total_train_acc: 0.9754020813623463 loss: 0.39263297617435455 test_loss: 0.08514727652072906 test_acc: 0.9344362745098039\n",
      "epoch 1878 total_train_acc: 0.9751655629139073 loss: 0.38765325397253036 test_loss: 0.4164469540119171 test_acc: 0.9332107843137255\n",
      "epoch 1879 total_train_acc: 0.9706717123935666 loss: 0.42513446509838104 test_loss: 0.25100818276405334 test_acc: 0.9338235294117647\n",
      "epoch 1880 total_train_acc: 0.9742194891201513 loss: 0.35569707304239273 test_loss: 0.03389989212155342 test_acc: 0.9338235294117647\n",
      "epoch 1881 total_train_acc: 0.9761116367076632 loss: 0.3850770480930805 test_loss: 0.1931731402873993 test_acc: 0.9338235294117647\n",
      "epoch 1882 total_train_acc: 0.9754020813623463 loss: 0.36022409051656723 test_loss: 0.3980642259120941 test_acc: 0.9332107843137255\n",
      "epoch 1883 total_train_acc: 0.9739829706717124 loss: 0.3589906878769398 test_loss: 0.27672168612480164 test_acc: 0.9344362745098039\n",
      "epoch 1884 total_train_acc: 0.9768211920529801 loss: 0.3455120660364628 test_loss: 0.24280476570129395 test_acc: 0.9325980392156863\n",
      "epoch 1885 total_train_acc: 0.977767265846736 loss: 0.35545967146754265 test_loss: 0.1734617054462433 test_acc: 0.9325980392156863\n",
      "epoch 1886 total_train_acc: 0.9754020813623463 loss: 0.3770526871085167 test_loss: 0.35436582565307617 test_acc: 0.9338235294117647\n",
      "epoch 1887 total_train_acc: 0.9713812677388837 loss: 0.39889439195394516 test_loss: 0.1616930514574051 test_acc: 0.9338235294117647\n",
      "epoch 1888 total_train_acc: 0.9746925260170294 loss: 0.3856852352619171 test_loss: 0.0549643337726593 test_acc: 0.9338235294117647\n",
      "epoch 1889 total_train_acc: 0.9730368968779565 loss: 0.35287846997380257 test_loss: 0.2520756721496582 test_acc: 0.9344362745098039\n",
      "epoch 1890 total_train_acc: 0.9718543046357616 loss: 0.3722502253949642 test_loss: 0.24228504300117493 test_acc: 0.9350490196078431\n",
      "epoch 1891 total_train_acc: 0.9706717123935666 loss: 0.40475528687238693 test_loss: 0.21070943772792816 test_acc: 0.9356617647058824\n",
      "epoch 1892 total_train_acc: 0.9737464522232734 loss: 0.3993164226412773 test_loss: 0.10644899308681488 test_acc: 0.9338235294117647\n",
      "epoch 1893 total_train_acc: 0.9737464522232734 loss: 0.35164259746670723 test_loss: 0.3135131299495697 test_acc: 0.9338235294117647\n",
      "epoch 1894 total_train_acc: 0.9744560075685903 loss: 0.385398268699646 test_loss: 0.5875000357627869 test_acc: 0.9338235294117647\n",
      "epoch 1895 total_train_acc: 0.978713339640492 loss: 0.3375047892332077 test_loss: 0.1878426969051361 test_acc: 0.9344362745098039\n",
      "epoch 1896 total_train_acc: 0.9713812677388837 loss: 0.38738586753606796 test_loss: 0.14603236317634583 test_acc: 0.9338235294117647\n",
      "epoch 1897 total_train_acc: 0.9739829706717124 loss: 0.35168858245015144 test_loss: 0.08198507875204086 test_acc: 0.9319852941176471\n",
      "epoch 1898 total_train_acc: 0.9706717123935666 loss: 0.37413303554058075 test_loss: 0.2765432894229889 test_acc: 0.9332107843137255\n",
      "epoch 1899 total_train_acc: 0.9728003784295175 loss: 0.3546868711709976 test_loss: 0.16769801080226898 test_acc: 0.9338235294117647\n",
      "epoch 1900 total_train_acc: 0.9746925260170294 loss: 0.38293688744306564 test_loss: 0.07797592878341675 test_acc: 0.9338235294117647\n",
      "epoch 1901 total_train_acc: 0.9746925260170294 loss: 0.37442219257354736 test_loss: 0.291864275932312 test_acc: 0.9325980392156863\n",
      "epoch 1902 total_train_acc: 0.9742194891201513 loss: 0.37564077228307724 test_loss: 0.46155792474746704 test_acc: 0.9325980392156863\n",
      "epoch 1903 total_train_acc: 0.9735099337748344 loss: 0.3860574811697006 test_loss: 0.26993802189826965 test_acc: 0.9325980392156863\n",
      "epoch 1904 total_train_acc: 0.9763481551561022 loss: 0.3837861120700836 test_loss: 0.004541625734418631 test_acc: 0.9325980392156863\n",
      "epoch 1905 total_train_acc: 0.9756385998107853 loss: 0.36305130273103714 test_loss: 0.19876646995544434 test_acc: 0.9319852941176471\n",
      "epoch 1906 total_train_acc: 0.9725638599810785 loss: 0.3838486969470978 test_loss: 0.3411625921726227 test_acc: 0.9319852941176471\n",
      "epoch 1907 total_train_acc: 0.9758751182592242 loss: 0.3879025727510452 test_loss: 0.1987067312002182 test_acc: 0.9332107843137255\n",
      "epoch 1908 total_train_acc: 0.9730368968779565 loss: 0.37592198699712753 test_loss: 0.04830043390393257 test_acc: 0.9338235294117647\n",
      "epoch 1909 total_train_acc: 0.9737464522232734 loss: 0.3716398514807224 test_loss: 0.1734224408864975 test_acc: 0.9332107843137255\n",
      "epoch 1910 total_train_acc: 0.9730368968779565 loss: 0.39687125384807587 test_loss: 0.12098795920610428 test_acc: 0.9325980392156863\n",
      "epoch 1911 total_train_acc: 0.9744560075685903 loss: 0.3847259059548378 test_loss: 0.0626123696565628 test_acc: 0.9350490196078431\n",
      "epoch 1912 total_train_acc: 0.9737464522232734 loss: 0.42792703956365585 test_loss: 0.2649611830711365 test_acc: 0.9344362745098039\n",
      "epoch 1913 total_train_acc: 0.9746925260170294 loss: 0.3372420109808445 test_loss: 0.03011290170252323 test_acc: 0.9344362745098039\n",
      "epoch 1914 total_train_acc: 0.9761116367076632 loss: 0.3870787210762501 test_loss: 0.40380316972732544 test_acc: 0.9344362745098039\n",
      "epoch 1915 total_train_acc: 0.9716177861873226 loss: 0.3946482762694359 test_loss: 0.25717607140541077 test_acc: 0.9338235294117647\n",
      "epoch 1916 total_train_acc: 0.9723273415326396 loss: 0.3935849219560623 test_loss: 0.7129979133605957 test_acc: 0.9344362745098039\n",
      "epoch 1917 total_train_acc: 0.9737464522232734 loss: 0.3699037581682205 test_loss: 0.18551449477672577 test_acc: 0.9338235294117647\n",
      "epoch 1918 total_train_acc: 0.9725638599810785 loss: 0.38300732523202896 test_loss: 0.1754411906003952 test_acc: 0.9344362745098039\n",
      "epoch 1919 total_train_acc: 0.9768211920529801 loss: 0.3582823872566223 test_loss: 0.11473263055086136 test_acc: 0.9338235294117647\n",
      "epoch 1920 total_train_acc: 0.9772942289498581 loss: 0.3463265374302864 test_loss: 0.018696874380111694 test_acc: 0.9338235294117647\n",
      "epoch 1921 total_train_acc: 0.9751655629139073 loss: 0.36898213997483253 test_loss: 0.10688355565071106 test_acc: 0.9338235294117647\n",
      "epoch 1922 total_train_acc: 0.9758751182592242 loss: 0.402704156935215 test_loss: 0.053628988564014435 test_acc: 0.9338235294117647\n",
      "epoch 1923 total_train_acc: 0.9746925260170294 loss: 0.38584309071302414 test_loss: 0.23904353380203247 test_acc: 0.9368872549019608\n",
      "epoch 1924 total_train_acc: 0.9744560075685903 loss: 0.39431681856513023 test_loss: 0.24672682583332062 test_acc: 0.9362745098039216\n",
      "epoch 1925 total_train_acc: 0.9728003784295175 loss: 0.38230767101049423 test_loss: 0.175934299826622 test_acc: 0.9338235294117647\n",
      "epoch 1926 total_train_acc: 0.9737464522232734 loss: 0.37298405542969704 test_loss: 0.0991613045334816 test_acc: 0.9344362745098039\n",
      "epoch 1927 total_train_acc: 0.9758751182592242 loss: 0.3875049725174904 test_loss: 0.09295118600130081 test_acc: 0.9338235294117647\n",
      "epoch 1928 total_train_acc: 0.977530747398297 loss: 0.3801429569721222 test_loss: 0.15156099200248718 test_acc: 0.9344362745098039\n",
      "epoch 1929 total_train_acc: 0.9746925260170294 loss: 0.3515031710267067 test_loss: 0.06906595081090927 test_acc: 0.9332107843137255\n",
      "epoch 1930 total_train_acc: 0.9730368968779565 loss: 0.3636973351240158 test_loss: 0.026777224615216255 test_acc: 0.9344362745098039\n",
      "epoch 1931 total_train_acc: 0.9746925260170294 loss: 0.36520904302597046 test_loss: 0.3419868052005768 test_acc: 0.9338235294117647\n",
      "epoch 1932 total_train_acc: 0.9749290444654684 loss: 0.36593005061149597 test_loss: 0.05723440274596214 test_acc: 0.9344362745098039\n",
      "epoch 1933 total_train_acc: 0.9730368968779565 loss: 0.3617320731282234 test_loss: 0.26532939076423645 test_acc: 0.9344362745098039\n",
      "epoch 1934 total_train_acc: 0.9749290444654684 loss: 0.38059698790311813 test_loss: 0.06083675101399422 test_acc: 0.9356617647058824\n",
      "epoch 1935 total_train_acc: 0.9746925260170294 loss: 0.35475556179881096 test_loss: 0.16871540248394012 test_acc: 0.9350490196078431\n",
      "epoch 1936 total_train_acc: 0.9763481551561022 loss: 0.3645154759287834 test_loss: 0.29257816076278687 test_acc: 0.9344362745098039\n",
      "epoch 1937 total_train_acc: 0.9758751182592242 loss: 0.3467828966677189 test_loss: 0.2901100814342499 test_acc: 0.9350490196078431\n",
      "epoch 1938 total_train_acc: 0.9725638599810785 loss: 0.42775267362594604 test_loss: 0.10025573521852493 test_acc: 0.9338235294117647\n",
      "epoch 1939 total_train_acc: 0.9742194891201513 loss: 0.3467395082116127 test_loss: 0.25117000937461853 test_acc: 0.9332107843137255\n",
      "epoch 1940 total_train_acc: 0.9697256385998108 loss: 0.4062684178352356 test_loss: 0.5110043883323669 test_acc: 0.9325980392156863\n",
      "epoch 1941 total_train_acc: 0.9737464522232734 loss: 0.3568742647767067 test_loss: 0.3173660337924957 test_acc: 0.9319852941176471\n",
      "epoch 1942 total_train_acc: 0.9713812677388837 loss: 0.3787194639444351 test_loss: 0.14169204235076904 test_acc: 0.9332107843137255\n",
      "epoch 1943 total_train_acc: 0.9730368968779565 loss: 0.37935667484998703 test_loss: 0.27814432978630066 test_acc: 0.9350490196078431\n",
      "epoch 1944 total_train_acc: 0.9746925260170294 loss: 0.3883768394589424 test_loss: 0.26037004590034485 test_acc: 0.9344362745098039\n",
      "epoch 1945 total_train_acc: 0.9739829706717124 loss: 0.37819910794496536 test_loss: 0.14824837446212769 test_acc: 0.9344362745098039\n",
      "epoch 1946 total_train_acc: 0.9728003784295175 loss: 0.37255948781967163 test_loss: 0.27360984683036804 test_acc: 0.9350490196078431\n",
      "epoch 1947 total_train_acc: 0.9739829706717124 loss: 0.37650705501437187 test_loss: 0.22031177580356598 test_acc: 0.9344362745098039\n",
      "epoch 1948 total_train_acc: 0.9756385998107853 loss: 0.38336043059825897 test_loss: 0.26375114917755127 test_acc: 0.9332107843137255\n",
      "epoch 1949 total_train_acc: 0.9704351939451277 loss: 0.360669519752264 test_loss: 0.6352636814117432 test_acc: 0.9344362745098039\n",
      "epoch 1950 total_train_acc: 0.9751655629139073 loss: 0.3492991477251053 test_loss: 0.023327350616455078 test_acc: 0.9319852941176471\n",
      "epoch 1951 total_train_acc: 0.9754020813623463 loss: 0.3877009116113186 test_loss: 0.11850690096616745 test_acc: 0.9332107843137255\n",
      "epoch 1952 total_train_acc: 0.9716177861873226 loss: 0.3789002299308777 test_loss: 0.2738829255104065 test_acc: 0.9338235294117647\n",
      "epoch 1953 total_train_acc: 0.9746925260170294 loss: 0.36758194863796234 test_loss: 0.16736245155334473 test_acc: 0.9338235294117647\n",
      "epoch 1954 total_train_acc: 0.9730368968779565 loss: 0.3692772090435028 test_loss: 0.11503984779119492 test_acc: 0.9338235294117647\n",
      "epoch 1955 total_train_acc: 0.9739829706717124 loss: 0.35388705134391785 test_loss: 0.383087694644928 test_acc: 0.9332107843137255\n",
      "epoch 1956 total_train_acc: 0.9749290444654684 loss: 0.3803451396524906 test_loss: 0.3785027265548706 test_acc: 0.9325980392156863\n",
      "epoch 1957 total_train_acc: 0.9723273415326396 loss: 0.3701280951499939 test_loss: 0.2165234088897705 test_acc: 0.9319852941176471\n",
      "epoch 1958 total_train_acc: 0.9746925260170294 loss: 0.375668503344059 test_loss: 0.15352869033813477 test_acc: 0.9332107843137255\n",
      "epoch 1959 total_train_acc: 0.9713812677388837 loss: 0.36774948239326477 test_loss: 0.03039739839732647 test_acc: 0.9332107843137255\n",
      "epoch 1960 total_train_acc: 0.9744560075685903 loss: 0.3890227898955345 test_loss: 0.11585181951522827 test_acc: 0.9325980392156863\n",
      "epoch 1961 total_train_acc: 0.9756385998107853 loss: 0.3467639312148094 test_loss: 0.10438811779022217 test_acc: 0.9332107843137255\n",
      "epoch 1962 total_train_acc: 0.9744560075685903 loss: 0.36634600907564163 test_loss: 0.13452956080436707 test_acc: 0.9344362745098039\n",
      "epoch 1963 total_train_acc: 0.9737464522232734 loss: 0.37174925953149796 test_loss: 0.2690437436103821 test_acc: 0.9325980392156863\n",
      "epoch 1964 total_train_acc: 0.9735099337748344 loss: 0.3867810592055321 test_loss: 0.33541548252105713 test_acc: 0.9313725490196079\n",
      "epoch 1965 total_train_acc: 0.9720908230842006 loss: 0.40674884617328644 test_loss: 0.29563450813293457 test_acc: 0.9332107843137255\n",
      "epoch 1966 total_train_acc: 0.9746925260170294 loss: 0.3945183902978897 test_loss: 0.1605898141860962 test_acc: 0.9325980392156863\n",
      "epoch 1967 total_train_acc: 0.9718543046357616 loss: 0.371075376868248 test_loss: 0.45659157633781433 test_acc: 0.9319852941176471\n",
      "epoch 1968 total_train_acc: 0.9758751182592242 loss: 0.3741118460893631 test_loss: 0.12125775963068008 test_acc: 0.9332107843137255\n",
      "epoch 1969 total_train_acc: 0.9749290444654684 loss: 0.35257885232567787 test_loss: 0.2455352246761322 test_acc: 0.9332107843137255\n",
      "epoch 1970 total_train_acc: 0.9756385998107853 loss: 0.36512291431427 test_loss: 0.06344233453273773 test_acc: 0.9332107843137255\n",
      "epoch 1971 total_train_acc: 0.9711447492904447 loss: 0.38876036554574966 test_loss: 0.11660832911729813 test_acc: 0.9332107843137255\n",
      "epoch 1972 total_train_acc: 0.9737464522232734 loss: 0.37303371727466583 test_loss: 0.03741894289851189 test_acc: 0.9344362745098039\n",
      "epoch 1973 total_train_acc: 0.9749290444654684 loss: 0.36271239817142487 test_loss: 0.2535622715950012 test_acc: 0.9338235294117647\n",
      "epoch 1974 total_train_acc: 0.9751655629139073 loss: 0.3467475399374962 test_loss: 0.10781371593475342 test_acc: 0.9350490196078431\n",
      "epoch 1975 total_train_acc: 0.9735099337748344 loss: 0.36642879247665405 test_loss: 0.34769752621650696 test_acc: 0.9344362745098039\n",
      "epoch 1976 total_train_acc: 0.9723273415326396 loss: 0.3826386220753193 test_loss: 0.012908566743135452 test_acc: 0.9344362745098039\n",
      "epoch 1977 total_train_acc: 0.9737464522232734 loss: 0.3636360354721546 test_loss: 0.35970360040664673 test_acc: 0.9338235294117647\n",
      "epoch 1978 total_train_acc: 0.9763481551561022 loss: 0.37725159525871277 test_loss: 0.12416163086891174 test_acc: 0.9356617647058824\n",
      "epoch 1979 total_train_acc: 0.9716177861873226 loss: 0.36108990758657455 test_loss: 0.21914589405059814 test_acc: 0.9325980392156863\n",
      "epoch 1980 total_train_acc: 0.9735099337748344 loss: 0.3863555043935776 test_loss: 0.2030981481075287 test_acc: 0.9325980392156863\n",
      "epoch 1981 total_train_acc: 0.9739829706717124 loss: 0.3816392198204994 test_loss: 0.3638499975204468 test_acc: 0.9332107843137255\n",
      "epoch 1982 total_train_acc: 0.9768211920529801 loss: 0.3692864775657654 test_loss: 0.01695626601576805 test_acc: 0.9332107843137255\n",
      "epoch 1983 total_train_acc: 0.977767265846736 loss: 0.3587682768702507 test_loss: 0.18570446968078613 test_acc: 0.9338235294117647\n",
      "epoch 1984 total_train_acc: 0.9723273415326396 loss: 0.42443618550896645 test_loss: 0.3385887145996094 test_acc: 0.9344362745098039\n",
      "epoch 1985 total_train_acc: 0.9730368968779565 loss: 0.36425016075372696 test_loss: 0.1713661402463913 test_acc: 0.9350490196078431\n",
      "epoch 1986 total_train_acc: 0.9713812677388837 loss: 0.38778602331876755 test_loss: 0.04490626975893974 test_acc: 0.9344362745098039\n",
      "epoch 1987 total_train_acc: 0.9706717123935666 loss: 0.4165761470794678 test_loss: 0.08041957765817642 test_acc: 0.9338235294117647\n",
      "epoch 1988 total_train_acc: 0.9758751182592242 loss: 0.3980465903878212 test_loss: 0.37719109654426575 test_acc: 0.9338235294117647\n",
      "epoch 1989 total_train_acc: 0.9744560075685903 loss: 0.38354120403528214 test_loss: 0.3238670825958252 test_acc: 0.9332107843137255\n",
      "epoch 1990 total_train_acc: 0.9723273415326396 loss: 0.3969648778438568 test_loss: 0.052702177315950394 test_acc: 0.9344362745098039\n",
      "epoch 1991 total_train_acc: 0.9754020813623463 loss: 0.3695010058581829 test_loss: 0.18490725755691528 test_acc: 0.9338235294117647\n",
      "epoch 1992 total_train_acc: 0.9758751182592242 loss: 0.3888994939625263 test_loss: 0.03797363117337227 test_acc: 0.9332107843137255\n",
      "epoch 1993 total_train_acc: 0.9742194891201513 loss: 0.3784867078065872 test_loss: 0.05052061006426811 test_acc: 0.9332107843137255\n",
      "epoch 1994 total_train_acc: 0.9765846736045412 loss: 0.3548203818500042 test_loss: 0.24248991906642914 test_acc: 0.9338235294117647\n",
      "epoch 1995 total_train_acc: 0.9758751182592242 loss: 0.35961421579122543 test_loss: 0.25112536549568176 test_acc: 0.9332107843137255\n",
      "epoch 1996 total_train_acc: 0.9718543046357616 loss: 0.37505314871668816 test_loss: 0.5998846292495728 test_acc: 0.9325980392156863\n",
      "epoch 1997 total_train_acc: 0.9742194891201513 loss: 0.3926832228899002 test_loss: 0.3047616183757782 test_acc: 0.9338235294117647\n",
      "epoch 1998 total_train_acc: 0.9749290444654684 loss: 0.3548358678817749 test_loss: 0.15401867032051086 test_acc: 0.9332107843137255\n",
      "epoch 1999 total_train_acc: 0.9709082308420057 loss: 0.3572143204510212 test_loss: 0.5054737329483032 test_acc: 0.9344362745098039\n"
     ]
    }
   ],
   "source": [
    "# 视情况加载已保存文件\n",
    "# checkpoint_pretrain = torch.load('./ckp/c6c6_ep_1600_2021_12_05_16_38_46.pth')\n",
    "# net.load_state_dict(checkpoint_pretrain['model'])\n",
    "ckpDir = './/ckp//c10'\n",
    "if not os.path.exists(ckpDir):\n",
    "    os.makedirs(ckpDir)\n",
    "timeForSave = datetime.datetime.now().strftime('%Y_%m_%d_%H:%M:%S')\n",
    "# 设置使用的训练设备\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "net = net.to(device)\n",
    "# 加载数据，设置优化器\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=1000,shuffle=True)\n",
    "# train_un_loader = torch.utils.data.DataLoader(train_unknown, batch_size=100,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100,shuffle=True)\n",
    "optimizer = torch.optim.Adam(net.parameters(),\n",
    "        lr=0.0002)\n",
    "lr_schedule = torch.optim.lr_scheduler.StepLR(\\\n",
    "        optimizer, 400, gamma=0.5, last_epoch=-1)\n",
    "total_test_acc = 0\n",
    "total_test_correct = 0\n",
    "totaltest = 0\n",
    "vizx = 0\n",
    "epoch_num = 2000\n",
    "# 开始迭代\n",
    "for epoch in range(epoch_num):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    curr_total_correct = 0\n",
    "    i = 0\n",
    "    net.train()\n",
    "    # Classify real examples into the correct K classes\n",
    "    total_traintnum = 0\n",
    "    for batch_kn in train_loader: # Get Batch\n",
    "        i+=1\n",
    "        images, labels = batch_kn\n",
    "        # 数据和标签转为所需数据类型\n",
    "        images = images.to(torch.float32)\n",
    "        labels = labels.long()\n",
    "        preds = net(images.to(device)) # Pass Batch\n",
    "        trainloss = F.cross_entropy(preds.to(device), labels.to(device)) # Calculate Loss\n",
    "        known_augmented_logits = F.pad(preds, (0,1))\n",
    "        known_log_soft_open = F.log_softmax(known_augmented_logits.to(device), dim=1)[:, -1]\n",
    "        # errknown =  known_log_soft_open.mean()\n",
    "        # trainloss = trainloss + errknown\n",
    "        optimizer.zero_grad()\n",
    "        trainloss.backward() # Calculate Gradients\n",
    "        optimizer.step() # Update Weights\n",
    "        total_loss += trainloss.item()\n",
    "        # total_correct += get_num_correct(preds.to(device), labels.to(device))\n",
    "        curr_total_correct = (reuse.get_num_correct(preds.to(device), labels.to(device)))\n",
    "        total_correct += curr_total_correct\n",
    "        total_traintnum += labels.size(0)\n",
    "    total_train_acc = total_correct/total_traintnum\n",
    "    total_correct = 0\n",
    "    # Classify aux_dataset examples as open set\n",
    "    # for batch_unkn in train_un_loader:\n",
    "    #     un_data,_ = batch_unkn\n",
    "    #     un_data = un_data.to(torch.float32)\n",
    "    #     classifier_logits = net(un_data.to(device))\n",
    "    #     augmented_logits = F.pad(classifier_logits, (0,1))\n",
    "    #     log_soft_open = F.log_softmax(augmented_logits.to(device), dim=1)[:, -1]\n",
    "    #     errOpenSet = -log_soft_open.mean()\n",
    "    #     optimizer.zero_grad()\n",
    "    #     errOpenSet.backward()\n",
    "    #     optimizer.step() # Update Weights\n",
    "\n",
    "    # 可视化\n",
    "    vizx+=1\n",
    "    viz.line([float(trainloss)],[vizx],\\\n",
    "        win='trainloss', update='append',opts=dict(title='trainloss'))   \n",
    "    viz2.line([float(optimizer.state_dict()['param_groups'][0]['lr'])],[vizx],\\\n",
    "        win='lr', update='append',opts=dict(title='lr'))\n",
    "    viz3.line([float(total_train_acc)],[vizx],\\\n",
    "        win='train_acc', update='append',opts=dict(title='train_acc'))\n",
    "    # viz5.line([float(errOpenSet)],[vizx],\\\n",
    "    #     win='errOpenSet', update='append',opts=dict(title='erropen'))   \n",
    "    # 测试\n",
    "    net.eval()\n",
    "    total_testnum = 0\n",
    "    for testemgdatas, testemglabels in test_loader: # Get Batch\n",
    "        testemgdatas = testemgdatas.to(torch.float32)\n",
    "        testemglabels = testemglabels.long()\n",
    "        predstest = net(testemgdatas.to(device))\n",
    "        testloss = F.cross_entropy(predstest.to(device), testemglabels.to(device)) # Calculate Loss\n",
    "        curr_test_correct = (get_num_correct(predstest.to(device), testemglabels.to(device)))\n",
    "        total_testnum += testemglabels.size(0)\n",
    "        total_test_correct += curr_test_correct\n",
    "        # totaltest += testemglabels.size(0)\n",
    "    # total_test_acc = total_test_correct/(trainlabel.size)\n",
    "    total_test_acc = total_test_correct/total_testnum\n",
    "    \n",
    "    viz1.line([float(testloss)],[vizx],win='testloss', update='append',opts=dict(title='testloss'))\n",
    "    viz4.line([float(total_test_acc)],[vizx],\\\n",
    "        win='test_acc', update='append',opts=dict(title='test_acc'))\n",
    "    total_test_correct = 0\n",
    "\n",
    "    print(\n",
    "        \"epoch\", epoch, \n",
    "        \"total_train_acc:\", total_train_acc, \n",
    "        \"loss:\", total_loss,\n",
    "        \"test_loss:\",float(testloss),\n",
    "        \"test_acc:\",total_test_acc\n",
    "    )\n",
    "   # 更新学习率\n",
    "    lr_schedule.step()\n",
    "   # 定期保存\n",
    "    if epoch%50 == 1:\n",
    "        timeForSave = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "        checkpointPath = ckpDir+'//c10_ep_'+str(epoch)+'_'+timeForSave+'.pth'\n",
    "        state = {'model': net.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch}\n",
    "        torch.save(state, checkpointPath)\n",
    "FinalPath = 'c10s'+timeForSave+'.pth'\n",
    "torch.save(net.state_dict(),FinalPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_for_vector = torch.load('./c10s2022_01_03_20_17_36.pth')\n",
    "net_vector = Network()\n",
    "net_vector.load_state_dict(checkpoint_for_vector)\n",
    "net_vector.eval()\n",
    "emg_vector_num = 0\n",
    "vector_Dataloader = torch.utils.data.DataLoader(train_set, batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算数据集的网络预测输出并构建新的特征向量四维 array\n",
    "# 注意迭代器只能建立一次，然后以此前进，所以这里单列一行，避免重复建设\n",
    "# 达成类似目标的做法有很多种，例如 append() 等，这里先完成功能，优化待后来同学了\n",
    "def emgdata_to_net_preds(data_set,net_vector):\n",
    "    batchl = iter(data_set)\n",
    "    emg_vec = [torch.tensor([],requires_grad=False) for i in range(len(data_set.label))]\n",
    "    for idx, _ in enumerate(emg_vec):\n",
    "        sample_data,sample_label = next(batchl)\n",
    "        sample_data = sample_data.to(torch.float32).unsqueeze(0)\n",
    "        sample_label = torch.as_tensor(sample_label).long()\n",
    "        emg_vec[idx] = net_vector(sample_data).detach().numpy()\n",
    "    emg_vec_np = np.array(emg_vec)\n",
    "    emg_vec_np = emg_vec_np[:,np.newaxis,:,:]\n",
    "    return emg_vec_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_vec_npp = emgdata_to_net_preds(data_set=train_set, net_vector = net_vector)\n",
    "# np.save('../data/OpenganDatafetrevec220111.npy',emg_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = [batch for batch in train_set]\n",
    "batch = next(batchl)\n",
    "# print(batch)\n",
    "sample_data,sample_label = batch\n",
    "# sample_data,sample_label = next(iter(vector_Dataloader))\n",
    "sample_data = sample_data.to(torch.float32)\n",
    "sample_data = sample_data.unsqueeze(0)\n",
    "sample_label = torch.as_tensor(sample_label)\n",
    "sample_label = sample_label.long()\n",
    "# print(sample_data)\n",
    "pre_vector = net_vector(sample_data)\n",
    "print(pre_vector)\n",
    "print(pre_vector.shape)\n",
    "# print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "tensor([[[[0.0269, 0.0293, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0488,\n",
      "           0.0024, 0.0073],\n",
      "          [0.0244, 0.0317, 0.0024, 0.0024, 0.0024, 0.0024, 0.1001, 0.0635,\n",
      "           0.0024, 0.0098],\n",
      "          [0.0220, 0.0244, 0.0024, 0.0024, 0.0024, 0.0024, 0.1074, 0.0757,\n",
      "           0.0024, 0.0195],\n",
      "          [0.0171, 0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.1172, 0.0854,\n",
      "           0.0024, 0.0220],\n",
      "          [0.0195, 0.0049, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0854,\n",
      "           0.0024, 0.0220],\n",
      "          [0.0269, 0.0049, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0806,\n",
      "           0.0024, 0.0146],\n",
      "          [0.0342, 0.0171, 0.0024, 0.0024, 0.0024, 0.0024, 0.1221, 0.0781,\n",
      "           0.0024, 0.0098],\n",
      "          [0.0391, 0.0171, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0732,\n",
      "           0.0024, 0.0049],\n",
      "          [0.0488, 0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.1001, 0.0659,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0464, 0.0122, 0.0024, 0.0024, 0.0049, 0.0024, 0.0830, 0.0610,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0464, 0.0073, 0.0024, 0.0024, 0.0024, 0.0024, 0.0757, 0.0562,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0439, 0.0244, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0562,\n",
      "           0.0024, 0.0073],\n",
      "          [0.0488, 0.0415, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0537,\n",
      "           0.0024, 0.0098],\n",
      "          [0.0586, 0.0635, 0.0024, 0.0024, 0.0024, 0.0024, 0.0952, 0.0562,\n",
      "           0.0024, 0.0122],\n",
      "          [0.0537, 0.1196, 0.0049, 0.0024, 0.0024, 0.0024, 0.1025, 0.0537,\n",
      "           0.0098, 0.0098],\n",
      "          [0.0488, 0.1538, 0.0073, 0.0024, 0.0024, 0.0024, 0.0952, 0.0513,\n",
      "           0.0610, 0.0073],\n",
      "          [0.0391, 0.1538, 0.0049, 0.0024, 0.0024, 0.0024, 0.0806, 0.0464,\n",
      "           0.0806, 0.0098],\n",
      "          [0.0317, 0.1367, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0415,\n",
      "           0.0806, 0.0073],\n",
      "          [0.0317, 0.1123, 0.0024, 0.0024, 0.0024, 0.0024, 0.0659, 0.0439,\n",
      "           0.0732, 0.0049],\n",
      "          [0.0269, 0.0928, 0.0024, 0.0024, 0.0024, 0.0024, 0.0586, 0.0439,\n",
      "           0.0586, 0.0024]]]])\n",
      "tensor([[  7.6465,  -5.4484,  -4.8876,  -2.8030, -16.2135,  -5.5774, -10.0558,\n",
      "           0.3164,   1.5851,   0.8212]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "emg_vec_batch = pre_vector\n",
    "print(emg_vec_batch.shape)\n",
    "\n",
    "sample_datas= next(iter(train_set))\n",
    "sample_data,sample_label = sample_datas\n",
    "sample_data = sample_data.to(torch.float32)\n",
    "sample_data = sample_data.unsqueeze(0)\n",
    "sample_label = torch.as_tensor(sample_label)\n",
    "sample_label = sample_label.long()\n",
    "print(sample_data)\n",
    "pre_vector = net_vector(sample_data)\n",
    "print(pre_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用训练好的模型，用其输出构建新数据集\n",
    "# 这段感觉应该放在数据处理的，但好像不是那么好切割开，就先这样吧\n",
    "checkpoint_for_vector = torch.load('./c10s2022_01_03_20_17_36.pth')\n",
    "net_vector = Network()\n",
    "net_vector.load_state_dict(checkpoint_for_vector)\n",
    "net_vector.eval()\n",
    "# emg_vector = []\n",
    "# emg_vector = np.array(emg_vector)\n",
    "emg_vector_num = 0\n",
    "vector_Dataloader = torch.utils.data.DataLoader(train_set, batch_size=1,shuffle=False)\n",
    "for emgdata, emglabel in vector_Dataloader:\n",
    "    emgdata = emgdata.to(torch.float32)\n",
    "    emglabel = emglabel.long()\n",
    "    pre_vector = net_vector(emgdata)\n",
    "    emg_vector[emg_vector_num,0,:,:] = pre_vector\n",
    "    emg_vector_num += 1\n",
    "print(emg_vector.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重新加载模型\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=32 * 4 * 34, out_features=128)\n",
    "        self.out = nn.Linear(in_features=128, out_features=6)\n",
    "        self.dr1 = nn.Dropout2d(0.2)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        # t = self.dr1(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 32 * 4 * 34)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.dr1(t)\n",
    "\n",
    "        # (5) output layer\n",
    "        t = self.out(t)\n",
    "\n",
    "        return t\n",
    "        \n",
    "net_eval = Network()\n",
    "\n",
    "checkpoint_eval = torch.load('./ckp/c6addnewloss/gap5_c6op_ep_600_2021_12_16_15_47_32.pth')\n",
    "net_eval.load_state_dict(checkpoint_eval['model'])\n",
    "# net_eval.load_state_dict(checkpoint_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "[tensor([[[[0.1587, 0.0293, 0.1221, 0.0439, 0.0024, 0.0024, 0.3979, 0.4346,\n",
      "           0.2100, 0.1123],\n",
      "          [0.1587, 0.0293, 0.1270, 0.0439, 0.0049, 0.0049, 0.3979, 0.4590,\n",
      "           0.2710, 0.1196],\n",
      "          [0.1636, 0.0317, 0.1270, 0.0439, 0.0049, 0.0049, 0.3882, 0.4810,\n",
      "           0.3247, 0.1221],\n",
      "          [0.1563, 0.0342, 0.1172, 0.0391, 0.0024, 0.0024, 0.3931, 0.4639,\n",
      "           0.3564, 0.1123],\n",
      "          [0.1514, 0.0317, 0.1025, 0.0342, 0.0024, 0.0024, 0.4053, 0.4517,\n",
      "           0.3491, 0.1074],\n",
      "          [0.1392, 0.0220, 0.0806, 0.0244, 0.0024, 0.0024, 0.3882, 0.4297,\n",
      "           0.3223, 0.0977],\n",
      "          [0.1196, 0.0098, 0.0610, 0.0171, 0.0024, 0.0024, 0.3491, 0.3906,\n",
      "           0.3101, 0.0854],\n",
      "          [0.1025, 0.0024, 0.0439, 0.0073, 0.0024, 0.0024, 0.3003, 0.3516,\n",
      "           0.2905, 0.0757],\n",
      "          [0.1050, 0.0024, 0.0366, 0.0024, 0.0024, 0.0024, 0.2686, 0.3784,\n",
      "           0.2588, 0.0732],\n",
      "          [0.1123, 0.0024, 0.0342, 0.0024, 0.0024, 0.0024, 0.3003, 0.4199,\n",
      "           0.2441, 0.0830],\n",
      "          [0.1123, 0.0024, 0.0317, 0.0024, 0.0024, 0.0024, 0.3223, 0.4199,\n",
      "           0.2417, 0.0854],\n",
      "          [0.1074, 0.0024, 0.0317, 0.0024, 0.0024, 0.0024, 0.3174, 0.4004,\n",
      "           0.2271, 0.0806],\n",
      "          [0.1050, 0.0024, 0.0293, 0.0024, 0.0024, 0.0024, 0.3125, 0.4272,\n",
      "           0.2173, 0.0830],\n",
      "          [0.1099, 0.0024, 0.0293, 0.0049, 0.0024, 0.0024, 0.3467, 0.4468,\n",
      "           0.2173, 0.0928],\n",
      "          [0.1050, 0.0024, 0.0415, 0.0073, 0.0024, 0.0024, 0.3467, 0.4297,\n",
      "           0.2417, 0.0879],\n",
      "          [0.1001, 0.0049, 0.0488, 0.0098, 0.0024, 0.0024, 0.3271, 0.4077,\n",
      "           0.2539, 0.0781],\n",
      "          [0.0928, 0.0024, 0.0464, 0.0073, 0.0024, 0.0024, 0.2979, 0.3857,\n",
      "           0.2368, 0.0635],\n",
      "          [0.0806, 0.0024, 0.0366, 0.0024, 0.0024, 0.0024, 0.2637, 0.3589,\n",
      "           0.2100, 0.0562],\n",
      "          [0.0708, 0.0024, 0.0586, 0.0024, 0.0024, 0.0024, 0.2271, 0.3442,\n",
      "           0.1831, 0.0537],\n",
      "          [0.0757, 0.0024, 0.0732, 0.0024, 0.0024, 0.0024, 0.2466, 0.3491,\n",
      "           0.1978, 0.0635],\n",
      "          [0.0757, 0.0024, 0.0732, 0.0024, 0.0024, 0.0024, 0.2637, 0.3345,\n",
      "           0.2319, 0.0684],\n",
      "          [0.0879, 0.0049, 0.0708, 0.0024, 0.0024, 0.0024, 0.2832, 0.3296,\n",
      "           0.2344, 0.0684],\n",
      "          [0.0977, 0.0024, 0.0635, 0.0024, 0.0024, 0.0024, 0.2759, 0.3271,\n",
      "           0.2148, 0.0635],\n",
      "          [0.0977, 0.0024, 0.0586, 0.0024, 0.0024, 0.0024, 0.2563, 0.3027,\n",
      "           0.1831, 0.0562],\n",
      "          [0.0977, 0.0024, 0.0537, 0.0049, 0.0024, 0.0024, 0.3076, 0.2905,\n",
      "           0.2295, 0.0513],\n",
      "          [0.0879, 0.0049, 0.0464, 0.0146, 0.0024, 0.0024, 0.3076, 0.2686,\n",
      "           0.2832, 0.0439],\n",
      "          [0.0781, 0.0098, 0.0439, 0.0171, 0.0024, 0.0024, 0.2905, 0.2417,\n",
      "           0.3125, 0.0366],\n",
      "          [0.0928, 0.0195, 0.0415, 0.0195, 0.0024, 0.0024, 0.2759, 0.2783,\n",
      "           0.3271, 0.0464],\n",
      "          [0.1099, 0.0244, 0.0391, 0.0195, 0.0024, 0.0024, 0.2563, 0.3052,\n",
      "           0.3101, 0.0586],\n",
      "          [0.1123, 0.0220, 0.0342, 0.0146, 0.0024, 0.0024, 0.2271, 0.3027,\n",
      "           0.2759, 0.0562],\n",
      "          [0.1123, 0.0146, 0.0342, 0.0098, 0.0024, 0.0024, 0.2051, 0.3052,\n",
      "           0.2368, 0.0537],\n",
      "          [0.1221, 0.0122, 0.0342, 0.0073, 0.0024, 0.0024, 0.2515, 0.3149,\n",
      "           0.2197, 0.0586],\n",
      "          [0.1245, 0.0146, 0.0366, 0.0049, 0.0024, 0.0024, 0.2637, 0.3198,\n",
      "           0.2417, 0.0537],\n",
      "          [0.1147, 0.0195, 0.0415, 0.0049, 0.0024, 0.0024, 0.2466, 0.3003,\n",
      "           0.2563, 0.0488],\n",
      "          [0.1001, 0.0146, 0.0464, 0.0049, 0.0024, 0.0024, 0.2246, 0.2734,\n",
      "           0.2466, 0.0415],\n",
      "          [0.0854, 0.0049, 0.0903, 0.0122, 0.0024, 0.0024, 0.2026, 0.2393,\n",
      "           0.2271, 0.0342],\n",
      "          [0.0977, 0.0073, 0.1123, 0.0220, 0.0024, 0.0049, 0.2051, 0.2515,\n",
      "           0.2075, 0.0317],\n",
      "          [0.1074, 0.0098, 0.1147, 0.0244, 0.0024, 0.0049, 0.2734, 0.2686,\n",
      "           0.1855, 0.0317],\n",
      "          [0.1074, 0.0049, 0.1025, 0.0195, 0.0024, 0.0024, 0.2881, 0.2832,\n",
      "           0.1563, 0.0269],\n",
      "          [0.1147, 0.0049, 0.0928, 0.0171, 0.0024, 0.0024, 0.2905, 0.3174,\n",
      "           0.1416, 0.0244]]]], dtype=torch.float64), tensor([7], dtype=torch.int16)]\n"
     ]
    }
   ],
   "source": [
    "# 自定义数据集类\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图片转换为Tensor,归一化至[0,1]\n",
    "])\n",
    "\n",
    "class EMGDataset(Dataset):\n",
    " \n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transforms = transform\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        emgData = self.data[index,:,:,:]\n",
    "        emgData = np.squeeze(emgData)#似乎不应该压缩了\n",
    "        emglabel = self.label[index]\n",
    "        emglabel = emglabel.astype(np.int16)\n",
    "        emgData = self.transforms(emgData)      \n",
    "        \n",
    "        return emgData,emglabel\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    " \n",
    " \n",
    "# if __name__ == '__main__':\n",
    "dataarray = np.load('../data/OpenSetDataSet.npy',allow_pickle=True)\n",
    "CNNdataset = dataarray.item()\n",
    "print(type(CNNdataset))\n",
    "opensetdata = CNNdataset['X_oo']\n",
    "opensetlabel = CNNdataset['Y_oo']\n",
    "# trainlabel = CNNdataset['Ytrain']\n",
    "# testdata = CNNdataset['Xtest']\n",
    "# testlabel = CNNdataset['Ytest']\n",
    "# # print(trainlabel[:,0])\n",
    "\n",
    "opensetlabel = opensetlabel[:,0]\n",
    "# testlabel = testlabel[:,0]\n",
    "# print(type(trainlabel))\n",
    "open_set = EMGDataset(opensetdata, opensetlabel)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, pin_memory=True,\n",
    "#                                             num_workers=3)\n",
    "openset_loader = torch.utils.data.DataLoader(open_set, batch_size=1,shuffle=True)\n",
    "sample = next(iter(openset_loader))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n",
      "C:\\Users\\cwdbo\\AppData\\Local\\Temp/ipykernel_9732/2954200349.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  preds = F.softmax(augmented_logits)\n",
      "C:\\Users\\cwdbo\\AppData\\Local\\Temp/ipykernel_9732/2954200349.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  preds2 = F.softmax(augmented_logits2)\n"
     ]
    }
   ],
   "source": [
    "# 效果评估\n",
    "openset_loader = torch.utils.data.DataLoader(open_set, batch_size=1,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1,shuffle=True)\n",
    "openset_scores = []\n",
    "openset_scores2 = []\n",
    "# 可视化\n",
    "viz6 = Visdom()\n",
    "viz7 = Visdom()\n",
    "vizx = 0\n",
    "openscoresave = []\n",
    "for opensetdata, opensetlabel in openset_loader: # Get Batch\n",
    "        opensetdata = opensetdata.to(torch.float32)\n",
    "        opensetlabel = opensetlabel.long()\n",
    "        logits = net_eval(opensetdata)\n",
    "        augmented_logits = F.pad(logits, pad=(0,1))\n",
    "        preds = F.softmax(augmented_logits)\n",
    "        prob_unknown = preds[:, -1]\n",
    "        prob_known = preds[:, :-1].max(dim=1)[0]\n",
    "        prob_open = prob_unknown - prob_known\n",
    "        openset_scores.extend(prob_open.data.cpu().numpy())\n",
    "        prob_openforvis = prob_open.float()\n",
    "        viz6.line([float(prob_openforvis)],[vizx],\\\n",
    "                win='openscore', update='append',opts=dict(title='openscore for unknown'))\n",
    "        vizx += 1\n",
    "\n",
    "vizx = 0\n",
    "for testemgdatas, testemglabels in test_loader: # Get Batch\n",
    "        testemgdatas = testemgdatas.to(torch.float32)\n",
    "        testemglabels = testemglabels.long()\n",
    "        logits2 = net_eval(testemgdatas)\n",
    "        augmented_logits2 = F.pad(logits2, pad=(0,1))\n",
    "        preds2 = F.softmax(augmented_logits2)\n",
    "        prob_unknown2 = preds2[:, -1]\n",
    "        prob_known2 = preds2[:, :-1].max(dim=1)[0]\n",
    "        prob_open2 = prob_unknown2 - prob_known2\n",
    "        openset_scores2.extend(prob_open2.data.cpu().numpy())\n",
    "        prob_openforvis2 = prob_open2.float()\n",
    "        viz7.line([float(prob_openforvis2)],[vizx],\\\n",
    "                win='openscore2', update='append',opts=dict(title='openscore for known'))\n",
    "        vizx += 1\n",
    "# for i, (images, labels) in enumerate(dataloader):\n",
    "#     images = Variable(images, volatile=True)\n",
    "#     logits = netC(images)\n",
    "#     augmented_logits = F.pad(logits, pad=(0,1))\n",
    "#     # The implicit K+1th class (the open set class) is computed\n",
    "#     #  by assuming an extra linear output with constant value 0\n",
    "#     preds = F.softmax(augmented_logits)\n",
    "#     #preds = augmented_logits\n",
    "#     prob_unknown = preds[:, -1]\n",
    "#     prob_known = preds[:, :-1].max(dim=1)[0]\n",
    "#     prob_open = prob_unknown - prob_known\n",
    "#     openset_scores.extend(prob_open.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "122afd33e14e141e8feafe6109b3cf33c81901f42114774f6f58cb0f50546406"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('ml2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
