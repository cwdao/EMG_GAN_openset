{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "\n",
    "class JointResize(object):\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, int):\n",
    "            self.size = (size, size)\n",
    "        elif isinstance(size, tuple):\n",
    "            self.size = size\n",
    "        else:\n",
    "            raise RuntimeError(\"size参数请设置为int或者tuple\")\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        img = img.resize(self.size)\n",
    "        mask = mask.resize(self.size)\n",
    "        return img, mask\n",
    "\n",
    "def make_dataset(root, prefix=('jpg', 'png')):\n",
    "    img_path = root[0]\n",
    "    gt_path = root[1]\n",
    "    img_list = [os.path.splitext(f)[0] for f in os.listdir(img_path) if f.endswith(prefix[0])]\n",
    "    return [(os.path.join(img_path, img_name + prefix[0]), os.path.join(gt_path, img_name + prefix[1])) for img_name in img_list]\n",
    "\n",
    "\n",
    "# 仅针对训练集\n",
    "class ImageFolder(data.Dataset):\n",
    "    def __init__(self, root, mode, in_size, prefix, use_bigt=False, split_rate=(1, 3)):\n",
    "        \"\"\"split_rate = label:unlabel\"\"\"\n",
    "        assert isinstance(mode, str), 'mode参数错误，应该为str类型'\n",
    "        self.mode = mode\n",
    "        self.use_bigt = use_bigt\n",
    "        self.split_rate = split_rate\n",
    "        self.r_l_rate = split_rate[1] // split_rate[0]\n",
    "\n",
    "        self.root_labeled = root[0]\n",
    "        self.imgs_labeled = make_dataset(self.root_labeled, prefix=prefix)\n",
    "\n",
    "        len_labeled = len(self.imgs_labeled)\n",
    "        self.length = len_labeled\n",
    "\n",
    "        self.root_unlabeled = root[1]\n",
    "        self.imgs_unlabeled = make_dataset(self.root_unlabeled, prefix=prefix)\n",
    "        \n",
    "        len_unlabeled = self.r_l_rate * len_labeled\n",
    "        \n",
    "        self.imgs_unlabeled = self.imgs_unlabeled * (self.r_l_rate + math.ceil(len_labeled / len_unlabeled))  # 扩展无标签的数据列表\n",
    "        self.imgs_unlabeled = self.imgs_unlabeled[0:len_unlabeled]\n",
    "\n",
    "        print(f\"使用比例为：{len_labeled / len_unlabeled}\")\n",
    "\n",
    "        # 仅是为了简单而仅使用一种变换\n",
    "        self.train_joint_transform = JointResize(in_size)\n",
    "        self.train_img_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # 处理的是Tensor\n",
    "        ])\n",
    "        # ToTensor 操作会将 PIL.Image 或形状为 H×W×D，数值范围为 [0, 255] 的 np.ndarray 转换为形状为 D×H×W，\n",
    "        # 数值范围为 [0.0, 1.0] 的 torch.Tensor。\n",
    "        self.train_gt_transform = transforms.ToTensor()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 这里一次性读取最简化比例数量的样本，所有的样本需要单独处理\n",
    "        img_labeled_path, gt_labeled_path = self.imgs_labeled[index]  # 0, 1 => 850\n",
    "        img_labeled = Image.open(img_labeled_path).convert('RGB')\n",
    "        img_labeled_name = (img_labeled_path.split(os.sep)[-1]).split('.')[0]\n",
    "\n",
    "        gt_labeled = Image.open(gt_labeled_path).convert('L')\n",
    "        back_gt_labeled = gt_labeled  \n",
    "        # 用于无标签数据使用联合调整函数的时候代替无标签数据真值进行占位\n",
    "        img_labeled, gt_labeled = self.train_joint_transform(img_labeled, gt_labeled)\n",
    "        img_labeled = self.train_img_transform(img_labeled)\n",
    "        gt_labeled = self.train_gt_transform(gt_labeled)\n",
    "        if self.use_bigt:\n",
    "            gt_labeled = gt_labeled.ge(0.5).float()  # 二值化\n",
    "        data_labeled = [img_labeled, gt_labeled, img_labeled_name]\n",
    "        \n",
    "        data_unlabeled = [[], []]\n",
    "        for idx_periter in range(self.r_l_rate):\n",
    "            # 这里不再使用真值，直接使用`_`接收\n",
    "            img_unlabeled_path, _ = self.imgs_unlabeled[index // self.r_l_rate + idx_periter]  \n",
    "            # 0, 1, 2, 3 => 3*850\n",
    "            img_unlabeled = Image.open(img_unlabeled_path).convert('RGB')\n",
    "            img_unlabeled_name = (img_unlabeled_path.split(os.sep)[-1]).split('.')[0]\n",
    "\n",
    "            img_unlabeled, _ = self.train_joint_transform(img_unlabeled, back_gt_labeled)  \n",
    "            # 这里为了使用那个联合调整的转换类，使用上面的target进行替代，但是要注意，不要再返回了\n",
    "            img_unlabeled = self.train_img_transform(img_unlabeled)\n",
    "                        \n",
    "            data_unlabeled[0].append(img_unlabeled)\n",
    "            data_unlabeled[1].append(img_unlabeled_name)\n",
    "\n",
    "        return data_labeled, data_unlabeled  # 输出名字方便比较\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    \n",
    "def my_collate(batch):\n",
    "    # 针对送进来的一个batch的数据进行整合，batch的各项表示各个样本\n",
    "    # batch 仅有一项 batch[0] 对应于下面的 train_data\n",
    "    # batch[0][0], batch[0][1] <==> data_labeled, data_unlabeled = train_data\n",
    "    # batch[0][0][0], batch[0][0][1], batch[0][0][2] <==> train_labeled_inputs, train_labeled_gts, train_labeled_names = data_labeled\n",
    "    # batch[0][1][0], batch[0][2][1] <==> train_unlabeled_inputs_list, train_unlabeled_names = data_unlabeled\n",
    "    \n",
    "    # 最直接的方法：\n",
    "    train_labeled_inputs, train_labeled_gts, train_labeled_names = [], [], []\n",
    "    train_unlabeled_inputs_list, train_unlabeled_names = [], []\n",
    "    for batch_iter in batch:\n",
    "        x, y = batch_iter\n",
    "        train_labeled_inputs.append(x[0])\n",
    "        train_labeled_gts.append(x[1])\n",
    "        train_labeled_names.append(x[2])\n",
    "        \n",
    "        train_unlabeled_inputs_list += y[0]\n",
    "        train_unlabeled_names += y[1]\n",
    "\n",
    "    train_labeled_inputs = torch.stack(train_labeled_inputs, 0)\n",
    "    train_unlabeled_inputs_list = torch.stack(train_unlabeled_inputs_list, 0)\n",
    "    train_labeled_gts = torch.stack(train_labeled_gts, 0)\n",
    "    print(train_unlabeled_inputs_list.size())\n",
    "    return ([train_labeled_inputs, train_unlabeled_inputs_list], \n",
    "            [train_labeled_gts],\n",
    "            [train_labeled_names, train_unlabeled_names])\n",
    "\n",
    "print(f\" ==>> 使用的训练集 <<==\\n -->> LABELED_PATH：{LABELED_PATH}\\n -->> UNLABELED_PATH：{UNLABELED_PATH}\")\n",
    "train_set = ImageFolder((LABELED_PATH, UNLABELED_PATH), \"train\", 320, prefix=('.jpg', '.png'), use_bigt=True, split_rate=(3, 9))\n",
    "# a simple custom collate function, just to show the idea\n",
    "train_loader = DataLoader(train_set, batch_size=3, num_workers=4, collate_fn=my_collate, shuffle=True, drop_last=False, pin_memory=True)\n",
    "print(\" ==>> data_loader构建完毕 <<==\")\n",
    "\n",
    "for train_idx, train_data in enumerate(train_loader):\n",
    "\n",
    "    train_inputs, train_gts, train_names = train_data\n",
    "    \n",
    "    train_labeled_inputs, train_unlabeled_inputs = train_inputs\n",
    "    train_labeled_gts = train_gts[0]\n",
    "    train_labeled_names, train_unlabeled_names = train_names\n",
    "    print(\"-->>\", train_labeled_inputs.size(), train_labeled_gts.size(), train_labeled_names)\n",
    "    print(\"-->>\", train_unlabeled_inputs.size(), train_unlabeled_names)\n",
    "    \n",
    "    train_labeled_inputs_batchsize = train_labeled_inputs.size(0)\n",
    "    train_unlabeled_inputs_batchsize = train_unlabeled_inputs.size(0)\n",
    "    \n",
    "    # 正常训练中下面应该有，这里为了方便就关掉了，这里之所以不先进行cat再进行to(dev)，是为了便于后面ema_model输入的时候使用一个已经在gpu上的张量，免去了再次搬运的麻烦\n",
    "    # train_labeled_inputs = train_labeled_inputs.to(dev)\n",
    "    # train_unlabeled_inputs = train_unlabeled_inputs.to(dev)\n",
    "    # train_gts = train_labeled_gts.to(self.dev)\n",
    "    train_inputs = torch.cat([train_labeled_inputs, train_unlabeled_inputs], dim=0)\n",
    "\n",
    "    # otr_total = net(train_inputs)\n",
    "    # labeled_otr, unlabeled_otr = otr_total.split((train_labeled_inputs_batchsize, train_unlabeled_inputs_batchsize), dim=0)\n",
    "    # with torch.no_grad():\n",
    "    #     ema_unlabeled_otr = ema_model(train_unlabeled_inputs)\n",
    "    print(\" ==>> 一个Batch结束了 <<== \")\n",
    "    if train_idx == 0:\n",
    "        break\n",
    "print(\" ==>> 一个Epoch结束了 <<== \")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
