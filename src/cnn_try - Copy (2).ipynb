{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0252, 0.0055],\n",
      "        [0.5658, 0.6703]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as scio\n",
    "import hiddenlayer as h\n",
    "from visdom import Visdom\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "\n",
    "# 这里很普通的检查cuda可用性\n",
    "x = torch.rand(2,2)\n",
    "print(x)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021_12_05_16:31:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "ckpDir = './/ckp//c6'\n",
    "if not os.path.exists(ckpDir):\n",
    "    os.makedirs(ckpDir)\n",
    "\n",
    "timeForSave = datetime.datetime.now().strftime('%Y_%m_%d_%H:%M:%S')\n",
    "print(timeForSave)\n",
    "vizx = 0\n",
    "viz = Visdom()\n",
    "viz1 = Visdom()\n",
    "viz2 = Visdom()\n",
    "viz3 = Visdom()\n",
    "viz4 = Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下两块是mat转numpy用的，训练时不需要运行，在此注掉了\n",
    "\n",
    "datas = scio.loadmat('../data/Forpytorch20211205T095754.mat')\n",
    "print(datas['segTR_X'].shape)\n",
    "Xtrain = np.transpose(datas['segTR_X'],[3,2,0,1])\n",
    "Xtest = np.transpose(datas['segTE_X'],[3,2,0,1])\n",
    "Xtest_o = np.transpose(datas['segTE_Xo'],[3,2,0,1])\n",
    "X_oo = np.transpose(datas['segTE_Xoo'],[3,2,0,1])\n",
    "\n",
    "print(Xtrain.shape,'\\n',Xtest.shape,'\\n',\\\n",
    "    Xtest_o.shape,'\\n',X_oo.shape)\n",
    "\n",
    "# 标签\n",
    "# print(datas1[0,0,:,:])\n",
    "Ytrain = datas['segTR_Y']\n",
    "Ytest = datas['segTE_Y']\n",
    "Ytest_o = datas['segTE_Yo']\n",
    "Y_oo = datas['segTE_Yoo']\n",
    "print(Ytrain.shape,'\\n',Ytest.shape,'\\n',\\\n",
    "    Ytest_o.shape,'\\n',Y_oo.shape)\n",
    "print(Ytrain.shape,'\\n',len(Ytrain),Ytrain[1,0])\n",
    "# print(len(datalb))\n",
    "# print(datas1[0,0,:,:])\n",
    "\n",
    "# print(datas['Y_TrainP'].ndim)\n",
    "# datalb = datas['Y_TrainP']\n",
    "# print(len(datalb))\n",
    "# print(datalb[1,0])\n",
    "# emg1 = datalb[1,0]\n",
    "# type(emg1)\n",
    "# emg1.astype(np.float32)\n",
    "# np.save('../data/trainY.npy',datalb)\n",
    "# print(datalb[1,0])\n",
    "dataset = {}\n",
    "dataset['Ytrain'] = Ytrain\n",
    "dataset['Xtrain'] = Xtrain\n",
    "dataset['Xtest'] = Xtest\n",
    "dataset['Xtest_o'] = Xtest_o\n",
    "dataset['X_oo'] = X_oo\n",
    "dataset['Ytest'] = Ytest\n",
    "dataset['Ytest_o'] = Ytest_o\n",
    "dataset['Y_oo'] = Y_oo\n",
    "np.save('../data/OpenSetDataSet.npy',dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('../data/trainX.npy',datas1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ytrain-20210721T193346.mat\n",
    "\n",
    "# datas = scio.loadmat('../data/Y_TrainP.mat')\n",
    "# print(datas['Y_TrainP'].ndim)\n",
    "# datalb = datas['Y_TrainP']\n",
    "# print(len(datalb))\n",
    "# print(datalb[1,0])\n",
    "# emg1 = datalb[1,0]\n",
    "# type(emg1)\n",
    "# emg1.astype(np.float32)\n",
    "# np.save('../data/trainY.npy',datalb)\n",
    "# print(datalb[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里检查了数据形状与维度，训练时不需要，注掉了\n",
    "\n",
    "# tx = np.load('../data/trainX.npy')\n",
    "# txp = tx[1,:,:,:]\n",
    "# print(txp.shape)\n",
    "# print(np.squeeze(txp).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "(tensor([[[0.0171, 0.0073, 0.0024, 0.0024, 0.0024, 0.0024, 0.0757, 0.0317,\n",
      "          0.0024, 0.0146],\n",
      "         [0.0171, 0.0244, 0.0024, 0.0024, 0.0024, 0.0024, 0.0684, 0.0342,\n",
      "          0.0024, 0.0146],\n",
      "         [0.0146, 0.0269, 0.0024, 0.0024, 0.0024, 0.0024, 0.0610, 0.0342,\n",
      "          0.0024, 0.0146],\n",
      "         [0.0220, 0.0195, 0.0024, 0.0024, 0.0024, 0.0024, 0.0513, 0.0366,\n",
      "          0.0024, 0.0098],\n",
      "         [0.0269, 0.0171, 0.0024, 0.0024, 0.0024, 0.0024, 0.0488, 0.0415,\n",
      "          0.0024, 0.0073],\n",
      "         [0.0269, 0.0293, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0488,\n",
      "          0.0024, 0.0073],\n",
      "         [0.0244, 0.0317, 0.0024, 0.0024, 0.0024, 0.0024, 0.1001, 0.0635,\n",
      "          0.0024, 0.0098],\n",
      "         [0.0220, 0.0244, 0.0024, 0.0024, 0.0024, 0.0024, 0.1074, 0.0757,\n",
      "          0.0024, 0.0195],\n",
      "         [0.0171, 0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.1172, 0.0854,\n",
      "          0.0024, 0.0220],\n",
      "         [0.0195, 0.0049, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0854,\n",
      "          0.0024, 0.0220],\n",
      "         [0.0269, 0.0049, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0806,\n",
      "          0.0024, 0.0146],\n",
      "         [0.0342, 0.0171, 0.0024, 0.0024, 0.0024, 0.0024, 0.1221, 0.0781,\n",
      "          0.0024, 0.0098],\n",
      "         [0.0391, 0.0171, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0732,\n",
      "          0.0024, 0.0049],\n",
      "         [0.0488, 0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.1001, 0.0659,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0464, 0.0122, 0.0024, 0.0024, 0.0049, 0.0024, 0.0830, 0.0610,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0464, 0.0073, 0.0024, 0.0024, 0.0024, 0.0024, 0.0757, 0.0562,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0439, 0.0244, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0562,\n",
      "          0.0024, 0.0073],\n",
      "         [0.0488, 0.0415, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0537,\n",
      "          0.0024, 0.0098],\n",
      "         [0.0586, 0.0635, 0.0024, 0.0024, 0.0024, 0.0024, 0.0952, 0.0562,\n",
      "          0.0024, 0.0122],\n",
      "         [0.0537, 0.1196, 0.0049, 0.0024, 0.0024, 0.0024, 0.1025, 0.0537,\n",
      "          0.0098, 0.0098],\n",
      "         [0.0488, 0.1538, 0.0073, 0.0024, 0.0024, 0.0024, 0.0952, 0.0513,\n",
      "          0.0610, 0.0073],\n",
      "         [0.0391, 0.1538, 0.0049, 0.0024, 0.0024, 0.0024, 0.0806, 0.0464,\n",
      "          0.0806, 0.0098],\n",
      "         [0.0317, 0.1367, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0415,\n",
      "          0.0806, 0.0073],\n",
      "         [0.0317, 0.1123, 0.0024, 0.0024, 0.0024, 0.0024, 0.0659, 0.0439,\n",
      "          0.0732, 0.0049],\n",
      "         [0.0269, 0.0928, 0.0024, 0.0024, 0.0024, 0.0024, 0.0586, 0.0439,\n",
      "          0.0586, 0.0024],\n",
      "         [0.0244, 0.0708, 0.0024, 0.0024, 0.0024, 0.0024, 0.0757, 0.0439,\n",
      "          0.0488, 0.0024],\n",
      "         [0.0220, 0.0537, 0.0122, 0.0024, 0.0024, 0.0024, 0.1025, 0.0464,\n",
      "          0.0366, 0.0024],\n",
      "         [0.0195, 0.0391, 0.0220, 0.0024, 0.0024, 0.0024, 0.1294, 0.0439,\n",
      "          0.0293, 0.0024],\n",
      "         [0.0146, 0.0269, 0.0195, 0.0024, 0.0024, 0.0024, 0.1294, 0.0464,\n",
      "          0.0171, 0.0024],\n",
      "         [0.0122, 0.0146, 0.0146, 0.0024, 0.0024, 0.0024, 0.1196, 0.0488,\n",
      "          0.0098, 0.0024],\n",
      "         [0.0073, 0.0024, 0.0049, 0.0024, 0.0024, 0.0024, 0.1099, 0.0464,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0122, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0952, 0.0464,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0903, 0.0439,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0293, 0.0220, 0.0024, 0.0024, 0.0024, 0.0024, 0.0879, 0.0415,\n",
      "          0.0269, 0.0024],\n",
      "         [0.0366, 0.0684, 0.0049, 0.0024, 0.0024, 0.0024, 0.1270, 0.0439,\n",
      "          0.0952, 0.0024],\n",
      "         [0.0366, 0.1001, 0.0024, 0.0024, 0.0024, 0.0024, 0.1440, 0.0439,\n",
      "          0.1465, 0.0024],\n",
      "         [0.0415, 0.1123, 0.0146, 0.0024, 0.0024, 0.0024, 0.1538, 0.0464,\n",
      "          0.1563, 0.0024],\n",
      "         [0.0391, 0.1050, 0.0220, 0.0024, 0.0024, 0.0024, 0.1587, 0.0464,\n",
      "          0.1465, 0.0024],\n",
      "         [0.0537, 0.0928, 0.0366, 0.0024, 0.0024, 0.0024, 0.1489, 0.0464,\n",
      "          0.1270, 0.0024],\n",
      "         [0.0586, 0.0806, 0.0586, 0.0024, 0.0024, 0.0024, 0.1392, 0.0415,\n",
      "          0.1050, 0.0024]]], dtype=torch.float64), 0)\n",
      "(tensor([[[0.0269, 0.0049, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0806,\n",
      "          0.0024, 0.0146],\n",
      "         [0.0342, 0.0171, 0.0024, 0.0024, 0.0024, 0.0024, 0.1221, 0.0781,\n",
      "          0.0024, 0.0098],\n",
      "         [0.0391, 0.0171, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0732,\n",
      "          0.0024, 0.0049],\n",
      "         [0.0488, 0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.1001, 0.0659,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0464, 0.0122, 0.0024, 0.0024, 0.0049, 0.0024, 0.0830, 0.0610,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0464, 0.0073, 0.0024, 0.0024, 0.0024, 0.0024, 0.0757, 0.0562,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0439, 0.0244, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0562,\n",
      "          0.0024, 0.0073],\n",
      "         [0.0488, 0.0415, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0537,\n",
      "          0.0024, 0.0098],\n",
      "         [0.0586, 0.0635, 0.0024, 0.0024, 0.0024, 0.0024, 0.0952, 0.0562,\n",
      "          0.0024, 0.0122],\n",
      "         [0.0537, 0.1196, 0.0049, 0.0024, 0.0024, 0.0024, 0.1025, 0.0537,\n",
      "          0.0098, 0.0098],\n",
      "         [0.0488, 0.1538, 0.0073, 0.0024, 0.0024, 0.0024, 0.0952, 0.0513,\n",
      "          0.0610, 0.0073],\n",
      "         [0.0391, 0.1538, 0.0049, 0.0024, 0.0024, 0.0024, 0.0806, 0.0464,\n",
      "          0.0806, 0.0098],\n",
      "         [0.0317, 0.1367, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0415,\n",
      "          0.0806, 0.0073],\n",
      "         [0.0317, 0.1123, 0.0024, 0.0024, 0.0024, 0.0024, 0.0659, 0.0439,\n",
      "          0.0732, 0.0049],\n",
      "         [0.0269, 0.0928, 0.0024, 0.0024, 0.0024, 0.0024, 0.0586, 0.0439,\n",
      "          0.0586, 0.0024],\n",
      "         [0.0244, 0.0708, 0.0024, 0.0024, 0.0024, 0.0024, 0.0757, 0.0439,\n",
      "          0.0488, 0.0024],\n",
      "         [0.0220, 0.0537, 0.0122, 0.0024, 0.0024, 0.0024, 0.1025, 0.0464,\n",
      "          0.0366, 0.0024],\n",
      "         [0.0195, 0.0391, 0.0220, 0.0024, 0.0024, 0.0024, 0.1294, 0.0439,\n",
      "          0.0293, 0.0024],\n",
      "         [0.0146, 0.0269, 0.0195, 0.0024, 0.0024, 0.0024, 0.1294, 0.0464,\n",
      "          0.0171, 0.0024],\n",
      "         [0.0122, 0.0146, 0.0146, 0.0024, 0.0024, 0.0024, 0.1196, 0.0488,\n",
      "          0.0098, 0.0024],\n",
      "         [0.0073, 0.0024, 0.0049, 0.0024, 0.0024, 0.0024, 0.1099, 0.0464,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0122, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0952, 0.0464,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0903, 0.0439,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0293, 0.0220, 0.0024, 0.0024, 0.0024, 0.0024, 0.0879, 0.0415,\n",
      "          0.0269, 0.0024],\n",
      "         [0.0366, 0.0684, 0.0049, 0.0024, 0.0024, 0.0024, 0.1270, 0.0439,\n",
      "          0.0952, 0.0024],\n",
      "         [0.0366, 0.1001, 0.0024, 0.0024, 0.0024, 0.0024, 0.1440, 0.0439,\n",
      "          0.1465, 0.0024],\n",
      "         [0.0415, 0.1123, 0.0146, 0.0024, 0.0024, 0.0024, 0.1538, 0.0464,\n",
      "          0.1563, 0.0024],\n",
      "         [0.0391, 0.1050, 0.0220, 0.0024, 0.0024, 0.0024, 0.1587, 0.0464,\n",
      "          0.1465, 0.0024],\n",
      "         [0.0537, 0.0928, 0.0366, 0.0024, 0.0024, 0.0024, 0.1489, 0.0464,\n",
      "          0.1270, 0.0024],\n",
      "         [0.0586, 0.0806, 0.0586, 0.0024, 0.0024, 0.0024, 0.1392, 0.0415,\n",
      "          0.1050, 0.0024],\n",
      "         [0.0537, 0.0684, 0.0708, 0.0024, 0.0024, 0.0024, 0.1343, 0.0415,\n",
      "          0.0830, 0.0024],\n",
      "         [0.0513, 0.0610, 0.0806, 0.0024, 0.0024, 0.0024, 0.1587, 0.0488,\n",
      "          0.0684, 0.0024],\n",
      "         [0.0464, 0.1074, 0.0806, 0.0024, 0.0024, 0.0024, 0.1636, 0.0586,\n",
      "          0.0635, 0.0024],\n",
      "         [0.0391, 0.1465, 0.0732, 0.0024, 0.0024, 0.0024, 0.1538, 0.0610,\n",
      "          0.0830, 0.0024],\n",
      "         [0.0464, 0.1514, 0.0635, 0.0024, 0.0024, 0.0024, 0.1367, 0.0586,\n",
      "          0.0854, 0.0024],\n",
      "         [0.0586, 0.1392, 0.0635, 0.0024, 0.0024, 0.0024, 0.1147, 0.0537,\n",
      "          0.0757, 0.0024],\n",
      "         [0.0610, 0.1196, 0.0708, 0.0024, 0.0024, 0.0024, 0.1392, 0.0562,\n",
      "          0.0659, 0.0049],\n",
      "         [0.0562, 0.1123, 0.0854, 0.0024, 0.0024, 0.0024, 0.1611, 0.0659,\n",
      "          0.0537, 0.0024],\n",
      "         [0.0513, 0.1074, 0.0928, 0.0024, 0.0024, 0.0024, 0.1758, 0.0684,\n",
      "          0.0439, 0.0024],\n",
      "         [0.0464, 0.0977, 0.1001, 0.0049, 0.0024, 0.0024, 0.1636, 0.0708,\n",
      "          0.0342, 0.0024]]], dtype=torch.float64), 0)\n"
     ]
    }
   ],
   "source": [
    "# 自定义数据集类\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图片转换为Tensor,归一化至[0,1]\n",
    "])\n",
    "\n",
    "class EMGDataset(Dataset):\n",
    " \n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transforms = transform\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        emgData = self.data[index,:,:,:]\n",
    "        emgData = np.squeeze(emgData)#似乎不应该压缩了\n",
    "        emglabel = self.label[index]\n",
    "        emglabel = emglabel.astype(np.int16)\n",
    "        emgData = self.transforms(emgData)      \n",
    "        \n",
    "        return emgData,emglabel\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    " \n",
    " \n",
    "# if __name__ == '__main__':\n",
    "dataarray = np.load('../data/OpenSetDataSet.npy',allow_pickle=True)\n",
    "CNNdataset = dataarray.item()\n",
    "print(type(CNNdataset))\n",
    "traindata = CNNdataset['Xtrain']\n",
    "trainlabel = CNNdataset['Ytrain']\n",
    "testdata = CNNdataset['Xtest']\n",
    "testlabel = CNNdataset['Ytest']\n",
    "# # print(trainlabel[:,0])\n",
    "\n",
    "trainlabel = trainlabel[:,0]\n",
    "testlabel = testlabel[:,0]\n",
    "# print(type(trainlabel))\n",
    "train_set = EMGDataset(traindata, trainlabel)\n",
    "test_set = EMGDataset(testdata, testlabel)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, pin_memory=True,\n",
    "#                                             num_workers=3)\n",
    "\n",
    "sample = next(iter(train_set))\n",
    "print(sample)\n",
    "sample = next(iter(test_set))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=4352, out_features=128, bias=True)\n",
      "  (out): Linear(in_features=128, out_features=6, bias=True)\n",
      "  (dr1): Dropout2d(p=0.2, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [1, 32, 38, 8]             320\n",
      "            Conv2d-2             [1, 32, 35, 5]           9,248\n",
      "            Linear-3                   [1, 128]         557,184\n",
      "         Dropout2d-4                   [1, 128]               0\n",
      "            Linear-5                     [1, 6]             774\n",
      "================================================================\n",
      "Total params: 567,526\n",
      "Trainable params: 567,526\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.12\n",
      "Params size (MB): 2.16\n",
      "Estimated Total Size (MB): 2.29\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.png'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义神经网络\n",
    "\n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=32 * 4 * 34, out_features=128)\n",
    "        self.out = nn.Linear(in_features=128, out_features=6)\n",
    "        self.dr1 = nn.Dropout2d(0.2)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        # t = self.dr1(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 32 * 4 * 34)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.dr1(t)\n",
    "\n",
    "        # (5) output layer\n",
    "        t = self.out(t)\n",
    "\n",
    "        return t\n",
    "        \n",
    "net = Network()\n",
    "# 打印网络，检查输入输出 shape是否正确\n",
    "print(net)\n",
    "summary(net,(1,40,10),batch_size = 1,device = \"cpu\")\n",
    "\n",
    "# 可视化结构，hiddenlayer\n",
    "# vis_graph = h.build_graph(net, torch.zeros([1,1,40,10]))\n",
    "# vis_graph.theme = h.graph.THEMES[\"blue\"].copy()\n",
    "# vis_graph.save(\"./CNNtrynetframe\")\n",
    "# 可视化结构，torchviz\n",
    "sampleInput = torch.randn(1,1,40,10).requires_grad_(True)\n",
    "sampleOutput = net(sampleInput)\n",
    "framevision = make_dot(sampleOutput, params=dict(list(net.named_parameters()) + [('x',sampleInput)]))\n",
    "framevision.format = \"png\"\n",
    "framevision.direcory = \"./\"\n",
    "framevision.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2552\n"
     ]
    }
   ],
   "source": [
    "# import datetime\n",
    "\n",
    "# timeForSave = datetime.datetime.now().strftime('%Y_%m_%d_%H:%M:%S')\n",
    "# print(timeForSave)\n",
    "print(trainlabel.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_train_acc: 0.19905956112852666 loss: 5.344899296760559\n",
      "epoch 1 total_train_acc: 0.21826018808777428 loss: 5.165574669837952\n",
      "epoch 2 total_train_acc: 0.22884012539184953 loss: 5.002880334854126\n",
      "epoch 3 total_train_acc: 0.3221003134796238 loss: 4.821783185005188\n",
      "epoch 4 total_train_acc: 0.38636363636363635 loss: 4.662320971488953\n",
      "epoch 5 total_train_acc: 0.4306426332288401 loss: 4.489436149597168\n",
      "epoch 6 total_train_acc: 0.4408307210031348 loss: 4.336328983306885\n",
      "epoch 7 total_train_acc: 0.4431818181818182 loss: 4.201362729072571\n",
      "epoch 8 total_train_acc: 0.46394984326018807 loss: 4.056785941123962\n",
      "epoch 9 total_train_acc: 0.460423197492163 loss: 3.96440052986145\n",
      "epoch 10 total_train_acc: 0.4862852664576803 loss: 3.872239828109741\n",
      "epoch 11 total_train_acc: 0.5066614420062696 loss: 3.7754141092300415\n",
      "epoch 12 total_train_acc: 0.4992163009404389 loss: 3.701851010322571\n",
      "epoch 13 total_train_acc: 0.5223354231974922 loss: 3.6414469480514526\n",
      "epoch 14 total_train_acc: 0.5282131661442007 loss: 3.5423693656921387\n",
      "epoch 15 total_train_acc: 0.5462382445141066 loss: 3.4461933374404907\n",
      "epoch 16 total_train_acc: 0.5564263322884012 loss: 3.376470446586609\n",
      "epoch 17 total_train_acc: 0.5771943573667712 loss: 3.3125829696655273\n",
      "epoch 18 total_train_acc: 0.5693573667711599 loss: 3.2649707794189453\n",
      "epoch 19 total_train_acc: 0.5940438871473355 loss: 3.2031211853027344\n",
      "epoch 20 total_train_acc: 0.609717868338558 loss: 3.1329216957092285\n",
      "epoch 21 total_train_acc: 0.6155956112852664 loss: 3.0737630128860474\n",
      "epoch 22 total_train_acc: 0.6253918495297806 loss: 3.0335882902145386\n",
      "epoch 23 total_train_acc: 0.6273510971786834 loss: 2.958568513393402\n",
      "epoch 24 total_train_acc: 0.6363636363636364 loss: 2.9184486269950867\n",
      "epoch 25 total_train_acc: 0.649294670846395 loss: 2.8722395300865173\n",
      "epoch 26 total_train_acc: 0.6590909090909091 loss: 2.819439113140106\n",
      "epoch 27 total_train_acc: 0.6724137931034483 loss: 2.7966233491897583\n",
      "epoch 28 total_train_acc: 0.6704545454545454 loss: 2.7601983547210693\n",
      "epoch 29 total_train_acc: 0.679858934169279 loss: 2.6952918767929077\n",
      "epoch 30 total_train_acc: 0.6790752351097179 loss: 2.677843928337097\n",
      "epoch 31 total_train_acc: 0.6880877742946708 loss: 2.613413095474243\n",
      "epoch 32 total_train_acc: 0.6963166144200627 loss: 2.5731754302978516\n",
      "epoch 33 total_train_acc: 0.6974921630094044 loss: 2.5557351112365723\n",
      "epoch 34 total_train_acc: 0.7100313479623824 loss: 2.501460015773773\n",
      "epoch 35 total_train_acc: 0.710423197492163 loss: 2.4758668541908264\n",
      "epoch 36 total_train_acc: 0.7096394984326019 loss: 2.458432972431183\n",
      "epoch 37 total_train_acc: 0.7119905956112853 loss: 2.434403657913208\n",
      "epoch 38 total_train_acc: 0.7233542319749217 loss: 2.4264038801193237\n",
      "epoch 39 total_train_acc: 0.7264890282131662 loss: 2.3692128658294678\n",
      "epoch 40 total_train_acc: 0.7300156739811913 loss: 2.339755654335022\n",
      "epoch 41 total_train_acc: 0.7382445141065831 loss: 2.2890851497650146\n",
      "epoch 42 total_train_acc: 0.7394200626959248 loss: 2.270627737045288\n",
      "epoch 43 total_train_acc: 0.7386363636363636 loss: 2.253929018974304\n",
      "epoch 44 total_train_acc: 0.7480407523510971 loss: 2.172734558582306\n",
      "epoch 45 total_train_acc: 0.7476489028213166 loss: 2.207470655441284\n",
      "epoch 46 total_train_acc: 0.7578369905956113 loss: 2.1364359855651855\n",
      "epoch 47 total_train_acc: 0.7496081504702194 loss: 2.1420478224754333\n",
      "epoch 48 total_train_acc: 0.7582288401253918 loss: 2.115275025367737\n",
      "epoch 49 total_train_acc: 0.7621473354231975 loss: 2.062222719192505\n",
      "epoch 50 total_train_acc: 0.765282131661442 loss: 2.0474356412887573\n",
      "epoch 51 total_train_acc: 0.7684169278996865 loss: 2.022846221923828\n",
      "epoch 52 total_train_acc: 0.7762539184952978 loss: 1.992270588874817\n",
      "epoch 53 total_train_acc: 0.7782131661442007 loss: 1.9336838126182556\n",
      "epoch 54 total_train_acc: 0.7782131661442007 loss: 1.9430913925170898\n",
      "epoch 55 total_train_acc: 0.780564263322884 loss: 1.8790873885154724\n",
      "epoch 56 total_train_acc: 0.7840909090909091 loss: 1.8900627493858337\n",
      "epoch 57 total_train_acc: 0.7852664576802508 loss: 1.8527393341064453\n",
      "epoch 58 total_train_acc: 0.7884012539184952 loss: 1.8391090035438538\n",
      "epoch 59 total_train_acc: 0.7856583072100314 loss: 1.8240466117858887\n",
      "epoch 60 total_train_acc: 0.79858934169279 loss: 1.774269938468933\n",
      "epoch 61 total_train_acc: 0.7970219435736677 loss: 1.7659196853637695\n",
      "epoch 62 total_train_acc: 0.8001567398119123 loss: 1.75379079580307\n",
      "epoch 63 total_train_acc: 0.8044670846394985 loss: 1.7222225666046143\n",
      "epoch 64 total_train_acc: 0.8099529780564263 loss: 1.6996030807495117\n",
      "epoch 65 total_train_acc: 0.8044670846394985 loss: 1.6849639415740967\n",
      "epoch 66 total_train_acc: 0.8181818181818182 loss: 1.6478139162063599\n",
      "epoch 67 total_train_acc: 0.8150470219435737 loss: 1.6329299807548523\n",
      "epoch 68 total_train_acc: 0.8162225705329154 loss: 1.601685881614685\n",
      "epoch 69 total_train_acc: 0.8193573667711599 loss: 1.6050403714179993\n",
      "epoch 70 total_train_acc: 0.817398119122257 loss: 1.6357015371322632\n",
      "epoch 71 total_train_acc: 0.8205329153605015 loss: 1.5863845944404602\n",
      "epoch 72 total_train_acc: 0.8283699059561128 loss: 1.570307195186615\n",
      "epoch 73 total_train_acc: 0.8385579937304075 loss: 1.5370007157325745\n",
      "epoch 74 total_train_acc: 0.8248432601880877 loss: 1.53111332654953\n",
      "epoch 75 total_train_acc: 0.8307210031347962 loss: 1.5036770105361938\n",
      "epoch 76 total_train_acc: 0.829153605015674 loss: 1.4792544841766357\n",
      "epoch 77 total_train_acc: 0.8342476489028213 loss: 1.4835494756698608\n",
      "epoch 78 total_train_acc: 0.8330721003134797 loss: 1.4186550378799438\n",
      "epoch 79 total_train_acc: 0.8463949843260188 loss: 1.4334695935249329\n",
      "epoch 80 total_train_acc: 0.8424764890282131 loss: 1.431530624628067\n",
      "epoch 81 total_train_acc: 0.8424764890282131 loss: 1.3800719380378723\n",
      "epoch 82 total_train_acc: 0.8460031347962382 loss: 1.4214406311511993\n",
      "epoch 83 total_train_acc: 0.8452194357366771 loss: 1.386498749256134\n",
      "epoch 84 total_train_acc: 0.8413009404388715 loss: 1.347125232219696\n",
      "epoch 85 total_train_acc: 0.8487460815047022 loss: 1.359173208475113\n",
      "epoch 86 total_train_acc: 0.8526645768025078 loss: 1.3334246575832367\n",
      "epoch 87 total_train_acc: 0.8620689655172413 loss: 1.3188812732696533\n",
      "epoch 88 total_train_acc: 0.8530564263322884 loss: 1.3533092141151428\n",
      "epoch 89 total_train_acc: 0.8632445141065831 loss: 1.298915147781372\n",
      "epoch 90 total_train_acc: 0.8589341692789969 loss: 1.3186846673488617\n",
      "epoch 91 total_train_acc: 0.8605015673981191 loss: 1.2901627123355865\n",
      "epoch 92 total_train_acc: 0.859717868338558 loss: 1.2737114429473877\n",
      "epoch 93 total_train_acc: 0.8663793103448276 loss: 1.2420906126499176\n",
      "epoch 94 total_train_acc: 0.8655956112852664 loss: 1.2303921282291412\n",
      "epoch 95 total_train_acc: 0.8648119122257053 loss: 1.251979649066925\n",
      "epoch 96 total_train_acc: 0.8687304075235109 loss: 1.2535563707351685\n",
      "epoch 97 total_train_acc: 0.8671630094043887 loss: 1.209159791469574\n",
      "epoch 98 total_train_acc: 0.8593260188087775 loss: 1.2044158577919006\n",
      "epoch 99 total_train_acc: 0.8730407523510971 loss: 1.1843946278095245\n",
      "epoch 100 total_train_acc: 0.8734326018808778 loss: 1.1650883853435516\n",
      "epoch 101 total_train_acc: 0.8785266457680251 loss: 1.1624752581119537\n",
      "epoch 102 total_train_acc: 0.8738244514106583 loss: 1.1736654937267303\n",
      "epoch 103 total_train_acc: 0.8695141065830722 loss: 1.1745342314243317\n",
      "epoch 104 total_train_acc: 0.8808777429467085 loss: 1.137943297624588\n",
      "epoch 105 total_train_acc: 0.8726489028213166 loss: 1.1754972636699677\n",
      "epoch 106 total_train_acc: 0.8781347962382445 loss: 1.1538178026676178\n",
      "epoch 107 total_train_acc: 0.8773510971786834 loss: 1.1224050223827362\n",
      "epoch 108 total_train_acc: 0.8855799373040752 loss: 1.1064396500587463\n",
      "epoch 109 total_train_acc: 0.8859717868338558 loss: 1.0795415937900543\n",
      "epoch 110 total_train_acc: 0.8804858934169278 loss: 1.0943868160247803\n",
      "epoch 111 total_train_acc: 0.8894984326018809 loss: 1.0751610100269318\n",
      "epoch 112 total_train_acc: 0.8867554858934169 loss: 1.0821238458156586\n",
      "epoch 113 total_train_acc: 0.8800940438871473 loss: 1.0843645930290222\n",
      "epoch 114 total_train_acc: 0.8871473354231975 loss: 1.0619786083698273\n",
      "epoch 115 total_train_acc: 0.8891065830721003 loss: 1.056684672832489\n",
      "epoch 116 total_train_acc: 0.8871473354231975 loss: 1.0650780200958252\n",
      "epoch 117 total_train_acc: 0.8898902821316614 loss: 1.064347743988037\n",
      "epoch 118 total_train_acc: 0.8926332288401254 loss: 1.045517474412918\n",
      "epoch 119 total_train_acc: 0.8887147335423198 loss: 1.0415273308753967\n",
      "epoch 120 total_train_acc: 0.8934169278996865 loss: 1.011524349451065\n",
      "epoch 121 total_train_acc: 0.8945924764890282 loss: 1.0319204330444336\n",
      "epoch 122 total_train_acc: 0.8934169278996865 loss: 1.0091884434223175\n",
      "epoch 123 total_train_acc: 0.8918495297805643 loss: 1.0099211931228638\n",
      "epoch 124 total_train_acc: 0.8973354231974922 loss: 1.0110398530960083\n",
      "epoch 125 total_train_acc: 0.8934169278996865 loss: 1.0125864148139954\n",
      "epoch 126 total_train_acc: 0.89576802507837 loss: 1.0061523020267487\n",
      "epoch 127 total_train_acc: 0.8942006269592476 loss: 0.978634774684906\n",
      "epoch 128 total_train_acc: 0.8996865203761756 loss: 0.9779147803783417\n",
      "epoch 129 total_train_acc: 0.9028213166144201 loss: 0.9559368789196014\n",
      "epoch 130 total_train_acc: 0.8977272727272727 loss: 0.9705644845962524\n",
      "epoch 131 total_train_acc: 0.8953761755485894 loss: 0.9726901054382324\n",
      "epoch 132 total_train_acc: 0.9043887147335423 loss: 0.9727261066436768\n",
      "epoch 133 total_train_acc: 0.9039968652037618 loss: 0.9971916675567627\n",
      "epoch 134 total_train_acc: 0.9004702194357367 loss: 0.9472720623016357\n",
      "epoch 135 total_train_acc: 0.9020376175548589 loss: 0.9284653663635254\n",
      "epoch 136 total_train_acc: 0.9024294670846394 loss: 0.9390417039394379\n",
      "epoch 137 total_train_acc: 0.905564263322884 loss: 0.9269523322582245\n",
      "epoch 138 total_train_acc: 0.9024294670846394 loss: 0.9540988504886627\n",
      "epoch 139 total_train_acc: 0.9047805642633229 loss: 0.9313595592975616\n",
      "epoch 140 total_train_acc: 0.9071316614420063 loss: 0.9209838211536407\n",
      "epoch 141 total_train_acc: 0.9102664576802508 loss: 0.9080135524272919\n",
      "epoch 142 total_train_acc: 0.9102664576802508 loss: 0.9252701103687286\n",
      "epoch 143 total_train_acc: 0.9028213166144201 loss: 0.9136472344398499\n",
      "epoch 144 total_train_acc: 0.9063479623824452 loss: 0.8930273056030273\n",
      "epoch 145 total_train_acc: 0.9075235109717869 loss: 0.896099179983139\n",
      "epoch 146 total_train_acc: 0.9086990595611285 loss: 0.8752965927124023\n",
      "epoch 147 total_train_acc: 0.9106583072100314 loss: 0.8663691580295563\n",
      "epoch 148 total_train_acc: 0.9036050156739812 loss: 0.8545078039169312\n",
      "epoch 149 total_train_acc: 0.9075235109717869 loss: 0.8585043549537659\n",
      "epoch 150 total_train_acc: 0.9114420062695925 loss: 0.8933148682117462\n",
      "epoch 151 total_train_acc: 0.9094827586206896 loss: 0.8555629849433899\n",
      "epoch 152 total_train_acc: 0.9122257053291536 loss: 0.8808692097663879\n",
      "epoch 153 total_train_acc: 0.9098746081504702 loss: 0.8610603511333466\n",
      "epoch 154 total_train_acc: 0.908307210031348 loss: 0.8510493040084839\n",
      "epoch 155 total_train_acc: 0.9102664576802508 loss: 0.8820418417453766\n",
      "epoch 156 total_train_acc: 0.9134012539184952 loss: 0.8459676504135132\n",
      "epoch 157 total_train_acc: 0.9086990595611285 loss: 0.8428938090801239\n",
      "epoch 158 total_train_acc: 0.911833855799373 loss: 0.8498735725879669\n",
      "epoch 159 total_train_acc: 0.9220219435736677 loss: 0.8210513144731522\n",
      "epoch 160 total_train_acc: 0.9126175548589341 loss: 0.8533703088760376\n",
      "epoch 161 total_train_acc: 0.9200626959247649 loss: 0.8109286725521088\n",
      "epoch 162 total_train_acc: 0.9090909090909091 loss: 0.8413889110088348\n",
      "epoch 163 total_train_acc: 0.9086990595611285 loss: 0.847012460231781\n",
      "epoch 164 total_train_acc: 0.9184952978056427 loss: 0.8030260354280472\n",
      "epoch 165 total_train_acc: 0.9165360501567398 loss: 0.8254744708538055\n",
      "epoch 166 total_train_acc: 0.9114420062695925 loss: 0.7922112494707108\n",
      "epoch 167 total_train_acc: 0.9165360501567398 loss: 0.8023465573787689\n",
      "epoch 168 total_train_acc: 0.9141849529780565 loss: 0.80056431889534\n",
      "epoch 169 total_train_acc: 0.9177115987460815 loss: 0.7766289412975311\n",
      "epoch 170 total_train_acc: 0.9169278996865203 loss: 0.7837565988302231\n",
      "epoch 171 total_train_acc: 0.920846394984326 loss: 0.8022993505001068\n",
      "epoch 172 total_train_acc: 0.9149686520376176 loss: 0.7880813777446747\n",
      "epoch 173 total_train_acc: 0.9192789968652038 loss: 0.8003661036491394\n",
      "epoch 174 total_train_acc: 0.9220219435736677 loss: 0.8015245795249939\n",
      "epoch 175 total_train_acc: 0.9196708463949843 loss: 0.7718960046768188\n",
      "epoch 176 total_train_acc: 0.9177115987460815 loss: 0.7725067436695099\n",
      "epoch 177 total_train_acc: 0.9192789968652038 loss: 0.7648974061012268\n",
      "epoch 178 total_train_acc: 0.9224137931034483 loss: 0.7553400844335556\n",
      "epoch 179 total_train_acc: 0.9192789968652038 loss: 0.7786006033420563\n",
      "epoch 180 total_train_acc: 0.9200626959247649 loss: 0.7651412785053253\n",
      "epoch 181 total_train_acc: 0.9228056426332288 loss: 0.771960124373436\n",
      "epoch 182 total_train_acc: 0.9181034482758621 loss: 0.7625942230224609\n",
      "epoch 183 total_train_acc: 0.9192789968652038 loss: 0.7317031174898148\n",
      "epoch 184 total_train_acc: 0.924373040752351 loss: 0.7423236221075058\n",
      "epoch 185 total_train_acc: 0.927115987460815 loss: 0.7435011863708496\n",
      "epoch 186 total_train_acc: 0.9212382445141066 loss: 0.7656306773424149\n",
      "epoch 187 total_train_acc: 0.9212382445141066 loss: 0.7317163199186325\n",
      "epoch 188 total_train_acc: 0.9228056426332288 loss: 0.7271042615175247\n",
      "epoch 189 total_train_acc: 0.9200626959247649 loss: 0.756778821349144\n",
      "epoch 190 total_train_acc: 0.9255485893416928 loss: 0.744018167257309\n",
      "epoch 191 total_train_acc: 0.920846394984326 loss: 0.7387385070323944\n",
      "epoch 192 total_train_acc: 0.9294670846394985 loss: 0.7192246168851852\n",
      "epoch 193 total_train_acc: 0.9239811912225705 loss: 0.7244367003440857\n",
      "epoch 194 total_train_acc: 0.9251567398119123 loss: 0.7283763140439987\n",
      "epoch 195 total_train_acc: 0.9259404388714734 loss: 0.7205961793661118\n",
      "epoch 196 total_train_acc: 0.9259404388714734 loss: 0.7272185385227203\n",
      "epoch 197 total_train_acc: 0.9290752351097179 loss: 0.7092781215906143\n",
      "epoch 198 total_train_acc: 0.9263322884012539 loss: 0.7462247014045715\n",
      "epoch 199 total_train_acc: 0.9286833855799373 loss: 0.6971099972724915\n",
      "epoch 200 total_train_acc: 0.9278996865203761 loss: 0.702711820602417\n",
      "epoch 201 total_train_acc: 0.9224137931034483 loss: 0.7290469110012054\n",
      "epoch 202 total_train_acc: 0.9231974921630094 loss: 0.7046856731176376\n",
      "epoch 203 total_train_acc: 0.9345611285266457 loss: 0.6838416159152985\n",
      "epoch 204 total_train_acc: 0.9318181818181818 loss: 0.6947114914655685\n",
      "epoch 205 total_train_acc: 0.92358934169279 loss: 0.6996411830186844\n",
      "epoch 206 total_train_acc: 0.9275078369905956 loss: 0.7079470455646515\n",
      "epoch 207 total_train_acc: 0.9282915360501567 loss: 0.6812019944190979\n",
      "epoch 208 total_train_acc: 0.9286833855799373 loss: 0.6985302865505219\n",
      "epoch 209 total_train_acc: 0.92358934169279 loss: 0.6755868047475815\n",
      "epoch 210 total_train_acc: 0.9286833855799373 loss: 0.6795557588338852\n",
      "epoch 211 total_train_acc: 0.9337774294670846 loss: 0.6941720247268677\n",
      "epoch 212 total_train_acc: 0.9302507836990596 loss: 0.6951844096183777\n",
      "epoch 213 total_train_acc: 0.9314263322884012 loss: 0.6729573458433151\n",
      "epoch 214 total_train_acc: 0.9314263322884012 loss: 0.6607556790113449\n",
      "epoch 215 total_train_acc: 0.9275078369905956 loss: 0.680182933807373\n",
      "epoch 216 total_train_acc: 0.9329937304075235 loss: 0.6612030565738678\n",
      "epoch 217 total_train_acc: 0.9302507836990596 loss: 0.6636698693037033\n",
      "epoch 218 total_train_acc: 0.9278996865203761 loss: 0.6842387467622757\n",
      "epoch 219 total_train_acc: 0.9310344827586207 loss: 0.679565042257309\n",
      "epoch 220 total_train_acc: 0.932601880877743 loss: 0.6627693027257919\n",
      "epoch 221 total_train_acc: 0.9322100313479624 loss: 0.6791094988584518\n",
      "epoch 222 total_train_acc: 0.9333855799373041 loss: 0.6473730206489563\n",
      "epoch 223 total_train_acc: 0.9341692789968652 loss: 0.6596277505159378\n",
      "epoch 224 total_train_acc: 0.9314263322884012 loss: 0.6468589752912521\n",
      "epoch 225 total_train_acc: 0.9361285266457681 loss: 0.6753923445940018\n",
      "epoch 226 total_train_acc: 0.9369122257053292 loss: 0.6333414763212204\n",
      "epoch 227 total_train_acc: 0.9329937304075235 loss: 0.6479523628950119\n",
      "epoch 228 total_train_acc: 0.9306426332288401 loss: 0.6581220775842667\n",
      "epoch 229 total_train_acc: 0.9349529780564263 loss: 0.6416518986225128\n",
      "epoch 230 total_train_acc: 0.9349529780564263 loss: 0.643867164850235\n",
      "epoch 231 total_train_acc: 0.9306426332288401 loss: 0.6342106759548187\n",
      "epoch 232 total_train_acc: 0.9329937304075235 loss: 0.6474525183439255\n",
      "epoch 233 total_train_acc: 0.9357366771159875 loss: 0.6268043220043182\n",
      "epoch 234 total_train_acc: 0.9353448275862069 loss: 0.6078364998102188\n",
      "epoch 235 total_train_acc: 0.9349529780564263 loss: 0.6388196647167206\n",
      "epoch 236 total_train_acc: 0.9404388714733543 loss: 0.6148413270711899\n",
      "epoch 237 total_train_acc: 0.9353448275862069 loss: 0.6327527910470963\n",
      "epoch 238 total_train_acc: 0.9392633228840125 loss: 0.6174579411745071\n",
      "epoch 239 total_train_acc: 0.9341692789968652 loss: 0.6411499232053757\n",
      "epoch 240 total_train_acc: 0.9349529780564263 loss: 0.624802902340889\n",
      "epoch 241 total_train_acc: 0.9380877742946708 loss: 0.6090698391199112\n",
      "epoch 242 total_train_acc: 0.9388714733542319 loss: 0.6085501611232758\n",
      "epoch 243 total_train_acc: 0.9341692789968652 loss: 0.6314521580934525\n",
      "epoch 244 total_train_acc: 0.9349529780564263 loss: 0.5967650413513184\n",
      "epoch 245 total_train_acc: 0.9349529780564263 loss: 0.6112923473119736\n",
      "epoch 246 total_train_acc: 0.9396551724137931 loss: 0.6033688336610794\n",
      "epoch 247 total_train_acc: 0.9361285266457681 loss: 0.6004222929477692\n",
      "epoch 248 total_train_acc: 0.9349529780564263 loss: 0.6038133054971695\n",
      "epoch 249 total_train_acc: 0.9380877742946708 loss: 0.6046771854162216\n",
      "epoch 250 total_train_acc: 0.9392633228840125 loss: 0.600288137793541\n",
      "epoch 251 total_train_acc: 0.9369122257053292 loss: 0.5845424681901932\n",
      "epoch 252 total_train_acc: 0.9412225705329154 loss: 0.5926249027252197\n",
      "epoch 253 total_train_acc: 0.9427899686520376 loss: 0.5749059766530991\n",
      "epoch 254 total_train_acc: 0.9392633228840125 loss: 0.5933883786201477\n",
      "epoch 255 total_train_acc: 0.9396551724137931 loss: 0.5885760486125946\n",
      "epoch 256 total_train_acc: 0.9416144200626959 loss: 0.5779698044061661\n",
      "epoch 257 total_train_acc: 0.9412225705329154 loss: 0.5978803038597107\n",
      "epoch 258 total_train_acc: 0.942398119122257 loss: 0.5704854726791382\n",
      "epoch 259 total_train_acc: 0.9392633228840125 loss: 0.588407576084137\n",
      "epoch 260 total_train_acc: 0.9431818181818182 loss: 0.5844690799713135\n",
      "epoch 261 total_train_acc: 0.9365203761755486 loss: 0.584201991558075\n",
      "epoch 262 total_train_acc: 0.942398119122257 loss: 0.5473999381065369\n",
      "epoch 263 total_train_acc: 0.9384796238244514 loss: 0.5855935215950012\n",
      "epoch 264 total_train_acc: 0.9396551724137931 loss: 0.5595747977495193\n",
      "epoch 265 total_train_acc: 0.9420062695924765 loss: 0.5607527196407318\n",
      "epoch 266 total_train_acc: 0.9420062695924765 loss: 0.5675068944692612\n",
      "epoch 267 total_train_acc: 0.9388714733542319 loss: 0.5830379724502563\n",
      "epoch 268 total_train_acc: 0.9427899686520376 loss: 0.5876405984163284\n",
      "epoch 269 total_train_acc: 0.9396551724137931 loss: 0.5741360187530518\n",
      "epoch 270 total_train_acc: 0.9420062695924765 loss: 0.5542450994253159\n",
      "epoch 271 total_train_acc: 0.9400470219435737 loss: 0.5604000985622406\n",
      "epoch 272 total_train_acc: 0.9439655172413793 loss: 0.5634491443634033\n",
      "epoch 273 total_train_acc: 0.942398119122257 loss: 0.5558199435472488\n",
      "epoch 274 total_train_acc: 0.9435736677115988 loss: 0.571847602725029\n",
      "epoch 275 total_train_acc: 0.9439655172413793 loss: 0.5503133833408356\n",
      "epoch 276 total_train_acc: 0.942398119122257 loss: 0.5648223161697388\n",
      "epoch 277 total_train_acc: 0.9420062695924765 loss: 0.5687483251094818\n",
      "epoch 278 total_train_acc: 0.9431818181818182 loss: 0.5566645711660385\n",
      "epoch 279 total_train_acc: 0.9420062695924765 loss: 0.5543064028024673\n",
      "epoch 280 total_train_acc: 0.9455329153605015 loss: 0.5475072115659714\n",
      "epoch 281 total_train_acc: 0.9435736677115988 loss: 0.5450861901044846\n",
      "epoch 282 total_train_acc: 0.9443573667711599 loss: 0.5759825706481934\n",
      "epoch 283 total_train_acc: 0.9471003134796239 loss: 0.5271859616041183\n",
      "epoch 284 total_train_acc: 0.9455329153605015 loss: 0.5470423698425293\n",
      "epoch 285 total_train_acc: 0.9459247648902821 loss: 0.5352369695901871\n",
      "epoch 286 total_train_acc: 0.9431818181818182 loss: 0.528252124786377\n",
      "epoch 287 total_train_acc: 0.947884012539185 loss: 0.523911640048027\n",
      "epoch 288 total_train_acc: 0.9392633228840125 loss: 0.5407097637653351\n",
      "epoch 289 total_train_acc: 0.9416144200626959 loss: 0.5300902426242828\n",
      "epoch 290 total_train_acc: 0.945141065830721 loss: 0.5442610532045364\n",
      "epoch 291 total_train_acc: 0.9443573667711599 loss: 0.5346878319978714\n",
      "epoch 292 total_train_acc: 0.9474921630094044 loss: 0.5238141715526581\n",
      "epoch 293 total_train_acc: 0.9474921630094044 loss: 0.5159373879432678\n",
      "epoch 294 total_train_acc: 0.9435736677115988 loss: 0.548579752445221\n",
      "epoch 295 total_train_acc: 0.9431818181818182 loss: 0.5298522710800171\n",
      "epoch 296 total_train_acc: 0.9431818181818182 loss: 0.5300565510988235\n",
      "epoch 297 total_train_acc: 0.9510188087774295 loss: 0.49846358597278595\n",
      "epoch 298 total_train_acc: 0.9435736677115988 loss: 0.5190898329019547\n",
      "epoch 299 total_train_acc: 0.942398119122257 loss: 0.5432833284139633\n",
      "epoch 300 total_train_acc: 0.9455329153605015 loss: 0.5340507328510284\n",
      "epoch 301 total_train_acc: 0.9463166144200627 loss: 0.5083566457033157\n",
      "epoch 302 total_train_acc: 0.9459247648902821 loss: 0.49917837977409363\n",
      "epoch 303 total_train_acc: 0.9447492163009404 loss: 0.5127526074647903\n",
      "epoch 304 total_train_acc: 0.9490595611285266 loss: 0.5184479057788849\n",
      "epoch 305 total_train_acc: 0.9447492163009404 loss: 0.5301036238670349\n",
      "epoch 306 total_train_acc: 0.95141065830721 loss: 0.5008853524923325\n",
      "epoch 307 total_train_acc: 0.9486677115987461 loss: 0.48332926630973816\n",
      "epoch 308 total_train_acc: 0.9490595611285266 loss: 0.4987330138683319\n",
      "epoch 309 total_train_acc: 0.9455329153605015 loss: 0.5085353851318359\n",
      "epoch 310 total_train_acc: 0.9474921630094044 loss: 0.49752868711948395\n",
      "epoch 311 total_train_acc: 0.9463166144200627 loss: 0.493100568652153\n",
      "epoch 312 total_train_acc: 0.9467084639498433 loss: 0.49410194158554077\n",
      "epoch 313 total_train_acc: 0.9443573667711599 loss: 0.5018828809261322\n",
      "epoch 314 total_train_acc: 0.9474921630094044 loss: 0.4977851063013077\n",
      "epoch 315 total_train_acc: 0.9443573667711599 loss: 0.4897022247314453\n",
      "epoch 316 total_train_acc: 0.9471003134796239 loss: 0.49628520011901855\n",
      "epoch 317 total_train_acc: 0.9482758620689655 loss: 0.4804328978061676\n",
      "epoch 318 total_train_acc: 0.9490595611285266 loss: 0.4741312712430954\n",
      "epoch 319 total_train_acc: 0.9482758620689655 loss: 0.5131128281354904\n",
      "epoch 320 total_train_acc: 0.9471003134796239 loss: 0.4732968360185623\n",
      "epoch 321 total_train_acc: 0.9486677115987461 loss: 0.47554348409175873\n",
      "epoch 322 total_train_acc: 0.947884012539185 loss: 0.4851270467042923\n",
      "epoch 323 total_train_acc: 0.9494514106583072 loss: 0.4869969040155411\n",
      "epoch 324 total_train_acc: 0.9471003134796239 loss: 0.488518089056015\n",
      "epoch 325 total_train_acc: 0.9494514106583072 loss: 0.5034843236207962\n",
      "epoch 326 total_train_acc: 0.9510188087774295 loss: 0.463414803147316\n",
      "epoch 327 total_train_acc: 0.9510188087774295 loss: 0.46393050253391266\n",
      "epoch 328 total_train_acc: 0.9498432601880877 loss: 0.46589331328868866\n",
      "epoch 329 total_train_acc: 0.950626959247649 loss: 0.4788403809070587\n",
      "epoch 330 total_train_acc: 0.9510188087774295 loss: 0.47005604207515717\n",
      "epoch 331 total_train_acc: 0.950626959247649 loss: 0.46447785198688507\n",
      "epoch 332 total_train_acc: 0.95141065830721 loss: 0.4821547269821167\n",
      "epoch 333 total_train_acc: 0.9502351097178683 loss: 0.4745170325040817\n",
      "epoch 334 total_train_acc: 0.950626959247649 loss: 0.4666806310415268\n",
      "epoch 335 total_train_acc: 0.9498432601880877 loss: 0.4463293105363846\n",
      "epoch 336 total_train_acc: 0.9502351097178683 loss: 0.46324522793293\n",
      "epoch 337 total_train_acc: 0.9510188087774295 loss: 0.4785178005695343\n",
      "epoch 338 total_train_acc: 0.9498432601880877 loss: 0.4925338625907898\n",
      "epoch 339 total_train_acc: 0.9490595611285266 loss: 0.47266438603401184\n",
      "epoch 340 total_train_acc: 0.9498432601880877 loss: 0.47461727261543274\n",
      "epoch 341 total_train_acc: 0.9498432601880877 loss: 0.4640452563762665\n",
      "epoch 342 total_train_acc: 0.9553291536050157 loss: 0.4586358368396759\n",
      "epoch 343 total_train_acc: 0.9533699059561128 loss: 0.45438840985298157\n",
      "epoch 344 total_train_acc: 0.954153605015674 loss: 0.44574397802352905\n",
      "epoch 345 total_train_acc: 0.9494514106583072 loss: 0.4802573621273041\n",
      "epoch 346 total_train_acc: 0.9482758620689655 loss: 0.45482539385557175\n",
      "epoch 347 total_train_acc: 0.9482758620689655 loss: 0.45752178132534027\n",
      "epoch 348 total_train_acc: 0.9502351097178683 loss: 0.45730261504650116\n",
      "epoch 349 total_train_acc: 0.9521943573667712 loss: 0.45434001088142395\n",
      "epoch 350 total_train_acc: 0.9533699059561128 loss: 0.45151475071907043\n",
      "epoch 351 total_train_acc: 0.9533699059561128 loss: 0.4339696615934372\n",
      "epoch 352 total_train_acc: 0.9494514106583072 loss: 0.4438382312655449\n",
      "epoch 353 total_train_acc: 0.950626959247649 loss: 0.44575949013233185\n",
      "epoch 354 total_train_acc: 0.9471003134796239 loss: 0.47020570933818817\n",
      "epoch 355 total_train_acc: 0.9533699059561128 loss: 0.45997782051563263\n",
      "epoch 356 total_train_acc: 0.9498432601880877 loss: 0.4548761248588562\n",
      "epoch 357 total_train_acc: 0.9537617554858934 loss: 0.4519314169883728\n",
      "epoch 358 total_train_acc: 0.9549373040752351 loss: 0.432606004178524\n",
      "epoch 359 total_train_acc: 0.9510188087774295 loss: 0.4292505234479904\n",
      "epoch 360 total_train_acc: 0.9521943573667712 loss: 0.43567007035017014\n",
      "epoch 361 total_train_acc: 0.9533699059561128 loss: 0.4388474375009537\n",
      "epoch 362 total_train_acc: 0.9525862068965517 loss: 0.4499560743570328\n",
      "epoch 363 total_train_acc: 0.9533699059561128 loss: 0.4411594867706299\n",
      "epoch 364 total_train_acc: 0.9533699059561128 loss: 0.42714156210422516\n",
      "epoch 365 total_train_acc: 0.954153605015674 loss: 0.4435512125492096\n",
      "epoch 366 total_train_acc: 0.9561128526645768 loss: 0.4207741320133209\n",
      "epoch 367 total_train_acc: 0.9486677115987461 loss: 0.44278065860271454\n",
      "epoch 368 total_train_acc: 0.9529780564263323 loss: 0.43368081748485565\n",
      "epoch 369 total_train_acc: 0.9549373040752351 loss: 0.45139381289482117\n",
      "epoch 370 total_train_acc: 0.9545454545454546 loss: 0.4145790636539459\n",
      "epoch 371 total_train_acc: 0.9549373040752351 loss: 0.4392955005168915\n",
      "epoch 372 total_train_acc: 0.9545454545454546 loss: 0.427145391702652\n",
      "epoch 373 total_train_acc: 0.9521943573667712 loss: 0.41927503049373627\n",
      "epoch 374 total_train_acc: 0.9565047021943573 loss: 0.42228348553180695\n",
      "epoch 375 total_train_acc: 0.9561128526645768 loss: 0.4032786339521408\n",
      "epoch 376 total_train_acc: 0.9537617554858934 loss: 0.41601189225912094\n",
      "epoch 377 total_train_acc: 0.9580721003134797 loss: 0.4100424647331238\n",
      "epoch 378 total_train_acc: 0.9537617554858934 loss: 0.4208325147628784\n",
      "epoch 379 total_train_acc: 0.9553291536050157 loss: 0.4167241156101227\n",
      "epoch 380 total_train_acc: 0.95141065830721 loss: 0.4406633973121643\n",
      "epoch 381 total_train_acc: 0.9565047021943573 loss: 0.4066941663622856\n",
      "epoch 382 total_train_acc: 0.9533699059561128 loss: 0.4381370395421982\n",
      "epoch 383 total_train_acc: 0.9584639498432602 loss: 0.41023263335227966\n",
      "epoch 384 total_train_acc: 0.9549373040752351 loss: 0.4357507675886154\n",
      "epoch 385 total_train_acc: 0.9553291536050157 loss: 0.4232402592897415\n",
      "epoch 386 total_train_acc: 0.9553291536050157 loss: 0.4287143796682358\n",
      "epoch 387 total_train_acc: 0.9549373040752351 loss: 0.42555100470781326\n",
      "epoch 388 total_train_acc: 0.9549373040752351 loss: 0.40054742991924286\n",
      "epoch 389 total_train_acc: 0.9572884012539185 loss: 0.39710286259651184\n",
      "epoch 390 total_train_acc: 0.9549373040752351 loss: 0.38918041437864304\n",
      "epoch 391 total_train_acc: 0.9533699059561128 loss: 0.41009311378002167\n",
      "epoch 392 total_train_acc: 0.9549373040752351 loss: 0.41124600172042847\n",
      "epoch 393 total_train_acc: 0.957680250783699 loss: 0.40027159452438354\n",
      "epoch 394 total_train_acc: 0.9549373040752351 loss: 0.4060589149594307\n",
      "epoch 395 total_train_acc: 0.9561128526645768 loss: 0.37666500359773636\n",
      "epoch 396 total_train_acc: 0.9545454545454546 loss: 0.3986172303557396\n",
      "epoch 397 total_train_acc: 0.954153605015674 loss: 0.3908711373806\n",
      "epoch 398 total_train_acc: 0.9549373040752351 loss: 0.3914516568183899\n",
      "epoch 399 total_train_acc: 0.9565047021943573 loss: 0.391724668443203\n",
      "epoch 400 total_train_acc: 0.9584639498432602 loss: 0.3922003358602524\n",
      "epoch 401 total_train_acc: 0.9557210031347962 loss: 0.4043998718261719\n",
      "epoch 402 total_train_acc: 0.9612068965517241 loss: 0.375759482383728\n",
      "epoch 403 total_train_acc: 0.9561128526645768 loss: 0.4085247069597244\n",
      "epoch 404 total_train_acc: 0.9553291536050157 loss: 0.3797386735677719\n",
      "epoch 405 total_train_acc: 0.9568965517241379 loss: 0.3812360316514969\n",
      "epoch 406 total_train_acc: 0.9553291536050157 loss: 0.39467480778694153\n",
      "epoch 407 total_train_acc: 0.957680250783699 loss: 0.38726674765348434\n",
      "epoch 408 total_train_acc: 0.9568965517241379 loss: 0.38882141560316086\n",
      "epoch 409 total_train_acc: 0.9553291536050157 loss: 0.38905905932188034\n",
      "epoch 410 total_train_acc: 0.957680250783699 loss: 0.401934415102005\n",
      "epoch 411 total_train_acc: 0.9561128526645768 loss: 0.3855845257639885\n",
      "epoch 412 total_train_acc: 0.954153605015674 loss: 0.3972359746694565\n",
      "epoch 413 total_train_acc: 0.9572884012539185 loss: 0.3814692050218582\n",
      "epoch 414 total_train_acc: 0.9596394984326019 loss: 0.3820134699344635\n",
      "epoch 415 total_train_acc: 0.9584639498432602 loss: 0.3803751990199089\n",
      "epoch 416 total_train_acc: 0.9572884012539185 loss: 0.3758201003074646\n",
      "epoch 417 total_train_acc: 0.9588557993730408 loss: 0.38122061640024185\n",
      "epoch 418 total_train_acc: 0.957680250783699 loss: 0.3764287307858467\n",
      "epoch 419 total_train_acc: 0.9584639498432602 loss: 0.37911177426576614\n",
      "epoch 420 total_train_acc: 0.957680250783699 loss: 0.3927721157670021\n",
      "epoch 421 total_train_acc: 0.9557210031347962 loss: 0.3977043107151985\n",
      "epoch 422 total_train_acc: 0.9588557993730408 loss: 0.3813791051506996\n",
      "epoch 423 total_train_acc: 0.9553291536050157 loss: 0.38787417113780975\n",
      "epoch 424 total_train_acc: 0.9608150470219435 loss: 0.37833718955516815\n",
      "epoch 425 total_train_acc: 0.9565047021943573 loss: 0.37895622849464417\n",
      "epoch 426 total_train_acc: 0.9572884012539185 loss: 0.3700825944542885\n",
      "epoch 427 total_train_acc: 0.957680250783699 loss: 0.3742864429950714\n",
      "epoch 428 total_train_acc: 0.9608150470219435 loss: 0.38092654943466187\n",
      "epoch 429 total_train_acc: 0.9549373040752351 loss: 0.3860132396221161\n",
      "epoch 430 total_train_acc: 0.9600313479623824 loss: 0.3755810856819153\n",
      "epoch 431 total_train_acc: 0.9612068965517241 loss: 0.3670921102166176\n",
      "epoch 432 total_train_acc: 0.9588557993730408 loss: 0.3615943416953087\n",
      "epoch 433 total_train_acc: 0.957680250783699 loss: 0.37406712770462036\n",
      "epoch 434 total_train_acc: 0.9596394984326019 loss: 0.3651820421218872\n",
      "epoch 435 total_train_acc: 0.9584639498432602 loss: 0.37477031350135803\n",
      "epoch 436 total_train_acc: 0.9584639498432602 loss: 0.3689366355538368\n",
      "epoch 437 total_train_acc: 0.9596394984326019 loss: 0.3625679612159729\n",
      "epoch 438 total_train_acc: 0.9592476489028213 loss: 0.3567601144313812\n",
      "epoch 439 total_train_acc: 0.9553291536050157 loss: 0.36350269615650177\n",
      "epoch 440 total_train_acc: 0.9580721003134797 loss: 0.3601856380701065\n",
      "epoch 441 total_train_acc: 0.9568965517241379 loss: 0.3671944811940193\n",
      "epoch 442 total_train_acc: 0.9537617554858934 loss: 0.36769795417785645\n",
      "epoch 443 total_train_acc: 0.9592476489028213 loss: 0.36670247465372086\n",
      "epoch 444 total_train_acc: 0.9592476489028213 loss: 0.37794219702482224\n",
      "epoch 445 total_train_acc: 0.9588557993730408 loss: 0.35324084013700485\n",
      "epoch 446 total_train_acc: 0.9600313479623824 loss: 0.3702775463461876\n",
      "epoch 447 total_train_acc: 0.960423197492163 loss: 0.354041188955307\n",
      "epoch 448 total_train_acc: 0.9600313479623824 loss: 0.3398173898458481\n",
      "epoch 449 total_train_acc: 0.9592476489028213 loss: 0.3788933679461479\n",
      "epoch 450 total_train_acc: 0.9549373040752351 loss: 0.3786236345767975\n",
      "epoch 451 total_train_acc: 0.9561128526645768 loss: 0.3627941235899925\n",
      "epoch 452 total_train_acc: 0.9600313479623824 loss: 0.3562140092253685\n",
      "epoch 453 total_train_acc: 0.9623824451410659 loss: 0.3468497171998024\n",
      "epoch 454 total_train_acc: 0.9600313479623824 loss: 0.3677368462085724\n",
      "epoch 455 total_train_acc: 0.9596394984326019 loss: 0.3845863789319992\n",
      "epoch 456 total_train_acc: 0.9545454545454546 loss: 0.38390397280454636\n",
      "epoch 457 total_train_acc: 0.963166144200627 loss: 0.35340873897075653\n",
      "epoch 458 total_train_acc: 0.9619905956112853 loss: 0.35711102932691574\n",
      "epoch 459 total_train_acc: 0.9612068965517241 loss: 0.36351272463798523\n",
      "epoch 460 total_train_acc: 0.9627742946708464 loss: 0.34750498831272125\n",
      "epoch 461 total_train_acc: 0.9600313479623824 loss: 0.357090063393116\n",
      "epoch 462 total_train_acc: 0.9584639498432602 loss: 0.35492824018001556\n",
      "epoch 463 total_train_acc: 0.9600313479623824 loss: 0.37190961092710495\n",
      "epoch 464 total_train_acc: 0.963166144200627 loss: 0.34789255261421204\n",
      "epoch 465 total_train_acc: 0.9588557993730408 loss: 0.352313794195652\n",
      "epoch 466 total_train_acc: 0.9627742946708464 loss: 0.3495577722787857\n",
      "epoch 467 total_train_acc: 0.9596394984326019 loss: 0.3716965764760971\n",
      "epoch 468 total_train_acc: 0.9600313479623824 loss: 0.3605050817131996\n",
      "epoch 469 total_train_acc: 0.9592476489028213 loss: 0.3541404455900192\n",
      "epoch 470 total_train_acc: 0.9608150470219435 loss: 0.3536844775080681\n",
      "epoch 471 total_train_acc: 0.957680250783699 loss: 0.3627985045313835\n",
      "epoch 472 total_train_acc: 0.9608150470219435 loss: 0.3514440506696701\n",
      "epoch 473 total_train_acc: 0.9568965517241379 loss: 0.36866211146116257\n",
      "epoch 474 total_train_acc: 0.9639498432601881 loss: 0.3519761860370636\n",
      "epoch 475 total_train_acc: 0.9619905956112853 loss: 0.35486675053834915\n",
      "epoch 476 total_train_acc: 0.9615987460815048 loss: 0.3574826493859291\n",
      "epoch 477 total_train_acc: 0.9584639498432602 loss: 0.3524651378393173\n",
      "epoch 478 total_train_acc: 0.9600313479623824 loss: 0.3551502972841263\n",
      "epoch 479 total_train_acc: 0.9608150470219435 loss: 0.34125930070877075\n",
      "epoch 480 total_train_acc: 0.9635579937304075 loss: 0.34604326635599136\n",
      "epoch 481 total_train_acc: 0.9588557993730408 loss: 0.35985638201236725\n",
      "epoch 482 total_train_acc: 0.9639498432601881 loss: 0.336374007165432\n",
      "epoch 483 total_train_acc: 0.9615987460815048 loss: 0.35181616246700287\n",
      "epoch 484 total_train_acc: 0.9623824451410659 loss: 0.3377038687467575\n",
      "epoch 485 total_train_acc: 0.9619905956112853 loss: 0.33655162900686264\n",
      "epoch 486 total_train_acc: 0.9619905956112853 loss: 0.35272907465696335\n",
      "epoch 487 total_train_acc: 0.9580721003134797 loss: 0.3533180430531502\n",
      "epoch 488 total_train_acc: 0.9592476489028213 loss: 0.3448864370584488\n",
      "epoch 489 total_train_acc: 0.9627742946708464 loss: 0.3424925431609154\n",
      "epoch 490 total_train_acc: 0.9639498432601881 loss: 0.35581281036138535\n",
      "epoch 491 total_train_acc: 0.966692789968652 loss: 0.3397258371114731\n",
      "epoch 492 total_train_acc: 0.9608150470219435 loss: 0.35609790682792664\n",
      "epoch 493 total_train_acc: 0.9612068965517241 loss: 0.35048140585422516\n",
      "epoch 494 total_train_acc: 0.9568965517241379 loss: 0.3524593263864517\n",
      "epoch 495 total_train_acc: 0.9565047021943573 loss: 0.3603096678853035\n",
      "epoch 496 total_train_acc: 0.9635579937304075 loss: 0.3439823016524315\n",
      "epoch 497 total_train_acc: 0.9600313479623824 loss: 0.3564179614186287\n",
      "epoch 498 total_train_acc: 0.963166144200627 loss: 0.34347639232873917\n",
      "epoch 499 total_train_acc: 0.963166144200627 loss: 0.3422895073890686\n",
      "epoch 500 total_train_acc: 0.9600313479623824 loss: 0.343873493373394\n",
      "epoch 501 total_train_acc: 0.9619905956112853 loss: 0.34155308455228806\n",
      "epoch 502 total_train_acc: 0.9612068965517241 loss: 0.3508966341614723\n",
      "epoch 503 total_train_acc: 0.9651253918495298 loss: 0.3562226891517639\n",
      "epoch 504 total_train_acc: 0.9655172413793104 loss: 0.31779251992702484\n",
      "epoch 505 total_train_acc: 0.9627742946708464 loss: 0.33426453173160553\n",
      "epoch 506 total_train_acc: 0.9600313479623824 loss: 0.3321782276034355\n",
      "epoch 507 total_train_acc: 0.9623824451410659 loss: 0.3363490402698517\n",
      "epoch 508 total_train_acc: 0.9643416927899686 loss: 0.34283488243818283\n",
      "epoch 509 total_train_acc: 0.957680250783699 loss: 0.3438452035188675\n",
      "epoch 510 total_train_acc: 0.9600313479623824 loss: 0.33604683727025986\n",
      "epoch 511 total_train_acc: 0.9623824451410659 loss: 0.3376355394721031\n",
      "epoch 512 total_train_acc: 0.9588557993730408 loss: 0.3365773856639862\n",
      "epoch 513 total_train_acc: 0.960423197492163 loss: 0.3328865170478821\n",
      "epoch 514 total_train_acc: 0.9627742946708464 loss: 0.33801204711198807\n",
      "epoch 515 total_train_acc: 0.9655172413793104 loss: 0.33422695100307465\n",
      "epoch 516 total_train_acc: 0.9635579937304075 loss: 0.34236734360456467\n",
      "epoch 517 total_train_acc: 0.9639498432601881 loss: 0.3283289670944214\n",
      "epoch 518 total_train_acc: 0.9615987460815048 loss: 0.3328845724463463\n",
      "epoch 519 total_train_acc: 0.9659090909090909 loss: 0.3174813687801361\n",
      "epoch 520 total_train_acc: 0.9608150470219435 loss: 0.3436120077967644\n",
      "epoch 521 total_train_acc: 0.9655172413793104 loss: 0.32001274824142456\n",
      "epoch 522 total_train_acc: 0.9584639498432602 loss: 0.3386148661375046\n",
      "epoch 523 total_train_acc: 0.960423197492163 loss: 0.3226587027311325\n",
      "epoch 524 total_train_acc: 0.960423197492163 loss: 0.3316943794488907\n",
      "epoch 525 total_train_acc: 0.9619905956112853 loss: 0.32915136963129044\n",
      "epoch 526 total_train_acc: 0.9635579937304075 loss: 0.3403829410672188\n",
      "epoch 527 total_train_acc: 0.9659090909090909 loss: 0.3309765085577965\n",
      "epoch 528 total_train_acc: 0.963166144200627 loss: 0.3433765694499016\n",
      "epoch 529 total_train_acc: 0.9635579937304075 loss: 0.3332126885652542\n",
      "epoch 530 total_train_acc: 0.963166144200627 loss: 0.3290637731552124\n",
      "epoch 531 total_train_acc: 0.963166144200627 loss: 0.3216404840350151\n",
      "epoch 532 total_train_acc: 0.9623824451410659 loss: 0.32091785967350006\n",
      "epoch 533 total_train_acc: 0.9639498432601881 loss: 0.3140317499637604\n",
      "epoch 534 total_train_acc: 0.9639498432601881 loss: 0.31453292071819305\n",
      "epoch 535 total_train_acc: 0.9623824451410659 loss: 0.33297496289014816\n",
      "epoch 536 total_train_acc: 0.963166144200627 loss: 0.323449470102787\n",
      "epoch 537 total_train_acc: 0.9651253918495298 loss: 0.3298436254262924\n",
      "epoch 538 total_train_acc: 0.9619905956112853 loss: 0.3199931010603905\n",
      "epoch 539 total_train_acc: 0.9647335423197492 loss: 0.32167859375476837\n",
      "epoch 540 total_train_acc: 0.9639498432601881 loss: 0.3297904282808304\n",
      "epoch 541 total_train_acc: 0.9647335423197492 loss: 0.31295306980609894\n",
      "epoch 542 total_train_acc: 0.9623824451410659 loss: 0.3182326555252075\n",
      "epoch 543 total_train_acc: 0.963166144200627 loss: 0.31875061243772507\n",
      "epoch 544 total_train_acc: 0.963166144200627 loss: 0.31764788180589676\n",
      "epoch 545 total_train_acc: 0.9608150470219435 loss: 0.3257591351866722\n",
      "epoch 546 total_train_acc: 0.9651253918495298 loss: 0.3221021816134453\n",
      "epoch 547 total_train_acc: 0.9612068965517241 loss: 0.3313231021165848\n",
      "epoch 548 total_train_acc: 0.9655172413793104 loss: 0.3100721538066864\n",
      "epoch 549 total_train_acc: 0.9623824451410659 loss: 0.3230622932314873\n",
      "epoch 550 total_train_acc: 0.9612068965517241 loss: 0.325017511844635\n",
      "epoch 551 total_train_acc: 0.9643416927899686 loss: 0.33461401611566544\n",
      "epoch 552 total_train_acc: 0.9635579937304075 loss: 0.3234100490808487\n",
      "epoch 553 total_train_acc: 0.9659090909090909 loss: 0.2981666177511215\n",
      "epoch 554 total_train_acc: 0.9635579937304075 loss: 0.3401111960411072\n",
      "epoch 555 total_train_acc: 0.9639498432601881 loss: 0.3149355500936508\n",
      "epoch 556 total_train_acc: 0.9635579937304075 loss: 0.3155770003795624\n",
      "epoch 557 total_train_acc: 0.9623824451410659 loss: 0.31440558284521103\n",
      "epoch 558 total_train_acc: 0.9647335423197492 loss: 0.3177390769124031\n",
      "epoch 559 total_train_acc: 0.9619905956112853 loss: 0.31496965140104294\n",
      "epoch 560 total_train_acc: 0.9651253918495298 loss: 0.2973043918609619\n",
      "epoch 561 total_train_acc: 0.9596394984326019 loss: 0.3267544284462929\n",
      "epoch 562 total_train_acc: 0.9655172413793104 loss: 0.30619435757398605\n",
      "epoch 563 total_train_acc: 0.963166144200627 loss: 0.31214556843042374\n",
      "epoch 564 total_train_acc: 0.9639498432601881 loss: 0.3297497555613518\n",
      "epoch 565 total_train_acc: 0.9663009404388715 loss: 0.30708062648773193\n",
      "epoch 566 total_train_acc: 0.9596394984326019 loss: 0.3123038113117218\n",
      "epoch 567 total_train_acc: 0.9655172413793104 loss: 0.2986367866396904\n",
      "epoch 568 total_train_acc: 0.9663009404388715 loss: 0.3078206554055214\n",
      "epoch 569 total_train_acc: 0.9655172413793104 loss: 0.31352438032627106\n",
      "epoch 570 total_train_acc: 0.9663009404388715 loss: 0.30429454892873764\n",
      "epoch 571 total_train_acc: 0.9651253918495298 loss: 0.2938631549477577\n",
      "epoch 572 total_train_acc: 0.9639498432601881 loss: 0.3068044036626816\n",
      "epoch 573 total_train_acc: 0.9627742946708464 loss: 0.3282368779182434\n",
      "epoch 574 total_train_acc: 0.9663009404388715 loss: 0.311181977391243\n",
      "epoch 575 total_train_acc: 0.9670846394984326 loss: 0.31365737318992615\n",
      "epoch 576 total_train_acc: 0.9682601880877743 loss: 0.3092443197965622\n",
      "epoch 577 total_train_acc: 0.9674764890282131 loss: 0.31988023966550827\n",
      "epoch 578 total_train_acc: 0.963166144200627 loss: 0.31642141193151474\n",
      "epoch 579 total_train_acc: 0.9670846394984326 loss: 0.3058617636561394\n",
      "epoch 580 total_train_acc: 0.963166144200627 loss: 0.3013767600059509\n",
      "epoch 581 total_train_acc: 0.9670846394984326 loss: 0.30322547256946564\n",
      "epoch 582 total_train_acc: 0.9651253918495298 loss: 0.3218580186367035\n",
      "epoch 583 total_train_acc: 0.9643416927899686 loss: 0.291979655623436\n",
      "epoch 584 total_train_acc: 0.9639498432601881 loss: 0.3169219493865967\n",
      "epoch 585 total_train_acc: 0.9655172413793104 loss: 0.3178274780511856\n",
      "epoch 586 total_train_acc: 0.9643416927899686 loss: 0.31153735518455505\n",
      "epoch 587 total_train_acc: 0.9643416927899686 loss: 0.29716144502162933\n",
      "epoch 588 total_train_acc: 0.9635579937304075 loss: 0.2970499247312546\n",
      "epoch 589 total_train_acc: 0.9635579937304075 loss: 0.3229820728302002\n",
      "epoch 590 total_train_acc: 0.9659090909090909 loss: 0.30695345997810364\n",
      "epoch 591 total_train_acc: 0.9623824451410659 loss: 0.3056683912873268\n",
      "epoch 592 total_train_acc: 0.9663009404388715 loss: 0.29912614822387695\n",
      "epoch 593 total_train_acc: 0.966692789968652 loss: 0.3024371564388275\n",
      "epoch 594 total_train_acc: 0.9678683385579937 loss: 0.28972741961479187\n",
      "epoch 595 total_train_acc: 0.9647335423197492 loss: 0.2918838635087013\n",
      "epoch 596 total_train_acc: 0.963166144200627 loss: 0.2983252629637718\n",
      "epoch 597 total_train_acc: 0.9670846394984326 loss: 0.31244949996471405\n",
      "epoch 598 total_train_acc: 0.9686520376175548 loss: 0.29943764209747314\n",
      "epoch 599 total_train_acc: 0.9647335423197492 loss: 0.2997954487800598\n",
      "epoch 600 total_train_acc: 0.9663009404388715 loss: 0.3031217232346535\n",
      "epoch 601 total_train_acc: 0.9663009404388715 loss: 0.31297827512025833\n",
      "epoch 602 total_train_acc: 0.9655172413793104 loss: 0.3102688118815422\n",
      "epoch 603 total_train_acc: 0.9651253918495298 loss: 0.294018991291523\n",
      "epoch 604 total_train_acc: 0.966692789968652 loss: 0.3061622604727745\n",
      "epoch 605 total_train_acc: 0.9725705329153606 loss: 0.2794850096106529\n",
      "epoch 606 total_train_acc: 0.9663009404388715 loss: 0.2957952097058296\n",
      "epoch 607 total_train_acc: 0.966692789968652 loss: 0.2944531738758087\n",
      "epoch 608 total_train_acc: 0.9655172413793104 loss: 0.29496873170137405\n",
      "epoch 609 total_train_acc: 0.9635579937304075 loss: 0.2913465052843094\n",
      "epoch 610 total_train_acc: 0.966692789968652 loss: 0.2909035086631775\n",
      "epoch 611 total_train_acc: 0.9659090909090909 loss: 0.3101678937673569\n",
      "epoch 612 total_train_acc: 0.9686520376175548 loss: 0.29167159646749496\n",
      "epoch 613 total_train_acc: 0.966692789968652 loss: 0.2967577800154686\n",
      "epoch 614 total_train_acc: 0.9686520376175548 loss: 0.30307842046022415\n",
      "epoch 615 total_train_acc: 0.9651253918495298 loss: 0.29722852259874344\n",
      "epoch 616 total_train_acc: 0.9663009404388715 loss: 0.2842155024409294\n",
      "epoch 617 total_train_acc: 0.969435736677116 loss: 0.29362812638282776\n",
      "epoch 618 total_train_acc: 0.9647335423197492 loss: 0.3079591616988182\n",
      "epoch 619 total_train_acc: 0.9670846394984326 loss: 0.2899787575006485\n",
      "epoch 620 total_train_acc: 0.9643416927899686 loss: 0.2930637523531914\n",
      "epoch 621 total_train_acc: 0.9670846394984326 loss: 0.2952372878789902\n",
      "epoch 622 total_train_acc: 0.9682601880877743 loss: 0.2830987051129341\n",
      "epoch 623 total_train_acc: 0.9674764890282131 loss: 0.2751169055700302\n",
      "epoch 624 total_train_acc: 0.9643416927899686 loss: 0.2963252067565918\n",
      "epoch 625 total_train_acc: 0.9698275862068966 loss: 0.2694370821118355\n",
      "epoch 626 total_train_acc: 0.9659090909090909 loss: 0.2924991622567177\n",
      "epoch 627 total_train_acc: 0.9670846394984326 loss: 0.2819368839263916\n",
      "epoch 628 total_train_acc: 0.9655172413793104 loss: 0.28260689973831177\n",
      "epoch 629 total_train_acc: 0.9655172413793104 loss: 0.2794836610555649\n",
      "epoch 630 total_train_acc: 0.9663009404388715 loss: 0.2831893563270569\n",
      "epoch 631 total_train_acc: 0.9670846394984326 loss: 0.2813548892736435\n",
      "epoch 632 total_train_acc: 0.9663009404388715 loss: 0.28593332320451736\n",
      "epoch 633 total_train_acc: 0.9678683385579937 loss: 0.2891678735613823\n",
      "epoch 634 total_train_acc: 0.969435736677116 loss: 0.27560438960790634\n",
      "epoch 635 total_train_acc: 0.9674764890282131 loss: 0.2750357985496521\n",
      "epoch 636 total_train_acc: 0.9670846394984326 loss: 0.28646261990070343\n",
      "epoch 637 total_train_acc: 0.969435736677116 loss: 0.2705741748213768\n",
      "epoch 638 total_train_acc: 0.9706112852664577 loss: 0.2845807895064354\n",
      "epoch 639 total_train_acc: 0.9655172413793104 loss: 0.3016817942261696\n",
      "epoch 640 total_train_acc: 0.9674764890282131 loss: 0.28283753991127014\n",
      "epoch 641 total_train_acc: 0.9682601880877743 loss: 0.2999897375702858\n",
      "epoch 642 total_train_acc: 0.966692789968652 loss: 0.2900907173752785\n",
      "epoch 643 total_train_acc: 0.9690438871473355 loss: 0.2951897457242012\n",
      "epoch 644 total_train_acc: 0.9670846394984326 loss: 0.2866579443216324\n",
      "epoch 645 total_train_acc: 0.9635579937304075 loss: 0.27171628177165985\n",
      "epoch 646 total_train_acc: 0.9702194357366771 loss: 0.2757707014679909\n",
      "epoch 647 total_train_acc: 0.9698275862068966 loss: 0.27802764624357224\n",
      "epoch 648 total_train_acc: 0.9690438871473355 loss: 0.2754634693264961\n",
      "epoch 649 total_train_acc: 0.9682601880877743 loss: 0.2769719809293747\n",
      "epoch 650 total_train_acc: 0.9663009404388715 loss: 0.2853301838040352\n",
      "epoch 651 total_train_acc: 0.9678683385579937 loss: 0.2777269780635834\n",
      "epoch 652 total_train_acc: 0.9686520376175548 loss: 0.2709462642669678\n",
      "epoch 653 total_train_acc: 0.9686520376175548 loss: 0.28667712211608887\n",
      "epoch 654 total_train_acc: 0.9686520376175548 loss: 0.2860960215330124\n",
      "epoch 655 total_train_acc: 0.9643416927899686 loss: 0.2924903854727745\n",
      "epoch 656 total_train_acc: 0.9729623824451411 loss: 0.26258906722068787\n",
      "epoch 657 total_train_acc: 0.9670846394984326 loss: 0.2750225067138672\n",
      "epoch 658 total_train_acc: 0.9682601880877743 loss: 0.29037612676620483\n",
      "epoch 659 total_train_acc: 0.9670846394984326 loss: 0.28948095440864563\n",
      "epoch 660 total_train_acc: 0.969435736677116 loss: 0.2773265317082405\n",
      "epoch 661 total_train_acc: 0.9678683385579937 loss: 0.2935059443116188\n",
      "epoch 662 total_train_acc: 0.9690438871473355 loss: 0.28381673246622086\n",
      "epoch 663 total_train_acc: 0.9663009404388715 loss: 0.28289616107940674\n",
      "epoch 664 total_train_acc: 0.9702194357366771 loss: 0.2629626616835594\n",
      "epoch 665 total_train_acc: 0.963166144200627 loss: 0.29585645347833633\n",
      "epoch 666 total_train_acc: 0.9627742946708464 loss: 0.2823541536927223\n",
      "epoch 667 total_train_acc: 0.9686520376175548 loss: 0.2811252102255821\n",
      "epoch 668 total_train_acc: 0.9706112852664577 loss: 0.2665049135684967\n",
      "epoch 669 total_train_acc: 0.9670846394984326 loss: 0.2761562317609787\n",
      "epoch 670 total_train_acc: 0.969435736677116 loss: 0.26222892105579376\n",
      "epoch 671 total_train_acc: 0.9690438871473355 loss: 0.25129280984401703\n",
      "epoch 672 total_train_acc: 0.9678683385579937 loss: 0.272409163415432\n",
      "epoch 673 total_train_acc: 0.9670846394984326 loss: 0.2739346772432327\n",
      "epoch 674 total_train_acc: 0.9686520376175548 loss: 0.2735287547111511\n",
      "epoch 675 total_train_acc: 0.9702194357366771 loss: 0.276577889919281\n",
      "epoch 676 total_train_acc: 0.9690438871473355 loss: 0.2670794203877449\n",
      "epoch 677 total_train_acc: 0.9706112852664577 loss: 0.27272865176200867\n",
      "epoch 678 total_train_acc: 0.9678683385579937 loss: 0.2758556827902794\n",
      "epoch 679 total_train_acc: 0.9702194357366771 loss: 0.27497101575136185\n",
      "epoch 680 total_train_acc: 0.9702194357366771 loss: 0.2531230300664902\n",
      "epoch 681 total_train_acc: 0.9721786833855799 loss: 0.2713394835591316\n",
      "epoch 682 total_train_acc: 0.969435736677116 loss: 0.26473449170589447\n",
      "epoch 683 total_train_acc: 0.9686520376175548 loss: 0.2699151858687401\n",
      "epoch 684 total_train_acc: 0.9717868338557993 loss: 0.2568930685520172\n",
      "epoch 685 total_train_acc: 0.9670846394984326 loss: 0.2677777335047722\n",
      "epoch 686 total_train_acc: 0.9698275862068966 loss: 0.26543329656124115\n",
      "epoch 687 total_train_acc: 0.966692789968652 loss: 0.27705157548189163\n",
      "epoch 688 total_train_acc: 0.9698275862068966 loss: 0.265581451356411\n",
      "epoch 689 total_train_acc: 0.969435736677116 loss: 0.2545994073152542\n",
      "epoch 690 total_train_acc: 0.969435736677116 loss: 0.2709193602204323\n",
      "epoch 691 total_train_acc: 0.9674764890282131 loss: 0.2640095576643944\n",
      "epoch 692 total_train_acc: 0.9713949843260188 loss: 0.2695784941315651\n",
      "epoch 693 total_train_acc: 0.9710031347962382 loss: 0.25811323523521423\n",
      "epoch 694 total_train_acc: 0.9698275862068966 loss: 0.25395599007606506\n",
      "epoch 695 total_train_acc: 0.9706112852664577 loss: 0.26485923677682877\n",
      "epoch 696 total_train_acc: 0.9713949843260188 loss: 0.26484374701976776\n",
      "epoch 697 total_train_acc: 0.9721786833855799 loss: 0.2639112323522568\n",
      "epoch 698 total_train_acc: 0.9702194357366771 loss: 0.27752798795700073\n",
      "epoch 699 total_train_acc: 0.9698275862068966 loss: 0.2647954002022743\n",
      "epoch 700 total_train_acc: 0.9698275862068966 loss: 0.2668909355998039\n",
      "epoch 701 total_train_acc: 0.9733542319749217 loss: 0.2556004300713539\n",
      "epoch 702 total_train_acc: 0.9698275862068966 loss: 0.2661035433411598\n",
      "epoch 703 total_train_acc: 0.9698275862068966 loss: 0.27026279270648956\n",
      "epoch 704 total_train_acc: 0.9686520376175548 loss: 0.26089566200971603\n",
      "epoch 705 total_train_acc: 0.9706112852664577 loss: 0.24852530285716057\n",
      "epoch 706 total_train_acc: 0.9682601880877743 loss: 0.27432841062545776\n",
      "epoch 707 total_train_acc: 0.9721786833855799 loss: 0.2564496248960495\n",
      "epoch 708 total_train_acc: 0.9717868338557993 loss: 0.26218151301145554\n",
      "epoch 709 total_train_acc: 0.9674764890282131 loss: 0.25678054988384247\n",
      "epoch 710 total_train_acc: 0.9698275862068966 loss: 0.2570323795080185\n",
      "epoch 711 total_train_acc: 0.9682601880877743 loss: 0.26163728535175323\n",
      "epoch 712 total_train_acc: 0.9686520376175548 loss: 0.26444973796606064\n",
      "epoch 713 total_train_acc: 0.9698275862068966 loss: 0.2837264686822891\n",
      "epoch 714 total_train_acc: 0.9733542319749217 loss: 0.24936073273420334\n",
      "epoch 715 total_train_acc: 0.969435736677116 loss: 0.26665760576725006\n",
      "epoch 716 total_train_acc: 0.9710031347962382 loss: 0.25357140600681305\n",
      "epoch 717 total_train_acc: 0.9713949843260188 loss: 0.24852567166090012\n",
      "epoch 718 total_train_acc: 0.9706112852664577 loss: 0.2635514363646507\n",
      "epoch 719 total_train_acc: 0.9717868338557993 loss: 0.28014785796403885\n",
      "epoch 720 total_train_acc: 0.9741379310344828 loss: 0.2585349828004837\n",
      "epoch 721 total_train_acc: 0.9698275862068966 loss: 0.2511250376701355\n",
      "epoch 722 total_train_acc: 0.9710031347962382 loss: 0.259120337665081\n",
      "epoch 723 total_train_acc: 0.9710031347962382 loss: 0.2521701529622078\n",
      "epoch 724 total_train_acc: 0.9682601880877743 loss: 0.25346604734659195\n",
      "epoch 725 total_train_acc: 0.9702194357366771 loss: 0.2490081638097763\n",
      "epoch 726 total_train_acc: 0.9670846394984326 loss: 0.26561378687620163\n",
      "epoch 727 total_train_acc: 0.9733542319749217 loss: 0.25219809263944626\n",
      "epoch 728 total_train_acc: 0.9710031347962382 loss: 0.25303731113672256\n",
      "epoch 729 total_train_acc: 0.9710031347962382 loss: 0.25006409734487534\n",
      "epoch 730 total_train_acc: 0.9682601880877743 loss: 0.2639852315187454\n",
      "epoch 731 total_train_acc: 0.9686520376175548 loss: 0.24981879442930222\n",
      "epoch 732 total_train_acc: 0.969435736677116 loss: 0.24394553154706955\n",
      "epoch 733 total_train_acc: 0.9710031347962382 loss: 0.2579569071531296\n",
      "epoch 734 total_train_acc: 0.9725705329153606 loss: 0.24511823803186417\n",
      "epoch 735 total_train_acc: 0.9725705329153606 loss: 0.2480827495455742\n",
      "epoch 736 total_train_acc: 0.9717868338557993 loss: 0.2630036175251007\n",
      "epoch 737 total_train_acc: 0.9674764890282131 loss: 0.24237190932035446\n",
      "epoch 738 total_train_acc: 0.9737460815047022 loss: 0.24485155940055847\n",
      "epoch 739 total_train_acc: 0.9749216300940439 loss: 0.2576363682746887\n",
      "epoch 740 total_train_acc: 0.969435736677116 loss: 0.2634762078523636\n",
      "epoch 741 total_train_acc: 0.9674764890282131 loss: 0.2704539969563484\n",
      "epoch 742 total_train_acc: 0.9713949843260188 loss: 0.2553395926952362\n",
      "epoch 743 total_train_acc: 0.9702194357366771 loss: 0.26300039142370224\n",
      "epoch 744 total_train_acc: 0.966692789968652 loss: 0.25967057794332504\n",
      "epoch 745 total_train_acc: 0.9741379310344828 loss: 0.24696076661348343\n",
      "epoch 746 total_train_acc: 0.9717868338557993 loss: 0.2564588412642479\n",
      "epoch 747 total_train_acc: 0.9725705329153606 loss: 0.25066056102514267\n",
      "epoch 748 total_train_acc: 0.9745297805642633 loss: 0.23492567986249924\n",
      "epoch 749 total_train_acc: 0.9686520376175548 loss: 0.24563030898571014\n",
      "epoch 750 total_train_acc: 0.969435736677116 loss: 0.2464704066514969\n",
      "epoch 751 total_train_acc: 0.9725705329153606 loss: 0.2306956723332405\n",
      "epoch 752 total_train_acc: 0.9690438871473355 loss: 0.27076049894094467\n",
      "epoch 753 total_train_acc: 0.9733542319749217 loss: 0.24537763744592667\n",
      "epoch 754 total_train_acc: 0.969435736677116 loss: 0.24933823943138123\n",
      "epoch 755 total_train_acc: 0.9725705329153606 loss: 0.2407708317041397\n",
      "epoch 756 total_train_acc: 0.9725705329153606 loss: 0.23979608714580536\n",
      "epoch 757 total_train_acc: 0.9729623824451411 loss: 0.2494354471564293\n",
      "epoch 758 total_train_acc: 0.9729623824451411 loss: 0.23288489878177643\n",
      "epoch 759 total_train_acc: 0.9702194357366771 loss: 0.24336481094360352\n",
      "epoch 760 total_train_acc: 0.9776645768025078 loss: 0.2313871681690216\n",
      "epoch 761 total_train_acc: 0.9710031347962382 loss: 0.24779976159334183\n",
      "epoch 762 total_train_acc: 0.9717868338557993 loss: 0.2425611987709999\n",
      "epoch 763 total_train_acc: 0.9713949843260188 loss: 0.23436283320188522\n",
      "epoch 764 total_train_acc: 0.9729623824451411 loss: 0.24409326910972595\n",
      "epoch 765 total_train_acc: 0.9710031347962382 loss: 0.23645488917827606\n",
      "epoch 766 total_train_acc: 0.9725705329153606 loss: 0.24932098388671875\n",
      "epoch 767 total_train_acc: 0.9706112852664577 loss: 0.23760122060775757\n",
      "epoch 768 total_train_acc: 0.9713949843260188 loss: 0.24123358726501465\n",
      "epoch 769 total_train_acc: 0.9702194357366771 loss: 0.24477168917655945\n",
      "epoch 770 total_train_acc: 0.9741379310344828 loss: 0.236714418977499\n",
      "epoch 771 total_train_acc: 0.9745297805642633 loss: 0.221781924366951\n",
      "epoch 772 total_train_acc: 0.9729623824451411 loss: 0.23880375921726227\n",
      "epoch 773 total_train_acc: 0.9729623824451411 loss: 0.23935019969940186\n",
      "epoch 774 total_train_acc: 0.975705329153605 loss: 0.23840225487947464\n",
      "epoch 775 total_train_acc: 0.9760971786833855 loss: 0.23357831686735153\n",
      "epoch 776 total_train_acc: 0.9710031347962382 loss: 0.23447776585817337\n",
      "epoch 777 total_train_acc: 0.9741379310344828 loss: 0.23098872601985931\n",
      "epoch 778 total_train_acc: 0.9686520376175548 loss: 0.24313533306121826\n",
      "epoch 779 total_train_acc: 0.9725705329153606 loss: 0.24646621942520142\n",
      "epoch 780 total_train_acc: 0.9713949843260188 loss: 0.2339731901884079\n",
      "epoch 781 total_train_acc: 0.9749216300940439 loss: 0.23227564990520477\n",
      "epoch 782 total_train_acc: 0.9698275862068966 loss: 0.2642787843942642\n",
      "epoch 783 total_train_acc: 0.975705329153605 loss: 0.21647946536540985\n",
      "epoch 784 total_train_acc: 0.9702194357366771 loss: 0.25131843239068985\n",
      "epoch 785 total_train_acc: 0.969435736677116 loss: 0.2400965541601181\n",
      "epoch 786 total_train_acc: 0.9713949843260188 loss: 0.24261249601840973\n",
      "epoch 787 total_train_acc: 0.9698275862068966 loss: 0.2552121728658676\n",
      "epoch 788 total_train_acc: 0.9729623824451411 loss: 0.23472055792808533\n",
      "epoch 789 total_train_acc: 0.9733542319749217 loss: 0.2426946684718132\n",
      "epoch 790 total_train_acc: 0.9710031347962382 loss: 0.2231394574046135\n",
      "epoch 791 total_train_acc: 0.975705329153605 loss: 0.23514515161514282\n",
      "epoch 792 total_train_acc: 0.9741379310344828 loss: 0.23214714229106903\n",
      "epoch 793 total_train_acc: 0.9768808777429467 loss: 0.24846404790878296\n",
      "epoch 794 total_train_acc: 0.9768808777429467 loss: 0.23091955482959747\n",
      "epoch 795 total_train_acc: 0.9737460815047022 loss: 0.23317024111747742\n",
      "epoch 796 total_train_acc: 0.9706112852664577 loss: 0.23978643864393234\n",
      "epoch 797 total_train_acc: 0.975705329153605 loss: 0.22721228003501892\n",
      "epoch 798 total_train_acc: 0.9729623824451411 loss: 0.22785985469818115\n",
      "epoch 799 total_train_acc: 0.9725705329153606 loss: 0.23518207669258118\n",
      "epoch 800 total_train_acc: 0.9729623824451411 loss: 0.227165125310421\n",
      "epoch 801 total_train_acc: 0.9745297805642633 loss: 0.22695311158895493\n",
      "epoch 802 total_train_acc: 0.9753134796238244 loss: 0.23351161181926727\n",
      "epoch 803 total_train_acc: 0.975705329153605 loss: 0.2347211241722107\n",
      "epoch 804 total_train_acc: 0.9764890282131662 loss: 0.22727221250534058\n",
      "epoch 805 total_train_acc: 0.9760971786833855 loss: 0.21686749160289764\n",
      "epoch 806 total_train_acc: 0.9721786833855799 loss: 0.23169247806072235\n",
      "epoch 807 total_train_acc: 0.978448275862069 loss: 0.22111835330724716\n",
      "epoch 808 total_train_acc: 0.9768808777429467 loss: 0.2210950329899788\n",
      "epoch 809 total_train_acc: 0.9733542319749217 loss: 0.22582507878541946\n",
      "epoch 810 total_train_acc: 0.978448275862069 loss: 0.20662012323737144\n",
      "epoch 811 total_train_acc: 0.9753134796238244 loss: 0.2201448157429695\n",
      "epoch 812 total_train_acc: 0.9721786833855799 loss: 0.22466124966740608\n",
      "epoch 813 total_train_acc: 0.9760971786833855 loss: 0.22918254882097244\n",
      "epoch 814 total_train_acc: 0.9721786833855799 loss: 0.23345379531383514\n",
      "epoch 815 total_train_acc: 0.9721786833855799 loss: 0.22913549095392227\n",
      "epoch 816 total_train_acc: 0.9749216300940439 loss: 0.22830237448215485\n",
      "epoch 817 total_train_acc: 0.975705329153605 loss: 0.231085155159235\n",
      "epoch 818 total_train_acc: 0.9710031347962382 loss: 0.2502019926905632\n",
      "epoch 819 total_train_acc: 0.9737460815047022 loss: 0.2236422523856163\n",
      "epoch 820 total_train_acc: 0.9760971786833855 loss: 0.21161625906825066\n",
      "epoch 821 total_train_acc: 0.975705329153605 loss: 0.22655212134122849\n",
      "epoch 822 total_train_acc: 0.9749216300940439 loss: 0.22448810935020447\n",
      "epoch 823 total_train_acc: 0.9788401253918495 loss: 0.21778470277786255\n",
      "epoch 824 total_train_acc: 0.9780564263322884 loss: 0.215934369713068\n",
      "epoch 825 total_train_acc: 0.9737460815047022 loss: 0.23227982223033905\n",
      "epoch 826 total_train_acc: 0.9737460815047022 loss: 0.22550678253173828\n",
      "epoch 827 total_train_acc: 0.9745297805642633 loss: 0.22702758014202118\n",
      "epoch 828 total_train_acc: 0.9717868338557993 loss: 0.2224689945578575\n",
      "epoch 829 total_train_acc: 0.9717868338557993 loss: 0.23416251689195633\n",
      "epoch 830 total_train_acc: 0.9749216300940439 loss: 0.21679076552391052\n",
      "epoch 831 total_train_acc: 0.9729623824451411 loss: 0.2224857658147812\n",
      "epoch 832 total_train_acc: 0.9764890282131662 loss: 0.22339559346437454\n",
      "epoch 833 total_train_acc: 0.9713949843260188 loss: 0.23524726927280426\n",
      "epoch 834 total_train_acc: 0.9745297805642633 loss: 0.2259097322821617\n",
      "epoch 835 total_train_acc: 0.9749216300940439 loss: 0.2273855060338974\n",
      "epoch 836 total_train_acc: 0.9713949843260188 loss: 0.23274734616279602\n",
      "epoch 837 total_train_acc: 0.9753134796238244 loss: 0.23667297512292862\n",
      "epoch 838 total_train_acc: 0.9745297805642633 loss: 0.23668299615383148\n",
      "epoch 839 total_train_acc: 0.9733542319749217 loss: 0.22694211453199387\n",
      "epoch 840 total_train_acc: 0.9745297805642633 loss: 0.22447944432497025\n",
      "epoch 841 total_train_acc: 0.9745297805642633 loss: 0.21566300839185715\n",
      "epoch 842 total_train_acc: 0.9745297805642633 loss: 0.21331378817558289\n",
      "epoch 843 total_train_acc: 0.9737460815047022 loss: 0.22756101191043854\n",
      "epoch 844 total_train_acc: 0.9745297805642633 loss: 0.21155443042516708\n",
      "epoch 845 total_train_acc: 0.9737460815047022 loss: 0.23995620757341385\n",
      "epoch 846 total_train_acc: 0.9745297805642633 loss: 0.21930739283561707\n",
      "epoch 847 total_train_acc: 0.9717868338557993 loss: 0.2276003062725067\n",
      "epoch 848 total_train_acc: 0.9749216300940439 loss: 0.21565794199705124\n",
      "epoch 849 total_train_acc: 0.9733542319749217 loss: 0.2188272774219513\n",
      "epoch 850 total_train_acc: 0.9753134796238244 loss: 0.21627970784902573\n",
      "epoch 851 total_train_acc: 0.9741379310344828 loss: 0.22901296615600586\n",
      "epoch 852 total_train_acc: 0.9737460815047022 loss: 0.216525100171566\n",
      "epoch 853 total_train_acc: 0.9772727272727273 loss: 0.1991044096648693\n",
      "epoch 854 total_train_acc: 0.975705329153605 loss: 0.21683549880981445\n",
      "epoch 855 total_train_acc: 0.9745297805642633 loss: 0.20954549312591553\n",
      "epoch 856 total_train_acc: 0.9749216300940439 loss: 0.22923921793699265\n",
      "epoch 857 total_train_acc: 0.9749216300940439 loss: 0.2198059782385826\n",
      "epoch 858 total_train_acc: 0.9749216300940439 loss: 0.21363984793424606\n",
      "epoch 859 total_train_acc: 0.9764890282131662 loss: 0.22337327152490616\n",
      "epoch 860 total_train_acc: 0.975705329153605 loss: 0.21634960919618607\n",
      "epoch 861 total_train_acc: 0.9745297805642633 loss: 0.2075086496770382\n",
      "epoch 862 total_train_acc: 0.9788401253918495 loss: 0.21389997750520706\n",
      "epoch 863 total_train_acc: 0.9796238244514106 loss: 0.20987113565206528\n",
      "epoch 864 total_train_acc: 0.9721786833855799 loss: 0.23027389496564865\n",
      "epoch 865 total_train_acc: 0.975705329153605 loss: 0.22292331978678703\n",
      "epoch 866 total_train_acc: 0.975705329153605 loss: 0.2074667066335678\n",
      "epoch 867 total_train_acc: 0.9733542319749217 loss: 0.21800103783607483\n",
      "epoch 868 total_train_acc: 0.9780564263322884 loss: 0.20824076235294342\n",
      "epoch 869 total_train_acc: 0.9788401253918495 loss: 0.21097902953624725\n",
      "epoch 870 total_train_acc: 0.9800156739811913 loss: 0.20361342653632164\n",
      "epoch 871 total_train_acc: 0.9733542319749217 loss: 0.22347567975521088\n",
      "epoch 872 total_train_acc: 0.9733542319749217 loss: 0.22945261746644974\n",
      "epoch 873 total_train_acc: 0.975705329153605 loss: 0.24169882014393806\n",
      "epoch 874 total_train_acc: 0.9796238244514106 loss: 0.21102070435881615\n",
      "epoch 875 total_train_acc: 0.9733542319749217 loss: 0.22711406648159027\n",
      "epoch 876 total_train_acc: 0.9745297805642633 loss: 0.23062297701835632\n",
      "epoch 877 total_train_acc: 0.9745297805642633 loss: 0.21726108342409134\n",
      "epoch 878 total_train_acc: 0.9725705329153606 loss: 0.23977210372686386\n",
      "epoch 879 total_train_acc: 0.9753134796238244 loss: 0.22514177113771439\n",
      "epoch 880 total_train_acc: 0.978448275862069 loss: 0.22348984330892563\n",
      "epoch 881 total_train_acc: 0.9764890282131662 loss: 0.21417056024074554\n",
      "epoch 882 total_train_acc: 0.9780564263322884 loss: 0.20610330626368523\n",
      "epoch 883 total_train_acc: 0.9764890282131662 loss: 0.20806487277150154\n",
      "epoch 884 total_train_acc: 0.9753134796238244 loss: 0.2139459252357483\n",
      "epoch 885 total_train_acc: 0.9745297805642633 loss: 0.21380867063999176\n",
      "epoch 886 total_train_acc: 0.9760971786833855 loss: 0.21686922013759613\n",
      "epoch 887 total_train_acc: 0.9772727272727273 loss: 0.2136017009615898\n",
      "epoch 888 total_train_acc: 0.9780564263322884 loss: 0.21081066876649857\n",
      "epoch 889 total_train_acc: 0.9776645768025078 loss: 0.2108789086341858\n",
      "epoch 890 total_train_acc: 0.9776645768025078 loss: 0.2122790738940239\n",
      "epoch 891 total_train_acc: 0.9710031347962382 loss: 0.23099710792303085\n",
      "epoch 892 total_train_acc: 0.9737460815047022 loss: 0.21281837671995163\n",
      "epoch 893 total_train_acc: 0.9745297805642633 loss: 0.21061719208955765\n",
      "epoch 894 total_train_acc: 0.9768808777429467 loss: 0.2456047236919403\n",
      "epoch 895 total_train_acc: 0.9737460815047022 loss: 0.21856538951396942\n",
      "epoch 896 total_train_acc: 0.9741379310344828 loss: 0.230746291577816\n",
      "epoch 897 total_train_acc: 0.975705329153605 loss: 0.21264160424470901\n",
      "epoch 898 total_train_acc: 0.9788401253918495 loss: 0.2166532315313816\n",
      "epoch 899 total_train_acc: 0.9753134796238244 loss: 0.21304122358560562\n",
      "epoch 900 total_train_acc: 0.9733542319749217 loss: 0.21478430181741714\n",
      "epoch 901 total_train_acc: 0.9753134796238244 loss: 0.21386364102363586\n",
      "epoch 902 total_train_acc: 0.9737460815047022 loss: 0.2077815681695938\n",
      "epoch 903 total_train_acc: 0.9749216300940439 loss: 0.21017231047153473\n",
      "epoch 904 total_train_acc: 0.9768808777429467 loss: 0.2096628025174141\n",
      "epoch 905 total_train_acc: 0.9749216300940439 loss: 0.22187714278697968\n",
      "epoch 906 total_train_acc: 0.9764890282131662 loss: 0.21553830057382584\n",
      "epoch 907 total_train_acc: 0.9745297805642633 loss: 0.2260093279182911\n",
      "epoch 908 total_train_acc: 0.9737460815047022 loss: 0.23012712597846985\n",
      "epoch 909 total_train_acc: 0.975705329153605 loss: 0.22458522021770477\n",
      "epoch 910 total_train_acc: 0.9760971786833855 loss: 0.21354209259152412\n",
      "epoch 911 total_train_acc: 0.9788401253918495 loss: 0.20641450583934784\n",
      "epoch 912 total_train_acc: 0.9733542319749217 loss: 0.21983958035707474\n",
      "epoch 913 total_train_acc: 0.9698275862068966 loss: 0.23459065705537796\n",
      "epoch 914 total_train_acc: 0.9745297805642633 loss: 0.21332956105470657\n",
      "epoch 915 total_train_acc: 0.9729623824451411 loss: 0.2338046282529831\n",
      "epoch 916 total_train_acc: 0.9760971786833855 loss: 0.22560696303844452\n",
      "epoch 917 total_train_acc: 0.9772727272727273 loss: 0.2047502063214779\n",
      "epoch 918 total_train_acc: 0.9753134796238244 loss: 0.21588125824928284\n",
      "epoch 919 total_train_acc: 0.9768808777429467 loss: 0.20953834056854248\n",
      "epoch 920 total_train_acc: 0.9788401253918495 loss: 0.2055351696908474\n",
      "epoch 921 total_train_acc: 0.9764890282131662 loss: 0.20222936943173409\n",
      "epoch 922 total_train_acc: 0.9796238244514106 loss: 0.2026565670967102\n",
      "epoch 923 total_train_acc: 0.9776645768025078 loss: 0.21064931899309158\n",
      "epoch 924 total_train_acc: 0.9741379310344828 loss: 0.22394564002752304\n",
      "epoch 925 total_train_acc: 0.9772727272727273 loss: 0.19732306152582169\n",
      "epoch 926 total_train_acc: 0.9768808777429467 loss: 0.21371255069971085\n",
      "epoch 927 total_train_acc: 0.9725705329153606 loss: 0.2229093685746193\n",
      "epoch 928 total_train_acc: 0.9729623824451411 loss: 0.22806783393025398\n",
      "epoch 929 total_train_acc: 0.978448275862069 loss: 0.2272414192557335\n",
      "epoch 930 total_train_acc: 0.97923197492163 loss: 0.20767728984355927\n",
      "epoch 931 total_train_acc: 0.9760971786833855 loss: 0.20784328132867813\n",
      "epoch 932 total_train_acc: 0.9760971786833855 loss: 0.2206381857395172\n",
      "epoch 933 total_train_acc: 0.9800156739811913 loss: 0.22325384616851807\n",
      "epoch 934 total_train_acc: 0.97923197492163 loss: 0.20492558926343918\n",
      "epoch 935 total_train_acc: 0.9745297805642633 loss: 0.22112353891134262\n",
      "epoch 936 total_train_acc: 0.975705329153605 loss: 0.20900407433509827\n",
      "epoch 937 total_train_acc: 0.9772727272727273 loss: 0.2032448947429657\n",
      "epoch 938 total_train_acc: 0.9804075235109718 loss: 0.1960086189210415\n",
      "epoch 939 total_train_acc: 0.9749216300940439 loss: 0.22208748012781143\n",
      "epoch 940 total_train_acc: 0.97923197492163 loss: 0.200437493622303\n",
      "epoch 941 total_train_acc: 0.9776645768025078 loss: 0.21409859508275986\n",
      "epoch 942 total_train_acc: 0.9788401253918495 loss: 0.20542972534894943\n",
      "epoch 943 total_train_acc: 0.9772727272727273 loss: 0.2035670094192028\n",
      "epoch 944 total_train_acc: 0.9776645768025078 loss: 0.2052004560828209\n",
      "epoch 945 total_train_acc: 0.975705329153605 loss: 0.21220599114894867\n",
      "epoch 946 total_train_acc: 0.9753134796238244 loss: 0.19959013909101486\n",
      "epoch 947 total_train_acc: 0.9737460815047022 loss: 0.22265280783176422\n",
      "epoch 948 total_train_acc: 0.975705329153605 loss: 0.21030936017632484\n",
      "epoch 949 total_train_acc: 0.97923197492163 loss: 0.19228360801935196\n",
      "epoch 950 total_train_acc: 0.9764890282131662 loss: 0.20699292793869972\n",
      "epoch 951 total_train_acc: 0.9764890282131662 loss: 0.20742632076144218\n",
      "epoch 952 total_train_acc: 0.9796238244514106 loss: 0.208108052611351\n",
      "epoch 953 total_train_acc: 0.9776645768025078 loss: 0.20277120172977448\n",
      "epoch 954 total_train_acc: 0.9729623824451411 loss: 0.22346030175685883\n",
      "epoch 955 total_train_acc: 0.9768808777429467 loss: 0.20263367891311646\n",
      "epoch 956 total_train_acc: 0.975705329153605 loss: 0.23536182194948196\n",
      "epoch 957 total_train_acc: 0.9768808777429467 loss: 0.20656399428844452\n",
      "epoch 958 total_train_acc: 0.9796238244514106 loss: 0.1913532353937626\n",
      "epoch 959 total_train_acc: 0.9776645768025078 loss: 0.21257469803094864\n",
      "epoch 960 total_train_acc: 0.9796238244514106 loss: 0.20198116451501846\n",
      "epoch 961 total_train_acc: 0.9768808777429467 loss: 0.2067972794175148\n",
      "epoch 962 total_train_acc: 0.9764890282131662 loss: 0.20305237919092178\n",
      "epoch 963 total_train_acc: 0.97923197492163 loss: 0.2004351131618023\n",
      "epoch 964 total_train_acc: 0.978448275862069 loss: 0.20245129615068436\n",
      "epoch 965 total_train_acc: 0.9780564263322884 loss: 0.21247006952762604\n",
      "epoch 966 total_train_acc: 0.9729623824451411 loss: 0.20365064591169357\n",
      "epoch 967 total_train_acc: 0.9776645768025078 loss: 0.20202801004052162\n",
      "epoch 968 total_train_acc: 0.97923197492163 loss: 0.2129877358675003\n",
      "epoch 969 total_train_acc: 0.9796238244514106 loss: 0.2069965824484825\n",
      "epoch 970 total_train_acc: 0.9768808777429467 loss: 0.20119690895080566\n",
      "epoch 971 total_train_acc: 0.9788401253918495 loss: 0.20638270676136017\n",
      "epoch 972 total_train_acc: 0.9780564263322884 loss: 0.19337380677461624\n",
      "epoch 973 total_train_acc: 0.978448275862069 loss: 0.22472405061125755\n",
      "epoch 974 total_train_acc: 0.9807993730407524 loss: 0.21007628738880157\n",
      "epoch 975 total_train_acc: 0.9768808777429467 loss: 0.19919909536838531\n",
      "epoch 976 total_train_acc: 0.9753134796238244 loss: 0.20846550911664963\n",
      "epoch 977 total_train_acc: 0.9741379310344828 loss: 0.21663372963666916\n",
      "epoch 978 total_train_acc: 0.9776645768025078 loss: 0.20750776678323746\n",
      "epoch 979 total_train_acc: 0.9772727272727273 loss: 0.19834225624799728\n",
      "epoch 980 total_train_acc: 0.9760971786833855 loss: 0.20013396069407463\n",
      "epoch 981 total_train_acc: 0.9753134796238244 loss: 0.2004007138311863\n",
      "epoch 982 total_train_acc: 0.9780564263322884 loss: 0.1889747753739357\n",
      "epoch 983 total_train_acc: 0.9780564263322884 loss: 0.20233633369207382\n",
      "epoch 984 total_train_acc: 0.975705329153605 loss: 0.21367955207824707\n",
      "epoch 985 total_train_acc: 0.9776645768025078 loss: 0.20304980129003525\n",
      "epoch 986 total_train_acc: 0.9796238244514106 loss: 0.20235372707247734\n",
      "epoch 987 total_train_acc: 0.978448275862069 loss: 0.21498820185661316\n",
      "epoch 988 total_train_acc: 0.9737460815047022 loss: 0.20602234452962875\n",
      "epoch 989 total_train_acc: 0.978448275862069 loss: 0.206802636384964\n",
      "epoch 990 total_train_acc: 0.9764890282131662 loss: 0.21717938780784607\n",
      "epoch 991 total_train_acc: 0.9788401253918495 loss: 0.20634130388498306\n",
      "epoch 992 total_train_acc: 0.9753134796238244 loss: 0.21742885187268257\n",
      "epoch 993 total_train_acc: 0.9796238244514106 loss: 0.19122106209397316\n",
      "epoch 994 total_train_acc: 0.9768808777429467 loss: 0.2044176496565342\n",
      "epoch 995 total_train_acc: 0.978448275862069 loss: 0.2116849720478058\n",
      "epoch 996 total_train_acc: 0.9745297805642633 loss: 0.20951435714960098\n",
      "epoch 997 total_train_acc: 0.9760971786833855 loss: 0.20409360527992249\n",
      "epoch 998 total_train_acc: 0.9807993730407524 loss: 0.18051541224122047\n",
      "epoch 999 total_train_acc: 0.975705329153605 loss: 0.20500196516513824\n",
      "epoch 1000 total_train_acc: 0.9796238244514106 loss: 0.2029583901166916\n",
      "epoch 1001 total_train_acc: 0.9760971786833855 loss: 0.19299092888832092\n",
      "epoch 1002 total_train_acc: 0.9749216300940439 loss: 0.19004542380571365\n",
      "epoch 1003 total_train_acc: 0.9772727272727273 loss: 0.20100697129964828\n",
      "epoch 1004 total_train_acc: 0.978448275862069 loss: 0.20127324759960175\n",
      "epoch 1005 total_train_acc: 0.978448275862069 loss: 0.2063852623105049\n",
      "epoch 1006 total_train_acc: 0.978448275862069 loss: 0.1946832872927189\n",
      "epoch 1007 total_train_acc: 0.9788401253918495 loss: 0.20146173983812332\n",
      "epoch 1008 total_train_acc: 0.9768808777429467 loss: 0.1884942576289177\n",
      "epoch 1009 total_train_acc: 0.978448275862069 loss: 0.19035184010863304\n",
      "epoch 1010 total_train_acc: 0.9772727272727273 loss: 0.20707380026578903\n",
      "epoch 1011 total_train_acc: 0.9788401253918495 loss: 0.19086384400725365\n",
      "epoch 1012 total_train_acc: 0.9780564263322884 loss: 0.1994122453033924\n",
      "epoch 1013 total_train_acc: 0.9804075235109718 loss: 0.19818807393312454\n",
      "epoch 1014 total_train_acc: 0.9788401253918495 loss: 0.1973894014954567\n",
      "epoch 1015 total_train_acc: 0.9764890282131662 loss: 0.21752707660198212\n",
      "epoch 1016 total_train_acc: 0.975705329153605 loss: 0.20227478444576263\n",
      "epoch 1017 total_train_acc: 0.978448275862069 loss: 0.21334387734532356\n",
      "epoch 1018 total_train_acc: 0.9804075235109718 loss: 0.19861283898353577\n",
      "epoch 1019 total_train_acc: 0.9764890282131662 loss: 0.19391600042581558\n",
      "epoch 1020 total_train_acc: 0.97923197492163 loss: 0.19158606603741646\n",
      "epoch 1021 total_train_acc: 0.9788401253918495 loss: 0.189935352653265\n",
      "epoch 1022 total_train_acc: 0.9772727272727273 loss: 0.2020415998995304\n",
      "epoch 1023 total_train_acc: 0.9780564263322884 loss: 0.20640414580702782\n",
      "epoch 1024 total_train_acc: 0.9811912225705329 loss: 0.19320190325379372\n",
      "epoch 1025 total_train_acc: 0.9772727272727273 loss: 0.1903844140470028\n",
      "epoch 1026 total_train_acc: 0.9807993730407524 loss: 0.19166039302945137\n",
      "epoch 1027 total_train_acc: 0.9780564263322884 loss: 0.20655912533402443\n",
      "epoch 1028 total_train_acc: 0.9737460815047022 loss: 0.19776539131999016\n",
      "epoch 1029 total_train_acc: 0.9788401253918495 loss: 0.18029804527759552\n",
      "epoch 1030 total_train_acc: 0.9776645768025078 loss: 0.1822449080646038\n",
      "epoch 1031 total_train_acc: 0.9768808777429467 loss: 0.20526308566331863\n",
      "epoch 1032 total_train_acc: 0.97923197492163 loss: 0.19059865549206734\n",
      "epoch 1033 total_train_acc: 0.9811912225705329 loss: 0.18631120398640633\n",
      "epoch 1034 total_train_acc: 0.9804075235109718 loss: 0.18132847175002098\n",
      "epoch 1035 total_train_acc: 0.9788401253918495 loss: 0.19033365696668625\n",
      "epoch 1036 total_train_acc: 0.9796238244514106 loss: 0.17769084498286247\n",
      "epoch 1037 total_train_acc: 0.9776645768025078 loss: 0.18474126607179642\n",
      "epoch 1038 total_train_acc: 0.978448275862069 loss: 0.19297445192933083\n",
      "epoch 1039 total_train_acc: 0.9780564263322884 loss: 0.2035532370209694\n",
      "epoch 1040 total_train_acc: 0.9745297805642633 loss: 0.20298710465431213\n",
      "epoch 1041 total_train_acc: 0.9776645768025078 loss: 0.19476862624287605\n",
      "epoch 1042 total_train_acc: 0.97923197492163 loss: 0.20228097960352898\n",
      "epoch 1043 total_train_acc: 0.9772727272727273 loss: 0.1960579939186573\n",
      "epoch 1044 total_train_acc: 0.978448275862069 loss: 0.18828009814023972\n",
      "epoch 1045 total_train_acc: 0.9780564263322884 loss: 0.19518658891320229\n",
      "epoch 1046 total_train_acc: 0.9780564263322884 loss: 0.1970871202647686\n",
      "epoch 1047 total_train_acc: 0.9796238244514106 loss: 0.18925121799111366\n",
      "epoch 1048 total_train_acc: 0.9796238244514106 loss: 0.20250016823410988\n",
      "epoch 1049 total_train_acc: 0.9811912225705329 loss: 0.2017137072980404\n",
      "epoch 1050 total_train_acc: 0.9772727272727273 loss: 0.19146069884300232\n",
      "epoch 1051 total_train_acc: 0.9776645768025078 loss: 0.19514930993318558\n",
      "epoch 1052 total_train_acc: 0.9796238244514106 loss: 0.18961436301469803\n",
      "epoch 1053 total_train_acc: 0.9788401253918495 loss: 0.19447898864746094\n",
      "epoch 1054 total_train_acc: 0.9764890282131662 loss: 0.20153728872537613\n",
      "epoch 1055 total_train_acc: 0.9796238244514106 loss: 0.18376173824071884\n",
      "epoch 1056 total_train_acc: 0.9796238244514106 loss: 0.19185658171772957\n",
      "epoch 1057 total_train_acc: 0.9807993730407524 loss: 0.18801094591617584\n",
      "epoch 1058 total_train_acc: 0.9807993730407524 loss: 0.19897713884711266\n",
      "epoch 1059 total_train_acc: 0.9776645768025078 loss: 0.19296860694885254\n",
      "epoch 1060 total_train_acc: 0.978448275862069 loss: 0.1833779625594616\n",
      "epoch 1061 total_train_acc: 0.9764890282131662 loss: 0.2044883705675602\n",
      "epoch 1062 total_train_acc: 0.9772727272727273 loss: 0.2011554054915905\n",
      "epoch 1063 total_train_acc: 0.9804075235109718 loss: 0.1801878809928894\n",
      "epoch 1064 total_train_acc: 0.9760971786833855 loss: 0.19313853606581688\n",
      "epoch 1065 total_train_acc: 0.9768808777429467 loss: 0.19799011945724487\n",
      "epoch 1066 total_train_acc: 0.9780564263322884 loss: 0.1915862299501896\n",
      "epoch 1067 total_train_acc: 0.9780564263322884 loss: 0.1940450370311737\n",
      "epoch 1068 total_train_acc: 0.9804075235109718 loss: 0.19088484346866608\n",
      "epoch 1069 total_train_acc: 0.9764890282131662 loss: 0.2047896832227707\n",
      "epoch 1070 total_train_acc: 0.9780564263322884 loss: 0.1823187880218029\n",
      "epoch 1071 total_train_acc: 0.9776645768025078 loss: 0.18906137719750404\n",
      "epoch 1072 total_train_acc: 0.978448275862069 loss: 0.18914029747247696\n",
      "epoch 1073 total_train_acc: 0.9768808777429467 loss: 0.1860247328877449\n",
      "epoch 1074 total_train_acc: 0.978448275862069 loss: 0.18882907927036285\n",
      "epoch 1075 total_train_acc: 0.9772727272727273 loss: 0.1919446550309658\n",
      "epoch 1076 total_train_acc: 0.9800156739811913 loss: 0.19825788214802742\n",
      "epoch 1077 total_train_acc: 0.9776645768025078 loss: 0.19702798873186111\n",
      "epoch 1078 total_train_acc: 0.9772727272727273 loss: 0.20916638150811195\n",
      "epoch 1079 total_train_acc: 0.9780564263322884 loss: 0.19483670964837074\n",
      "epoch 1080 total_train_acc: 0.9788401253918495 loss: 0.1784670688211918\n",
      "epoch 1081 total_train_acc: 0.9772727272727273 loss: 0.194722980260849\n",
      "epoch 1082 total_train_acc: 0.9780564263322884 loss: 0.18725311383605003\n",
      "epoch 1083 total_train_acc: 0.9768808777429467 loss: 0.20865753665566444\n",
      "epoch 1084 total_train_acc: 0.9788401253918495 loss: 0.19684741646051407\n",
      "epoch 1085 total_train_acc: 0.9772727272727273 loss: 0.19558202847838402\n",
      "epoch 1086 total_train_acc: 0.9827586206896551 loss: 0.17702621594071388\n",
      "epoch 1087 total_train_acc: 0.9788401253918495 loss: 0.18902338668704033\n",
      "epoch 1088 total_train_acc: 0.9804075235109718 loss: 0.16785667836666107\n",
      "epoch 1089 total_train_acc: 0.978448275862069 loss: 0.20267772674560547\n",
      "epoch 1090 total_train_acc: 0.9796238244514106 loss: 0.1765037551522255\n",
      "epoch 1091 total_train_acc: 0.9796238244514106 loss: 0.1753484532237053\n",
      "epoch 1092 total_train_acc: 0.9768808777429467 loss: 0.20102251693606377\n",
      "epoch 1093 total_train_acc: 0.9749216300940439 loss: 0.1930873841047287\n",
      "epoch 1094 total_train_acc: 0.97923197492163 loss: 0.19379939883947372\n",
      "epoch 1095 total_train_acc: 0.97923197492163 loss: 0.19124317169189453\n",
      "epoch 1096 total_train_acc: 0.975705329153605 loss: 0.20077861100435257\n",
      "epoch 1097 total_train_acc: 0.975705329153605 loss: 0.20457232370972633\n",
      "epoch 1098 total_train_acc: 0.9804075235109718 loss: 0.192435584962368\n",
      "epoch 1099 total_train_acc: 0.9788401253918495 loss: 0.18731167912483215\n",
      "epoch 1100 total_train_acc: 0.978448275862069 loss: 0.18627836927771568\n",
      "epoch 1101 total_train_acc: 0.9776645768025078 loss: 0.19765961542725563\n",
      "epoch 1102 total_train_acc: 0.9768808777429467 loss: 0.18075726553797722\n",
      "epoch 1103 total_train_acc: 0.9800156739811913 loss: 0.1928957737982273\n",
      "epoch 1104 total_train_acc: 0.9804075235109718 loss: 0.19205627217888832\n",
      "epoch 1105 total_train_acc: 0.97923197492163 loss: 0.17883579805493355\n",
      "epoch 1106 total_train_acc: 0.9753134796238244 loss: 0.19352084398269653\n",
      "epoch 1107 total_train_acc: 0.9807993730407524 loss: 0.19516926258802414\n",
      "epoch 1108 total_train_acc: 0.9753134796238244 loss: 0.19863112270832062\n",
      "epoch 1109 total_train_acc: 0.978448275862069 loss: 0.19871555268764496\n",
      "epoch 1110 total_train_acc: 0.9768808777429467 loss: 0.20325056463479996\n",
      "epoch 1111 total_train_acc: 0.9780564263322884 loss: 0.19328827410936356\n",
      "epoch 1112 total_train_acc: 0.981974921630094 loss: 0.18595873564481735\n",
      "epoch 1113 total_train_acc: 0.97923197492163 loss: 0.19120525941252708\n",
      "epoch 1114 total_train_acc: 0.9796238244514106 loss: 0.19213074073195457\n",
      "epoch 1115 total_train_acc: 0.9749216300940439 loss: 0.19236205145716667\n",
      "epoch 1116 total_train_acc: 0.9788401253918495 loss: 0.18430812656879425\n",
      "epoch 1117 total_train_acc: 0.9839341692789969 loss: 0.16912845894694328\n",
      "epoch 1118 total_train_acc: 0.9772727272727273 loss: 0.1951025389134884\n",
      "epoch 1119 total_train_acc: 0.9800156739811913 loss: 0.17798283696174622\n",
      "epoch 1120 total_train_acc: 0.981974921630094 loss: 0.18170373514294624\n",
      "epoch 1121 total_train_acc: 0.9737460815047022 loss: 0.21634946390986443\n",
      "epoch 1122 total_train_acc: 0.9815830721003135 loss: 0.18919749185442924\n",
      "epoch 1123 total_train_acc: 0.9796238244514106 loss: 0.1710699237883091\n",
      "epoch 1124 total_train_acc: 0.9788401253918495 loss: 0.19109029695391655\n",
      "epoch 1125 total_train_acc: 0.9811912225705329 loss: 0.17856990173459053\n",
      "epoch 1126 total_train_acc: 0.9780564263322884 loss: 0.20064129307866096\n",
      "epoch 1127 total_train_acc: 0.981974921630094 loss: 0.18709994852542877\n",
      "epoch 1128 total_train_acc: 0.978448275862069 loss: 0.18466762080788612\n",
      "epoch 1129 total_train_acc: 0.981974921630094 loss: 0.17999914288520813\n",
      "epoch 1130 total_train_acc: 0.9811912225705329 loss: 0.17696933820843697\n",
      "epoch 1131 total_train_acc: 0.9804075235109718 loss: 0.17851782217621803\n",
      "epoch 1132 total_train_acc: 0.97923197492163 loss: 0.18868473172187805\n",
      "epoch 1133 total_train_acc: 0.9796238244514106 loss: 0.19391540437936783\n",
      "epoch 1134 total_train_acc: 0.9780564263322884 loss: 0.1776738204061985\n",
      "epoch 1135 total_train_acc: 0.9807993730407524 loss: 0.18076101690530777\n",
      "epoch 1136 total_train_acc: 0.97923197492163 loss: 0.18181301653385162\n",
      "epoch 1137 total_train_acc: 0.9796238244514106 loss: 0.18328826874494553\n",
      "epoch 1138 total_train_acc: 0.9780564263322884 loss: 0.17320357263088226\n",
      "epoch 1139 total_train_acc: 0.9788401253918495 loss: 0.19024891406297684\n",
      "epoch 1140 total_train_acc: 0.97923197492163 loss: 0.17589762434363365\n",
      "epoch 1141 total_train_acc: 0.978448275862069 loss: 0.1926615983247757\n",
      "epoch 1142 total_train_acc: 0.978448275862069 loss: 0.18456488475203514\n",
      "epoch 1143 total_train_acc: 0.9827586206896551 loss: 0.18012144416570663\n",
      "epoch 1144 total_train_acc: 0.9772727272727273 loss: 0.19080136716365814\n",
      "epoch 1145 total_train_acc: 0.9804075235109718 loss: 0.18766823783516884\n",
      "epoch 1146 total_train_acc: 0.9776645768025078 loss: 0.17412510141730309\n",
      "epoch 1147 total_train_acc: 0.9788401253918495 loss: 0.1942138858139515\n",
      "epoch 1148 total_train_acc: 0.9811912225705329 loss: 0.1843184493482113\n",
      "epoch 1149 total_train_acc: 0.981974921630094 loss: 0.1717332862317562\n",
      "epoch 1150 total_train_acc: 0.9807993730407524 loss: 0.19583851844072342\n",
      "epoch 1151 total_train_acc: 0.9800156739811913 loss: 0.17987937852740288\n",
      "epoch 1152 total_train_acc: 0.9827586206896551 loss: 0.16379643976688385\n",
      "epoch 1153 total_train_acc: 0.9804075235109718 loss: 0.18143152818083763\n",
      "epoch 1154 total_train_acc: 0.9788401253918495 loss: 0.18175086379051208\n",
      "epoch 1155 total_train_acc: 0.9772727272727273 loss: 0.1880849376320839\n",
      "epoch 1156 total_train_acc: 0.9807993730407524 loss: 0.1735159009695053\n",
      "epoch 1157 total_train_acc: 0.9772727272727273 loss: 0.19493911415338516\n",
      "epoch 1158 total_train_acc: 0.978448275862069 loss: 0.17894496768712997\n",
      "epoch 1159 total_train_acc: 0.97923197492163 loss: 0.18504350632429123\n",
      "epoch 1160 total_train_acc: 0.9800156739811913 loss: 0.18189505860209465\n",
      "epoch 1161 total_train_acc: 0.9780564263322884 loss: 0.18365545943379402\n",
      "epoch 1162 total_train_acc: 0.9807993730407524 loss: 0.18103328347206116\n",
      "epoch 1163 total_train_acc: 0.978448275862069 loss: 0.17800766602158546\n",
      "epoch 1164 total_train_acc: 0.9807993730407524 loss: 0.16658564656972885\n",
      "epoch 1165 total_train_acc: 0.9815830721003135 loss: 0.1665707491338253\n",
      "epoch 1166 total_train_acc: 0.9776645768025078 loss: 0.18398115038871765\n",
      "epoch 1167 total_train_acc: 0.9815830721003135 loss: 0.17836345359683037\n",
      "epoch 1168 total_train_acc: 0.9780564263322884 loss: 0.18807825446128845\n",
      "epoch 1169 total_train_acc: 0.9835423197492164 loss: 0.16580519080162048\n",
      "epoch 1170 total_train_acc: 0.9776645768025078 loss: 0.1874610297381878\n",
      "epoch 1171 total_train_acc: 0.9796238244514106 loss: 0.1710645817220211\n",
      "epoch 1172 total_train_acc: 0.9815830721003135 loss: 0.18066700920462608\n",
      "epoch 1173 total_train_acc: 0.9788401253918495 loss: 0.17840974777936935\n",
      "epoch 1174 total_train_acc: 0.9815830721003135 loss: 0.1963733434677124\n",
      "epoch 1175 total_train_acc: 0.9827586206896551 loss: 0.16303038597106934\n",
      "epoch 1176 total_train_acc: 0.9807993730407524 loss: 0.16216279566287994\n",
      "epoch 1177 total_train_acc: 0.9804075235109718 loss: 0.18909547850489616\n",
      "epoch 1178 total_train_acc: 0.9811912225705329 loss: 0.1749986931681633\n",
      "epoch 1179 total_train_acc: 0.9804075235109718 loss: 0.19049149751663208\n",
      "epoch 1180 total_train_acc: 0.9811912225705329 loss: 0.17640597000718117\n",
      "epoch 1181 total_train_acc: 0.9807993730407524 loss: 0.17242391780018806\n",
      "epoch 1182 total_train_acc: 0.9788401253918495 loss: 0.19399017468094826\n",
      "epoch 1183 total_train_acc: 0.9788401253918495 loss: 0.1888507567346096\n",
      "epoch 1184 total_train_acc: 0.9796238244514106 loss: 0.17876539751887321\n",
      "epoch 1185 total_train_acc: 0.9772727272727273 loss: 0.1860169805586338\n",
      "epoch 1186 total_train_acc: 0.97923197492163 loss: 0.18421046435832977\n",
      "epoch 1187 total_train_acc: 0.9807993730407524 loss: 0.177264254540205\n",
      "epoch 1188 total_train_acc: 0.9807993730407524 loss: 0.1696290634572506\n",
      "epoch 1189 total_train_acc: 0.9780564263322884 loss: 0.17756019532680511\n",
      "epoch 1190 total_train_acc: 0.9804075235109718 loss: 0.18820485100150108\n",
      "epoch 1191 total_train_acc: 0.97923197492163 loss: 0.1698140613734722\n",
      "epoch 1192 total_train_acc: 0.981974921630094 loss: 0.16805096343159676\n",
      "epoch 1193 total_train_acc: 0.9831504702194357 loss: 0.15976230427622795\n",
      "epoch 1194 total_train_acc: 0.9811912225705329 loss: 0.17996836081147194\n",
      "epoch 1195 total_train_acc: 0.9811912225705329 loss: 0.17427187785506248\n",
      "epoch 1196 total_train_acc: 0.9776645768025078 loss: 0.1810293346643448\n",
      "epoch 1197 total_train_acc: 0.9804075235109718 loss: 0.16116664186120033\n",
      "epoch 1198 total_train_acc: 0.9811912225705329 loss: 0.17547214403748512\n",
      "epoch 1199 total_train_acc: 0.9811912225705329 loss: 0.17693942785263062\n",
      "epoch 1200 total_train_acc: 0.9804075235109718 loss: 0.17837611585855484\n",
      "epoch 1201 total_train_acc: 0.9776645768025078 loss: 0.18402906879782677\n",
      "epoch 1202 total_train_acc: 0.9815830721003135 loss: 0.16938264667987823\n",
      "epoch 1203 total_train_acc: 0.9807993730407524 loss: 0.17608100548386574\n",
      "epoch 1204 total_train_acc: 0.9839341692789969 loss: 0.1646980382502079\n",
      "epoch 1205 total_train_acc: 0.9807993730407524 loss: 0.16928962245583534\n",
      "epoch 1206 total_train_acc: 0.9804075235109718 loss: 0.16935177519917488\n",
      "epoch 1207 total_train_acc: 0.9835423197492164 loss: 0.16147976741194725\n",
      "epoch 1208 total_train_acc: 0.9815830721003135 loss: 0.16639763116836548\n",
      "epoch 1209 total_train_acc: 0.9811912225705329 loss: 0.17139598354697227\n",
      "epoch 1210 total_train_acc: 0.981974921630094 loss: 0.17246459424495697\n",
      "epoch 1211 total_train_acc: 0.9831504702194357 loss: 0.17416907474398613\n",
      "epoch 1212 total_train_acc: 0.9815830721003135 loss: 0.17901378870010376\n",
      "epoch 1213 total_train_acc: 0.9835423197492164 loss: 0.16505900397896767\n",
      "epoch 1214 total_train_acc: 0.9796238244514106 loss: 0.17773953080177307\n",
      "epoch 1215 total_train_acc: 0.9835423197492164 loss: 0.162887554615736\n",
      "epoch 1216 total_train_acc: 0.9823667711598746 loss: 0.1721511036157608\n",
      "epoch 1217 total_train_acc: 0.9796238244514106 loss: 0.17870492488145828\n",
      "epoch 1218 total_train_acc: 0.9804075235109718 loss: 0.17121460661292076\n",
      "epoch 1219 total_train_acc: 0.9815830721003135 loss: 0.16311726719141006\n",
      "epoch 1220 total_train_acc: 0.9796238244514106 loss: 0.17119808495044708\n",
      "epoch 1221 total_train_acc: 0.97923197492163 loss: 0.17663831636309624\n",
      "epoch 1222 total_train_acc: 0.9823667711598746 loss: 0.1750010959804058\n",
      "epoch 1223 total_train_acc: 0.9807993730407524 loss: 0.1869584657251835\n",
      "epoch 1224 total_train_acc: 0.9796238244514106 loss: 0.1805453710258007\n",
      "epoch 1225 total_train_acc: 0.9815830721003135 loss: 0.1643737182021141\n",
      "epoch 1226 total_train_acc: 0.9788401253918495 loss: 0.18306495994329453\n",
      "epoch 1227 total_train_acc: 0.9796238244514106 loss: 0.16996373981237411\n",
      "epoch 1228 total_train_acc: 0.9815830721003135 loss: 0.16841457784175873\n",
      "epoch 1229 total_train_acc: 0.9804075235109718 loss: 0.1781751923263073\n",
      "epoch 1230 total_train_acc: 0.9807993730407524 loss: 0.1772238239645958\n",
      "epoch 1231 total_train_acc: 0.9811912225705329 loss: 0.17816385626792908\n",
      "epoch 1232 total_train_acc: 0.9823667711598746 loss: 0.17058393359184265\n",
      "epoch 1233 total_train_acc: 0.9807993730407524 loss: 0.17340178042650223\n",
      "epoch 1234 total_train_acc: 0.97923197492163 loss: 0.18581873178482056\n",
      "epoch 1235 total_train_acc: 0.97923197492163 loss: 0.17945387586951256\n",
      "epoch 1236 total_train_acc: 0.9827586206896551 loss: 0.16160155460238457\n",
      "epoch 1237 total_train_acc: 0.9831504702194357 loss: 0.1655993089079857\n",
      "epoch 1238 total_train_acc: 0.981974921630094 loss: 0.17054107040166855\n",
      "epoch 1239 total_train_acc: 0.9835423197492164 loss: 0.17809373885393143\n",
      "epoch 1240 total_train_acc: 0.9800156739811913 loss: 0.17324328050017357\n",
      "epoch 1241 total_train_acc: 0.9815830721003135 loss: 0.16568544879555702\n",
      "epoch 1242 total_train_acc: 0.9796238244514106 loss: 0.18200212344527245\n",
      "epoch 1243 total_train_acc: 0.981974921630094 loss: 0.16711897403001785\n",
      "epoch 1244 total_train_acc: 0.97923197492163 loss: 0.1826387383043766\n",
      "epoch 1245 total_train_acc: 0.981974921630094 loss: 0.17381977289915085\n",
      "epoch 1246 total_train_acc: 0.9772727272727273 loss: 0.18178677186369896\n",
      "epoch 1247 total_train_acc: 0.9815830721003135 loss: 0.16171029582619667\n",
      "epoch 1248 total_train_acc: 0.981974921630094 loss: 0.1737595684826374\n",
      "epoch 1249 total_train_acc: 0.9811912225705329 loss: 0.1665424406528473\n",
      "epoch 1250 total_train_acc: 0.9831504702194357 loss: 0.16799217835068703\n",
      "epoch 1251 total_train_acc: 0.9823667711598746 loss: 0.1776760369539261\n",
      "epoch 1252 total_train_acc: 0.9800156739811913 loss: 0.17745019122958183\n",
      "epoch 1253 total_train_acc: 0.981974921630094 loss: 0.17744725942611694\n",
      "epoch 1254 total_train_acc: 0.981974921630094 loss: 0.15719518065452576\n",
      "epoch 1255 total_train_acc: 0.9831504702194357 loss: 0.16761408001184464\n",
      "epoch 1256 total_train_acc: 0.9823667711598746 loss: 0.16234686970710754\n",
      "epoch 1257 total_train_acc: 0.9804075235109718 loss: 0.17136209830641747\n",
      "epoch 1258 total_train_acc: 0.9772727272727273 loss: 0.18678946048021317\n",
      "epoch 1259 total_train_acc: 0.9835423197492164 loss: 0.1723039261996746\n",
      "epoch 1260 total_train_acc: 0.9811912225705329 loss: 0.17478418350219727\n",
      "epoch 1261 total_train_acc: 0.9827586206896551 loss: 0.1589542254805565\n",
      "epoch 1262 total_train_acc: 0.9827586206896551 loss: 0.16511395573616028\n",
      "epoch 1263 total_train_acc: 0.9804075235109718 loss: 0.17678897455334663\n",
      "epoch 1264 total_train_acc: 0.978448275862069 loss: 0.18867596983909607\n",
      "epoch 1265 total_train_acc: 0.9811912225705329 loss: 0.16975752264261246\n",
      "epoch 1266 total_train_acc: 0.9815830721003135 loss: 0.1635536178946495\n",
      "epoch 1267 total_train_acc: 0.981974921630094 loss: 0.1666419468820095\n",
      "epoch 1268 total_train_acc: 0.9796238244514106 loss: 0.1720711775124073\n",
      "epoch 1269 total_train_acc: 0.9835423197492164 loss: 0.16562341153621674\n",
      "epoch 1270 total_train_acc: 0.97923197492163 loss: 0.1711118184030056\n",
      "epoch 1271 total_train_acc: 0.9804075235109718 loss: 0.17011229693889618\n",
      "epoch 1272 total_train_acc: 0.981974921630094 loss: 0.17422137036919594\n",
      "epoch 1273 total_train_acc: 0.9807993730407524 loss: 0.17791107669472694\n",
      "epoch 1274 total_train_acc: 0.97923197492163 loss: 0.1804945468902588\n",
      "epoch 1275 total_train_acc: 0.9768808777429467 loss: 0.18922194465994835\n",
      "epoch 1276 total_train_acc: 0.9788401253918495 loss: 0.18926316127181053\n",
      "epoch 1277 total_train_acc: 0.981974921630094 loss: 0.1776174232363701\n",
      "epoch 1278 total_train_acc: 0.9800156739811913 loss: 0.17147240415215492\n",
      "epoch 1279 total_train_acc: 0.9811912225705329 loss: 0.16494852304458618\n",
      "epoch 1280 total_train_acc: 0.9807993730407524 loss: 0.17195289209485054\n",
      "epoch 1281 total_train_acc: 0.981974921630094 loss: 0.16844908893108368\n",
      "epoch 1282 total_train_acc: 0.9796238244514106 loss: 0.17586788162589073\n",
      "epoch 1283 total_train_acc: 0.9815830721003135 loss: 0.16786041855812073\n",
      "epoch 1284 total_train_acc: 0.981974921630094 loss: 0.16675793379545212\n",
      "epoch 1285 total_train_acc: 0.9823667711598746 loss: 0.17287565395236015\n",
      "epoch 1286 total_train_acc: 0.9807993730407524 loss: 0.1673181839287281\n",
      "epoch 1287 total_train_acc: 0.981974921630094 loss: 0.16679532080888748\n",
      "epoch 1288 total_train_acc: 0.9804075235109718 loss: 0.16393690928816795\n",
      "epoch 1289 total_train_acc: 0.9800156739811913 loss: 0.18298543617129326\n",
      "epoch 1290 total_train_acc: 0.9804075235109718 loss: 0.162474125623703\n",
      "epoch 1291 total_train_acc: 0.9815830721003135 loss: 0.16654767468571663\n",
      "epoch 1292 total_train_acc: 0.9800156739811913 loss: 0.16640692204236984\n",
      "epoch 1293 total_train_acc: 0.9815830721003135 loss: 0.18078133836388588\n",
      "epoch 1294 total_train_acc: 0.97923197492163 loss: 0.168973620980978\n",
      "epoch 1295 total_train_acc: 0.9804075235109718 loss: 0.1672518029808998\n",
      "epoch 1296 total_train_acc: 0.97923197492163 loss: 0.16864768043160439\n",
      "epoch 1297 total_train_acc: 0.9804075235109718 loss: 0.17320771515369415\n",
      "epoch 1298 total_train_acc: 0.9811912225705329 loss: 0.17703668400645256\n",
      "epoch 1299 total_train_acc: 0.978448275862069 loss: 0.1756909415125847\n",
      "epoch 1300 total_train_acc: 0.9811912225705329 loss: 0.17083922773599625\n",
      "epoch 1301 total_train_acc: 0.9796238244514106 loss: 0.16772888600826263\n",
      "epoch 1302 total_train_acc: 0.9807993730407524 loss: 0.17161514982581139\n",
      "epoch 1303 total_train_acc: 0.97923197492163 loss: 0.17058482393622398\n",
      "epoch 1304 total_train_acc: 0.9815830721003135 loss: 0.161265067756176\n",
      "epoch 1305 total_train_acc: 0.9780564263322884 loss: 0.18343517184257507\n",
      "epoch 1306 total_train_acc: 0.984717868338558 loss: 0.16583947837352753\n",
      "epoch 1307 total_train_acc: 0.9800156739811913 loss: 0.18381882831454277\n",
      "epoch 1308 total_train_acc: 0.9839341692789969 loss: 0.15988485142588615\n",
      "epoch 1309 total_train_acc: 0.9827586206896551 loss: 0.16174429655075073\n",
      "epoch 1310 total_train_acc: 0.9804075235109718 loss: 0.17148204520344734\n",
      "epoch 1311 total_train_acc: 0.9823667711598746 loss: 0.1693853847682476\n",
      "epoch 1312 total_train_acc: 0.9843260188087775 loss: 0.1587606966495514\n",
      "epoch 1313 total_train_acc: 0.97923197492163 loss: 0.17551767453551292\n",
      "epoch 1314 total_train_acc: 0.9839341692789969 loss: 0.15710041299462318\n",
      "epoch 1315 total_train_acc: 0.9807993730407524 loss: 0.16394006088376045\n",
      "epoch 1316 total_train_acc: 0.9831504702194357 loss: 0.1727987602353096\n",
      "epoch 1317 total_train_acc: 0.9815830721003135 loss: 0.1598217785358429\n",
      "epoch 1318 total_train_acc: 0.981974921630094 loss: 0.15990019962191582\n",
      "epoch 1319 total_train_acc: 0.9811912225705329 loss: 0.17030252516269684\n",
      "epoch 1320 total_train_acc: 0.981974921630094 loss: 0.1684688851237297\n",
      "epoch 1321 total_train_acc: 0.9823667711598746 loss: 0.16812411323189735\n",
      "epoch 1322 total_train_acc: 0.9815830721003135 loss: 0.17017868906259537\n",
      "epoch 1323 total_train_acc: 0.9800156739811913 loss: 0.16518884152173996\n",
      "epoch 1324 total_train_acc: 0.984717868338558 loss: 0.1630469188094139\n",
      "epoch 1325 total_train_acc: 0.984717868338558 loss: 0.15416021272540092\n",
      "epoch 1326 total_train_acc: 0.97923197492163 loss: 0.18012988939881325\n",
      "epoch 1327 total_train_acc: 0.9788401253918495 loss: 0.16815238445997238\n",
      "epoch 1328 total_train_acc: 0.981974921630094 loss: 0.1630583330988884\n",
      "epoch 1329 total_train_acc: 0.9835423197492164 loss: 0.1597607545554638\n",
      "epoch 1330 total_train_acc: 0.9827586206896551 loss: 0.16086716949939728\n",
      "epoch 1331 total_train_acc: 0.9804075235109718 loss: 0.17971782758831978\n",
      "epoch 1332 total_train_acc: 0.981974921630094 loss: 0.17567211389541626\n",
      "epoch 1333 total_train_acc: 0.9815830721003135 loss: 0.1674889363348484\n",
      "epoch 1334 total_train_acc: 0.981974921630094 loss: 0.16890856623649597\n",
      "epoch 1335 total_train_acc: 0.9835423197492164 loss: 0.15417681634426117\n",
      "epoch 1336 total_train_acc: 0.9831504702194357 loss: 0.16523738205432892\n",
      "epoch 1337 total_train_acc: 0.9815830721003135 loss: 0.16103872656822205\n",
      "epoch 1338 total_train_acc: 0.9835423197492164 loss: 0.16264807060360909\n",
      "epoch 1339 total_train_acc: 0.9827586206896551 loss: 0.1590072400867939\n",
      "epoch 1340 total_train_acc: 0.97923197492163 loss: 0.1683589480817318\n",
      "epoch 1341 total_train_acc: 0.9807993730407524 loss: 0.16294265165925026\n",
      "epoch 1342 total_train_acc: 0.9804075235109718 loss: 0.17136971279978752\n",
      "epoch 1343 total_train_acc: 0.9815830721003135 loss: 0.1591823734343052\n",
      "epoch 1344 total_train_acc: 0.9823667711598746 loss: 0.15723718330264091\n",
      "epoch 1345 total_train_acc: 0.97923197492163 loss: 0.17635588347911835\n",
      "epoch 1346 total_train_acc: 0.984717868338558 loss: 0.15085400268435478\n",
      "epoch 1347 total_train_acc: 0.9827586206896551 loss: 0.1694837175309658\n",
      "epoch 1348 total_train_acc: 0.9807993730407524 loss: 0.16289229691028595\n",
      "epoch 1349 total_train_acc: 0.9776645768025078 loss: 0.17042956873774529\n",
      "epoch 1350 total_train_acc: 0.9815830721003135 loss: 0.16684094816446304\n",
      "epoch 1351 total_train_acc: 0.9815830721003135 loss: 0.16181829199194908\n",
      "epoch 1352 total_train_acc: 0.9772727272727273 loss: 0.1772484853863716\n",
      "epoch 1353 total_train_acc: 0.9823667711598746 loss: 0.16407204419374466\n",
      "epoch 1354 total_train_acc: 0.981974921630094 loss: 0.16259165480732918\n",
      "epoch 1355 total_train_acc: 0.9827586206896551 loss: 0.16433118656277657\n",
      "epoch 1356 total_train_acc: 0.9811912225705329 loss: 0.15854785591363907\n",
      "epoch 1357 total_train_acc: 0.981974921630094 loss: 0.17351442947983742\n",
      "epoch 1358 total_train_acc: 0.9835423197492164 loss: 0.15632573887705803\n",
      "epoch 1359 total_train_acc: 0.9827586206896551 loss: 0.1576373316347599\n",
      "epoch 1360 total_train_acc: 0.9804075235109718 loss: 0.17092015221714973\n",
      "epoch 1361 total_train_acc: 0.9796238244514106 loss: 0.16303163021802902\n",
      "epoch 1362 total_train_acc: 0.981974921630094 loss: 0.175105519592762\n",
      "epoch 1363 total_train_acc: 0.9800156739811913 loss: 0.1749233976006508\n",
      "epoch 1364 total_train_acc: 0.981974921630094 loss: 0.16691773012280464\n",
      "epoch 1365 total_train_acc: 0.9807993730407524 loss: 0.17474811896681786\n",
      "epoch 1366 total_train_acc: 0.9800156739811913 loss: 0.16685178875923157\n",
      "epoch 1367 total_train_acc: 0.9815830721003135 loss: 0.1625531166791916\n",
      "epoch 1368 total_train_acc: 0.9815830721003135 loss: 0.1676497459411621\n",
      "epoch 1369 total_train_acc: 0.9843260188087775 loss: 0.1670820266008377\n",
      "epoch 1370 total_train_acc: 0.9807993730407524 loss: 0.16993637010455132\n",
      "epoch 1371 total_train_acc: 0.978448275862069 loss: 0.17917458713054657\n",
      "epoch 1372 total_train_acc: 0.9831504702194357 loss: 0.15927181020379066\n",
      "epoch 1373 total_train_acc: 0.9839341692789969 loss: 0.16001217812299728\n",
      "epoch 1374 total_train_acc: 0.9800156739811913 loss: 0.1645205207169056\n",
      "epoch 1375 total_train_acc: 0.9804075235109718 loss: 0.17328278720378876\n",
      "epoch 1376 total_train_acc: 0.9807993730407524 loss: 0.16676973178982735\n",
      "epoch 1377 total_train_acc: 0.9804075235109718 loss: 0.16534211859107018\n",
      "epoch 1378 total_train_acc: 0.9804075235109718 loss: 0.1624910719692707\n",
      "epoch 1379 total_train_acc: 0.981974921630094 loss: 0.1642555221915245\n",
      "epoch 1380 total_train_acc: 0.9811912225705329 loss: 0.16625773161649704\n",
      "epoch 1381 total_train_acc: 0.9835423197492164 loss: 0.16047510132193565\n",
      "epoch 1382 total_train_acc: 0.9831504702194357 loss: 0.15797164291143417\n",
      "epoch 1383 total_train_acc: 0.9851097178683386 loss: 0.15946979448199272\n",
      "epoch 1384 total_train_acc: 0.981974921630094 loss: 0.16354991868138313\n",
      "epoch 1385 total_train_acc: 0.9839341692789969 loss: 0.1547037996351719\n",
      "epoch 1386 total_train_acc: 0.9858934169278997 loss: 0.14781147614121437\n",
      "epoch 1387 total_train_acc: 0.9815830721003135 loss: 0.15325401350855827\n",
      "epoch 1388 total_train_acc: 0.9800156739811913 loss: 0.16248751431703568\n",
      "epoch 1389 total_train_acc: 0.9839341692789969 loss: 0.1659947633743286\n",
      "epoch 1390 total_train_acc: 0.9800156739811913 loss: 0.16674988344311714\n",
      "epoch 1391 total_train_acc: 0.9855015673981191 loss: 0.157244011759758\n",
      "epoch 1392 total_train_acc: 0.9807993730407524 loss: 0.15254808962345123\n",
      "epoch 1393 total_train_acc: 0.9831504702194357 loss: 0.14865127578377724\n",
      "epoch 1394 total_train_acc: 0.9823667711598746 loss: 0.16431820392608643\n",
      "epoch 1395 total_train_acc: 0.9804075235109718 loss: 0.1562880389392376\n",
      "epoch 1396 total_train_acc: 0.9839341692789969 loss: 0.1649220660328865\n",
      "epoch 1397 total_train_acc: 0.981974921630094 loss: 0.17196008935570717\n",
      "epoch 1398 total_train_acc: 0.9800156739811913 loss: 0.16204002127051353\n",
      "epoch 1399 total_train_acc: 0.9823667711598746 loss: 0.15125642344355583\n",
      "epoch 1400 total_train_acc: 0.981974921630094 loss: 0.15971002355217934\n",
      "epoch 1401 total_train_acc: 0.9839341692789969 loss: 0.16462360322475433\n",
      "epoch 1402 total_train_acc: 0.9839341692789969 loss: 0.15488439798355103\n",
      "epoch 1403 total_train_acc: 0.9804075235109718 loss: 0.16481758654117584\n",
      "epoch 1404 total_train_acc: 0.9839341692789969 loss: 0.16659999638795853\n",
      "epoch 1405 total_train_acc: 0.9815830721003135 loss: 0.16095591336488724\n",
      "epoch 1406 total_train_acc: 0.9823667711598746 loss: 0.16522466391324997\n",
      "epoch 1407 total_train_acc: 0.9839341692789969 loss: 0.15918929129838943\n",
      "epoch 1408 total_train_acc: 0.9823667711598746 loss: 0.15581192448735237\n",
      "epoch 1409 total_train_acc: 0.9811912225705329 loss: 0.17247219756245613\n",
      "epoch 1410 total_train_acc: 0.9796238244514106 loss: 0.16646812856197357\n",
      "epoch 1411 total_train_acc: 0.981974921630094 loss: 0.16711537539958954\n",
      "epoch 1412 total_train_acc: 0.9811912225705329 loss: 0.17548762634396553\n",
      "epoch 1413 total_train_acc: 0.9815830721003135 loss: 0.16242844238877296\n",
      "epoch 1414 total_train_acc: 0.9807993730407524 loss: 0.1620478630065918\n",
      "epoch 1415 total_train_acc: 0.9807993730407524 loss: 0.16306720674037933\n",
      "epoch 1416 total_train_acc: 0.9835423197492164 loss: 0.15467535704374313\n",
      "epoch 1417 total_train_acc: 0.981974921630094 loss: 0.1586320735514164\n",
      "epoch 1418 total_train_acc: 0.9807993730407524 loss: 0.1674834005534649\n",
      "epoch 1419 total_train_acc: 0.9815830721003135 loss: 0.1578136757016182\n",
      "epoch 1420 total_train_acc: 0.9811912225705329 loss: 0.1599518544971943\n",
      "epoch 1421 total_train_acc: 0.9811912225705329 loss: 0.15585971251130104\n",
      "epoch 1422 total_train_acc: 0.9835423197492164 loss: 0.15844512358307838\n",
      "epoch 1423 total_train_acc: 0.9827586206896551 loss: 0.15784039720892906\n",
      "epoch 1424 total_train_acc: 0.9831504702194357 loss: 0.15912393480539322\n",
      "epoch 1425 total_train_acc: 0.9827586206896551 loss: 0.15831157192587852\n",
      "epoch 1426 total_train_acc: 0.9843260188087775 loss: 0.16612423211336136\n",
      "epoch 1427 total_train_acc: 0.9831504702194357 loss: 0.15204205363988876\n",
      "epoch 1428 total_train_acc: 0.981974921630094 loss: 0.1651075780391693\n",
      "epoch 1429 total_train_acc: 0.9831504702194357 loss: 0.15978499874472618\n",
      "epoch 1430 total_train_acc: 0.9835423197492164 loss: 0.15446437895298004\n",
      "epoch 1431 total_train_acc: 0.97923197492163 loss: 0.1772729530930519\n",
      "epoch 1432 total_train_acc: 0.9835423197492164 loss: 0.15155407786369324\n",
      "epoch 1433 total_train_acc: 0.97923197492163 loss: 0.17345454543828964\n",
      "epoch 1434 total_train_acc: 0.9815830721003135 loss: 0.1695088930428028\n",
      "epoch 1435 total_train_acc: 0.9827586206896551 loss: 0.15766305103898048\n",
      "epoch 1436 total_train_acc: 0.9835423197492164 loss: 0.15861868858337402\n",
      "epoch 1437 total_train_acc: 0.9800156739811913 loss: 0.17576895654201508\n",
      "epoch 1438 total_train_acc: 0.9862852664576802 loss: 0.1519409827888012\n",
      "epoch 1439 total_train_acc: 0.9780564263322884 loss: 0.16483165696263313\n",
      "epoch 1440 total_train_acc: 0.9823667711598746 loss: 0.15223495662212372\n",
      "epoch 1441 total_train_acc: 0.9823667711598746 loss: 0.16057319566607475\n",
      "epoch 1442 total_train_acc: 0.9827586206896551 loss: 0.15794211253523827\n",
      "epoch 1443 total_train_acc: 0.9815830721003135 loss: 0.1643236018717289\n",
      "epoch 1444 total_train_acc: 0.9804075235109718 loss: 0.15571464970707893\n",
      "epoch 1445 total_train_acc: 0.9858934169278997 loss: 0.15631423518061638\n",
      "epoch 1446 total_train_acc: 0.9807993730407524 loss: 0.16935515403747559\n",
      "epoch 1447 total_train_acc: 0.9827586206896551 loss: 0.15673242881894112\n",
      "epoch 1448 total_train_acc: 0.9827586206896551 loss: 0.16176782548427582\n",
      "epoch 1449 total_train_acc: 0.9823667711598746 loss: 0.1580401472747326\n",
      "epoch 1450 total_train_acc: 0.981974921630094 loss: 0.16387202218174934\n",
      "epoch 1451 total_train_acc: 0.9839341692789969 loss: 0.16195488348603249\n",
      "epoch 1452 total_train_acc: 0.9839341692789969 loss: 0.15563329681754112\n",
      "epoch 1453 total_train_acc: 0.9800156739811913 loss: 0.15747518092393875\n",
      "epoch 1454 total_train_acc: 0.9811912225705329 loss: 0.1550525799393654\n",
      "epoch 1455 total_train_acc: 0.9796238244514106 loss: 0.1615329198539257\n",
      "epoch 1456 total_train_acc: 0.9839341692789969 loss: 0.1640201210975647\n",
      "epoch 1457 total_train_acc: 0.981974921630094 loss: 0.16734154149889946\n",
      "epoch 1458 total_train_acc: 0.9839341692789969 loss: 0.1549643725156784\n",
      "epoch 1459 total_train_acc: 0.9811912225705329 loss: 0.17152836546301842\n",
      "epoch 1460 total_train_acc: 0.9800156739811913 loss: 0.1616029068827629\n",
      "epoch 1461 total_train_acc: 0.9788401253918495 loss: 0.1638879030942917\n",
      "epoch 1462 total_train_acc: 0.984717868338558 loss: 0.1674446016550064\n",
      "epoch 1463 total_train_acc: 0.9827586206896551 loss: 0.1505286693572998\n",
      "epoch 1464 total_train_acc: 0.9831504702194357 loss: 0.1537022516131401\n",
      "epoch 1465 total_train_acc: 0.9835423197492164 loss: 0.15053993463516235\n",
      "epoch 1466 total_train_acc: 0.9804075235109718 loss: 0.1558561883866787\n",
      "epoch 1467 total_train_acc: 0.9835423197492164 loss: 0.16302276775240898\n",
      "epoch 1468 total_train_acc: 0.97923197492163 loss: 0.17014581337571144\n",
      "epoch 1469 total_train_acc: 0.9804075235109718 loss: 0.16842065751552582\n",
      "epoch 1470 total_train_acc: 0.9827586206896551 loss: 0.16066941246390343\n",
      "epoch 1471 total_train_acc: 0.9776645768025078 loss: 0.1723252385854721\n",
      "epoch 1472 total_train_acc: 0.981974921630094 loss: 0.1593141220510006\n",
      "epoch 1473 total_train_acc: 0.981974921630094 loss: 0.15869367867708206\n",
      "epoch 1474 total_train_acc: 0.9862852664576802 loss: 0.16601861268281937\n",
      "epoch 1475 total_train_acc: 0.9851097178683386 loss: 0.1476985365152359\n",
      "epoch 1476 total_train_acc: 0.9811912225705329 loss: 0.1604928821325302\n",
      "epoch 1477 total_train_acc: 0.9827586206896551 loss: 0.1501142755150795\n",
      "epoch 1478 total_train_acc: 0.9823667711598746 loss: 0.16214913502335548\n",
      "epoch 1479 total_train_acc: 0.9835423197492164 loss: 0.15970509499311447\n",
      "epoch 1480 total_train_acc: 0.9843260188087775 loss: 0.14851446449756622\n",
      "epoch 1481 total_train_acc: 0.9823667711598746 loss: 0.15935002639889717\n",
      "epoch 1482 total_train_acc: 0.9796238244514106 loss: 0.17125975713133812\n",
      "epoch 1483 total_train_acc: 0.9855015673981191 loss: 0.15854573994874954\n",
      "epoch 1484 total_train_acc: 0.9827586206896551 loss: 0.15569975227117538\n",
      "epoch 1485 total_train_acc: 0.981974921630094 loss: 0.16829899698495865\n",
      "epoch 1486 total_train_acc: 0.9811912225705329 loss: 0.16153687611222267\n",
      "epoch 1487 total_train_acc: 0.9811912225705329 loss: 0.15185074880719185\n",
      "epoch 1488 total_train_acc: 0.9807993730407524 loss: 0.16572748869657516\n",
      "epoch 1489 total_train_acc: 0.9811912225705329 loss: 0.1608435921370983\n",
      "epoch 1490 total_train_acc: 0.984717868338558 loss: 0.151640135794878\n",
      "epoch 1491 total_train_acc: 0.9780564263322884 loss: 0.15304041653871536\n",
      "epoch 1492 total_train_acc: 0.9807993730407524 loss: 0.15741411969065666\n",
      "epoch 1493 total_train_acc: 0.9815830721003135 loss: 0.15904362872242928\n",
      "epoch 1494 total_train_acc: 0.9835423197492164 loss: 0.1520194336771965\n",
      "epoch 1495 total_train_acc: 0.9831504702194357 loss: 0.16187892481684685\n",
      "epoch 1496 total_train_acc: 0.981974921630094 loss: 0.15597056597471237\n",
      "epoch 1497 total_train_acc: 0.9843260188087775 loss: 0.15819906815886497\n",
      "epoch 1498 total_train_acc: 0.9855015673981191 loss: 0.13978344202041626\n",
      "epoch 1499 total_train_acc: 0.981974921630094 loss: 0.1511857770383358\n",
      "epoch 1500 total_train_acc: 0.9835423197492164 loss: 0.15627727285027504\n",
      "epoch 1501 total_train_acc: 0.981974921630094 loss: 0.15972064435482025\n",
      "epoch 1502 total_train_acc: 0.9815830721003135 loss: 0.15642770007252693\n",
      "epoch 1503 total_train_acc: 0.9800156739811913 loss: 0.15599128231406212\n",
      "epoch 1504 total_train_acc: 0.981974921630094 loss: 0.16982392594218254\n",
      "epoch 1505 total_train_acc: 0.9827586206896551 loss: 0.15568611398339272\n",
      "epoch 1506 total_train_acc: 0.984717868338558 loss: 0.15401052311062813\n",
      "epoch 1507 total_train_acc: 0.9839341692789969 loss: 0.15983041003346443\n",
      "epoch 1508 total_train_acc: 0.9831504702194357 loss: 0.14926957711577415\n",
      "epoch 1509 total_train_acc: 0.981974921630094 loss: 0.15517310053110123\n",
      "epoch 1510 total_train_acc: 0.9835423197492164 loss: 0.16585158929228783\n",
      "epoch 1511 total_train_acc: 0.9831504702194357 loss: 0.15260225534439087\n",
      "epoch 1512 total_train_acc: 0.9851097178683386 loss: 0.149973563849926\n",
      "epoch 1513 total_train_acc: 0.9831504702194357 loss: 0.1455170027911663\n",
      "epoch 1514 total_train_acc: 0.9839341692789969 loss: 0.15754707530140877\n",
      "epoch 1515 total_train_acc: 0.9835423197492164 loss: 0.15885557606816292\n",
      "epoch 1516 total_train_acc: 0.9851097178683386 loss: 0.1451386734843254\n",
      "epoch 1517 total_train_acc: 0.9772727272727273 loss: 0.1593790501356125\n",
      "epoch 1518 total_train_acc: 0.9862852664576802 loss: 0.15053506940603256\n",
      "epoch 1519 total_train_acc: 0.9827586206896551 loss: 0.16098395362496376\n",
      "epoch 1520 total_train_acc: 0.9815830721003135 loss: 0.1623244471848011\n",
      "epoch 1521 total_train_acc: 0.9807993730407524 loss: 0.15994538366794586\n",
      "epoch 1522 total_train_acc: 0.981974921630094 loss: 0.1505262367427349\n",
      "epoch 1523 total_train_acc: 0.9835423197492164 loss: 0.15194949507713318\n",
      "epoch 1524 total_train_acc: 0.9823667711598746 loss: 0.15820931643247604\n",
      "epoch 1525 total_train_acc: 0.981974921630094 loss: 0.15297756716609\n",
      "epoch 1526 total_train_acc: 0.981974921630094 loss: 0.16043809428811073\n",
      "epoch 1527 total_train_acc: 0.9835423197492164 loss: 0.15551356971263885\n",
      "epoch 1528 total_train_acc: 0.9839341692789969 loss: 0.14992883801460266\n",
      "epoch 1529 total_train_acc: 0.9827586206896551 loss: 0.15537584945559502\n",
      "epoch 1530 total_train_acc: 0.9835423197492164 loss: 0.15599080175161362\n",
      "epoch 1531 total_train_acc: 0.9823667711598746 loss: 0.15324127301573753\n",
      "epoch 1532 total_train_acc: 0.9823667711598746 loss: 0.15130183473229408\n",
      "epoch 1533 total_train_acc: 0.984717868338558 loss: 0.14411542937159538\n",
      "epoch 1534 total_train_acc: 0.981974921630094 loss: 0.1663469448685646\n",
      "epoch 1535 total_train_acc: 0.9823667711598746 loss: 0.1624324880540371\n",
      "epoch 1536 total_train_acc: 0.9811912225705329 loss: 0.1614127904176712\n",
      "epoch 1537 total_train_acc: 0.981974921630094 loss: 0.15483153983950615\n",
      "epoch 1538 total_train_acc: 0.9823667711598746 loss: 0.1598590798676014\n",
      "epoch 1539 total_train_acc: 0.9858934169278997 loss: 0.1444074735045433\n",
      "epoch 1540 total_train_acc: 0.9839341692789969 loss: 0.14930318295955658\n",
      "epoch 1541 total_train_acc: 0.9796238244514106 loss: 0.15235955640673637\n",
      "epoch 1542 total_train_acc: 0.981974921630094 loss: 0.1591334454715252\n",
      "epoch 1543 total_train_acc: 0.9843260188087775 loss: 0.15324100479483604\n",
      "epoch 1544 total_train_acc: 0.9807993730407524 loss: 0.1693563498556614\n",
      "epoch 1545 total_train_acc: 0.9843260188087775 loss: 0.1560741849243641\n",
      "epoch 1546 total_train_acc: 0.9851097178683386 loss: 0.15198243036866188\n",
      "epoch 1547 total_train_acc: 0.981974921630094 loss: 0.15179085731506348\n",
      "epoch 1548 total_train_acc: 0.9827586206896551 loss: 0.16095618903636932\n",
      "epoch 1549 total_train_acc: 0.981974921630094 loss: 0.15011600777506828\n",
      "epoch 1550 total_train_acc: 0.9811912225705329 loss: 0.1623990386724472\n",
      "epoch 1551 total_train_acc: 0.9815830721003135 loss: 0.16262996196746826\n",
      "epoch 1552 total_train_acc: 0.9811912225705329 loss: 0.1588829681277275\n",
      "epoch 1553 total_train_acc: 0.9855015673981191 loss: 0.14430904015898705\n",
      "epoch 1554 total_train_acc: 0.9851097178683386 loss: 0.15528006479144096\n",
      "epoch 1555 total_train_acc: 0.9843260188087775 loss: 0.15919603779911995\n",
      "epoch 1556 total_train_acc: 0.9815830721003135 loss: 0.15372934192419052\n",
      "epoch 1557 total_train_acc: 0.9815830721003135 loss: 0.15168674290180206\n",
      "epoch 1558 total_train_acc: 0.9843260188087775 loss: 0.13996756821870804\n",
      "epoch 1559 total_train_acc: 0.9815830721003135 loss: 0.16159049794077873\n",
      "epoch 1560 total_train_acc: 0.9858934169278997 loss: 0.15669771283864975\n",
      "epoch 1561 total_train_acc: 0.9815830721003135 loss: 0.16568772494792938\n",
      "epoch 1562 total_train_acc: 0.9804075235109718 loss: 0.16011659055948257\n",
      "epoch 1563 total_train_acc: 0.9823667711598746 loss: 0.1446530669927597\n",
      "epoch 1564 total_train_acc: 0.9827586206896551 loss: 0.16519135609269142\n",
      "epoch 1565 total_train_acc: 0.9858934169278997 loss: 0.14379609748721123\n",
      "epoch 1566 total_train_acc: 0.9839341692789969 loss: 0.14580319449305534\n",
      "epoch 1567 total_train_acc: 0.9835423197492164 loss: 0.1474231295287609\n",
      "epoch 1568 total_train_acc: 0.9851097178683386 loss: 0.1379370503127575\n",
      "epoch 1569 total_train_acc: 0.9827586206896551 loss: 0.158347949385643\n",
      "epoch 1570 total_train_acc: 0.981974921630094 loss: 0.15775710344314575\n",
      "epoch 1571 total_train_acc: 0.9811912225705329 loss: 0.15920917689800262\n",
      "epoch 1572 total_train_acc: 0.9804075235109718 loss: 0.16457988694310188\n",
      "epoch 1573 total_train_acc: 0.9851097178683386 loss: 0.14352988824248314\n",
      "epoch 1574 total_train_acc: 0.9827586206896551 loss: 0.14600040763616562\n",
      "epoch 1575 total_train_acc: 0.9851097178683386 loss: 0.14017094299197197\n",
      "epoch 1576 total_train_acc: 0.9823667711598746 loss: 0.15687697008252144\n",
      "epoch 1577 total_train_acc: 0.9835423197492164 loss: 0.16279489174485207\n",
      "epoch 1578 total_train_acc: 0.9831504702194357 loss: 0.15394840762019157\n",
      "epoch 1579 total_train_acc: 0.9839341692789969 loss: 0.1393618881702423\n",
      "epoch 1580 total_train_acc: 0.9835423197492164 loss: 0.15293660387396812\n",
      "epoch 1581 total_train_acc: 0.9831504702194357 loss: 0.1460459940135479\n",
      "epoch 1582 total_train_acc: 0.984717868338558 loss: 0.15172604471445084\n",
      "epoch 1583 total_train_acc: 0.984717868338558 loss: 0.15417960658669472\n",
      "epoch 1584 total_train_acc: 0.9831504702194357 loss: 0.15008336305618286\n",
      "epoch 1585 total_train_acc: 0.9807993730407524 loss: 0.1655653715133667\n",
      "epoch 1586 total_train_acc: 0.9823667711598746 loss: 0.16348498314619064\n",
      "epoch 1587 total_train_acc: 0.9811912225705329 loss: 0.16449103131890297\n",
      "epoch 1588 total_train_acc: 0.981974921630094 loss: 0.15229855850338936\n",
      "epoch 1589 total_train_acc: 0.9807993730407524 loss: 0.14805357158184052\n",
      "epoch 1590 total_train_acc: 0.9835423197492164 loss: 0.16069898381829262\n",
      "epoch 1591 total_train_acc: 0.9831504702194357 loss: 0.14139008149504662\n",
      "epoch 1592 total_train_acc: 0.9839341692789969 loss: 0.1476365625858307\n",
      "epoch 1593 total_train_acc: 0.9811912225705329 loss: 0.1597033552825451\n",
      "epoch 1594 total_train_acc: 0.9831504702194357 loss: 0.13851350918412209\n",
      "epoch 1595 total_train_acc: 0.9835423197492164 loss: 0.14643267169594765\n",
      "epoch 1596 total_train_acc: 0.9831504702194357 loss: 0.16787226870656013\n",
      "epoch 1597 total_train_acc: 0.9839341692789969 loss: 0.15066732838749886\n",
      "epoch 1598 total_train_acc: 0.9827586206896551 loss: 0.16184968873858452\n",
      "epoch 1599 total_train_acc: 0.9831504702194357 loss: 0.1503579244017601\n",
      "epoch 1600 total_train_acc: 0.9835423197492164 loss: 0.1523178033530712\n",
      "epoch 1601 total_train_acc: 0.9839341692789969 loss: 0.15030938386917114\n",
      "epoch 1602 total_train_acc: 0.9858934169278997 loss: 0.13755396381020546\n",
      "epoch 1603 total_train_acc: 0.9855015673981191 loss: 0.15390156209468842\n",
      "epoch 1604 total_train_acc: 0.9835423197492164 loss: 0.15178073942661285\n",
      "epoch 1605 total_train_acc: 0.9831504702194357 loss: 0.15091579034924507\n",
      "epoch 1606 total_train_acc: 0.9851097178683386 loss: 0.1539251133799553\n",
      "epoch 1607 total_train_acc: 0.9811912225705329 loss: 0.15450384840369225\n",
      "epoch 1608 total_train_acc: 0.9827586206896551 loss: 0.15195244550704956\n",
      "epoch 1609 total_train_acc: 0.9835423197492164 loss: 0.14733286947011948\n",
      "epoch 1610 total_train_acc: 0.9855015673981191 loss: 0.1524108238518238\n",
      "epoch 1611 total_train_acc: 0.9807993730407524 loss: 0.16021474078297615\n",
      "epoch 1612 total_train_acc: 0.9823667711598746 loss: 0.15456363931298256\n",
      "epoch 1613 total_train_acc: 0.9831504702194357 loss: 0.15016239881515503\n",
      "epoch 1614 total_train_acc: 0.9851097178683386 loss: 0.14062178879976273\n",
      "epoch 1615 total_train_acc: 0.9804075235109718 loss: 0.16844170913100243\n",
      "epoch 1616 total_train_acc: 0.9851097178683386 loss: 0.1441683992743492\n",
      "epoch 1617 total_train_acc: 0.9851097178683386 loss: 0.15070851519703865\n",
      "epoch 1618 total_train_acc: 0.981974921630094 loss: 0.15511492639780045\n",
      "epoch 1619 total_train_acc: 0.9823667711598746 loss: 0.1543874852359295\n",
      "epoch 1620 total_train_acc: 0.9839341692789969 loss: 0.1513073667883873\n",
      "epoch 1621 total_train_acc: 0.9843260188087775 loss: 0.1512855850160122\n",
      "epoch 1622 total_train_acc: 0.9827586206896551 loss: 0.14095813408493996\n",
      "epoch 1623 total_train_acc: 0.984717868338558 loss: 0.14166413247585297\n",
      "epoch 1624 total_train_acc: 0.9839341692789969 loss: 0.1467030793428421\n",
      "epoch 1625 total_train_acc: 0.9855015673981191 loss: 0.14524966478347778\n",
      "epoch 1626 total_train_acc: 0.9862852664576802 loss: 0.14005989953875542\n",
      "epoch 1627 total_train_acc: 0.9807993730407524 loss: 0.15375976264476776\n",
      "epoch 1628 total_train_acc: 0.987460815047022 loss: 0.1420859917998314\n",
      "epoch 1629 total_train_acc: 0.9831504702194357 loss: 0.1539975143969059\n",
      "epoch 1630 total_train_acc: 0.9835423197492164 loss: 0.14736903458833694\n",
      "epoch 1631 total_train_acc: 0.9839341692789969 loss: 0.1561673991382122\n",
      "epoch 1632 total_train_acc: 0.9843260188087775 loss: 0.13875167444348335\n",
      "epoch 1633 total_train_acc: 0.981974921630094 loss: 0.14746950939297676\n",
      "epoch 1634 total_train_acc: 0.9811912225705329 loss: 0.15065496042370796\n",
      "epoch 1635 total_train_acc: 0.9760971786833855 loss: 0.17106623947620392\n",
      "epoch 1636 total_train_acc: 0.9843260188087775 loss: 0.13832508400082588\n",
      "epoch 1637 total_train_acc: 0.981974921630094 loss: 0.15501880273222923\n",
      "epoch 1638 total_train_acc: 0.9831504702194357 loss: 0.15163681656122208\n",
      "epoch 1639 total_train_acc: 0.9862852664576802 loss: 0.1534707061946392\n",
      "epoch 1640 total_train_acc: 0.9823667711598746 loss: 0.15375268459320068\n",
      "epoch 1641 total_train_acc: 0.9831504702194357 loss: 0.15661254152655602\n",
      "epoch 1642 total_train_acc: 0.9827586206896551 loss: 0.15341338887810707\n",
      "epoch 1643 total_train_acc: 0.9827586206896551 loss: 0.15479040145874023\n",
      "epoch 1644 total_train_acc: 0.9843260188087775 loss: 0.1534811332821846\n",
      "epoch 1645 total_train_acc: 0.9831504702194357 loss: 0.1607849895954132\n",
      "epoch 1646 total_train_acc: 0.9835423197492164 loss: 0.159015242010355\n",
      "epoch 1647 total_train_acc: 0.9831504702194357 loss: 0.16079365089535713\n",
      "epoch 1648 total_train_acc: 0.9851097178683386 loss: 0.1382475532591343\n",
      "epoch 1649 total_train_acc: 0.9827586206896551 loss: 0.15296142920851707\n",
      "epoch 1650 total_train_acc: 0.9823667711598746 loss: 0.14867442101240158\n",
      "epoch 1651 total_train_acc: 0.9804075235109718 loss: 0.15526745840907097\n",
      "epoch 1652 total_train_acc: 0.9839341692789969 loss: 0.13902034610509872\n",
      "epoch 1653 total_train_acc: 0.981974921630094 loss: 0.14828446879982948\n",
      "epoch 1654 total_train_acc: 0.9855015673981191 loss: 0.14118726179003716\n",
      "epoch 1655 total_train_acc: 0.9800156739811913 loss: 0.15659912303090096\n",
      "epoch 1656 total_train_acc: 0.9823667711598746 loss: 0.14617175608873367\n",
      "epoch 1657 total_train_acc: 0.9866771159874608 loss: 0.14955439046025276\n",
      "epoch 1658 total_train_acc: 0.9870689655172413 loss: 0.14540382847189903\n",
      "epoch 1659 total_train_acc: 0.9843260188087775 loss: 0.15829050540924072\n",
      "epoch 1660 total_train_acc: 0.9862852664576802 loss: 0.14195320010185242\n",
      "epoch 1661 total_train_acc: 0.9827586206896551 loss: 0.15539458021521568\n",
      "epoch 1662 total_train_acc: 0.9870689655172413 loss: 0.15443198755383492\n",
      "epoch 1663 total_train_acc: 0.981974921630094 loss: 0.1515825353562832\n",
      "epoch 1664 total_train_acc: 0.981974921630094 loss: 0.15907922387123108\n",
      "epoch 1665 total_train_acc: 0.9827586206896551 loss: 0.14495251327753067\n",
      "epoch 1666 total_train_acc: 0.9858934169278997 loss: 0.1432802863419056\n",
      "epoch 1667 total_train_acc: 0.9858934169278997 loss: 0.14116976037621498\n",
      "epoch 1668 total_train_acc: 0.9839341692789969 loss: 0.14763125032186508\n",
      "epoch 1669 total_train_acc: 0.9831504702194357 loss: 0.14241959527134895\n",
      "epoch 1670 total_train_acc: 0.9827586206896551 loss: 0.15424994751811028\n",
      "epoch 1671 total_train_acc: 0.9835423197492164 loss: 0.15672198683023453\n",
      "epoch 1672 total_train_acc: 0.9815830721003135 loss: 0.15662118047475815\n",
      "epoch 1673 total_train_acc: 0.981974921630094 loss: 0.1502627357840538\n",
      "epoch 1674 total_train_acc: 0.9839341692789969 loss: 0.15111589804291725\n",
      "epoch 1675 total_train_acc: 0.9831504702194357 loss: 0.1469087339937687\n",
      "epoch 1676 total_train_acc: 0.9831504702194357 loss: 0.1568678431212902\n",
      "epoch 1677 total_train_acc: 0.9835423197492164 loss: 0.15207690745592117\n",
      "epoch 1678 total_train_acc: 0.9835423197492164 loss: 0.14064070954918861\n",
      "epoch 1679 total_train_acc: 0.9835423197492164 loss: 0.1571992002427578\n",
      "epoch 1680 total_train_acc: 0.9855015673981191 loss: 0.14111729711294174\n",
      "epoch 1681 total_train_acc: 0.9827586206896551 loss: 0.15794019028544426\n",
      "epoch 1682 total_train_acc: 0.9866771159874608 loss: 0.1391851268708706\n",
      "epoch 1683 total_train_acc: 0.987460815047022 loss: 0.13950928673148155\n",
      "epoch 1684 total_train_acc: 0.9839341692789969 loss: 0.1553225815296173\n",
      "epoch 1685 total_train_acc: 0.9866771159874608 loss: 0.14461948350071907\n",
      "epoch 1686 total_train_acc: 0.9827586206896551 loss: 0.16375000402331352\n",
      "epoch 1687 total_train_acc: 0.9823667711598746 loss: 0.15331927314400673\n",
      "epoch 1688 total_train_acc: 0.984717868338558 loss: 0.15679926052689552\n",
      "epoch 1689 total_train_acc: 0.9858934169278997 loss: 0.15716414153575897\n",
      "epoch 1690 total_train_acc: 0.9835423197492164 loss: 0.14748472720384598\n",
      "epoch 1691 total_train_acc: 0.9851097178683386 loss: 0.14269909635186195\n",
      "epoch 1692 total_train_acc: 0.9851097178683386 loss: 0.14120670780539513\n",
      "epoch 1693 total_train_acc: 0.9862852664576802 loss: 0.14892401546239853\n",
      "epoch 1694 total_train_acc: 0.9835423197492164 loss: 0.1501266025006771\n",
      "epoch 1695 total_train_acc: 0.9831504702194357 loss: 0.14728587120771408\n",
      "epoch 1696 total_train_acc: 0.9827586206896551 loss: 0.1578678973019123\n",
      "epoch 1697 total_train_acc: 0.9796238244514106 loss: 0.16456355899572372\n",
      "epoch 1698 total_train_acc: 0.9835423197492164 loss: 0.15480937063694\n",
      "epoch 1699 total_train_acc: 0.9855015673981191 loss: 0.1481257975101471\n",
      "epoch 1700 total_train_acc: 0.9843260188087775 loss: 0.13486752659082413\n",
      "epoch 1701 total_train_acc: 0.9839341692789969 loss: 0.14193742349743843\n",
      "epoch 1702 total_train_acc: 0.9843260188087775 loss: 0.1495165191590786\n",
      "epoch 1703 total_train_acc: 0.984717868338558 loss: 0.15532261505723\n",
      "epoch 1704 total_train_acc: 0.9827586206896551 loss: 0.13981493934988976\n",
      "epoch 1705 total_train_acc: 0.9839341692789969 loss: 0.14729409664869308\n",
      "epoch 1706 total_train_acc: 0.9815830721003135 loss: 0.1489056721329689\n",
      "epoch 1707 total_train_acc: 0.9843260188087775 loss: 0.14496754109859467\n",
      "epoch 1708 total_train_acc: 0.9843260188087775 loss: 0.14684314280748367\n",
      "epoch 1709 total_train_acc: 0.9839341692789969 loss: 0.1487341672182083\n",
      "epoch 1710 total_train_acc: 0.9855015673981191 loss: 0.14686084166169167\n",
      "epoch 1711 total_train_acc: 0.9862852664576802 loss: 0.14541899040341377\n",
      "epoch 1712 total_train_acc: 0.9843260188087775 loss: 0.14671137556433678\n",
      "epoch 1713 total_train_acc: 0.9815830721003135 loss: 0.1565515324473381\n",
      "epoch 1714 total_train_acc: 0.9823667711598746 loss: 0.14045190438628197\n",
      "epoch 1715 total_train_acc: 0.9843260188087775 loss: 0.1430765762925148\n",
      "epoch 1716 total_train_acc: 0.9815830721003135 loss: 0.15204916894435883\n",
      "epoch 1717 total_train_acc: 0.9815830721003135 loss: 0.14644109457731247\n",
      "epoch 1718 total_train_acc: 0.9831504702194357 loss: 0.15550243109464645\n",
      "epoch 1719 total_train_acc: 0.9807993730407524 loss: 0.15213079750537872\n",
      "epoch 1720 total_train_acc: 0.9858934169278997 loss: 0.14198680967092514\n",
      "epoch 1721 total_train_acc: 0.9843260188087775 loss: 0.14127759262919426\n",
      "epoch 1722 total_train_acc: 0.9831504702194357 loss: 0.1470421589910984\n",
      "epoch 1723 total_train_acc: 0.9858934169278997 loss: 0.1456800065934658\n",
      "epoch 1724 total_train_acc: 0.9851097178683386 loss: 0.14610571786761284\n",
      "epoch 1725 total_train_acc: 0.981974921630094 loss: 0.1514841578900814\n",
      "epoch 1726 total_train_acc: 0.9851097178683386 loss: 0.14280138164758682\n",
      "epoch 1727 total_train_acc: 0.9855015673981191 loss: 0.1560966596007347\n",
      "epoch 1728 total_train_acc: 0.9843260188087775 loss: 0.13785215094685555\n",
      "epoch 1729 total_train_acc: 0.9831504702194357 loss: 0.1465221382677555\n",
      "epoch 1730 total_train_acc: 0.987460815047022 loss: 0.13684532046318054\n",
      "epoch 1731 total_train_acc: 0.9827586206896551 loss: 0.14577166736125946\n",
      "epoch 1732 total_train_acc: 0.9835423197492164 loss: 0.13946321606636047\n",
      "epoch 1733 total_train_acc: 0.9815830721003135 loss: 0.14332127943634987\n",
      "epoch 1734 total_train_acc: 0.9858934169278997 loss: 0.1394447311758995\n",
      "epoch 1735 total_train_acc: 0.9858934169278997 loss: 0.1389506496489048\n",
      "epoch 1736 total_train_acc: 0.9843260188087775 loss: 0.14079976081848145\n",
      "epoch 1737 total_train_acc: 0.9843260188087775 loss: 0.1396522894501686\n",
      "epoch 1738 total_train_acc: 0.9831504702194357 loss: 0.1526080146431923\n",
      "epoch 1739 total_train_acc: 0.9843260188087775 loss: 0.1408122517168522\n",
      "epoch 1740 total_train_acc: 0.984717868338558 loss: 0.14172673597931862\n",
      "epoch 1741 total_train_acc: 0.9851097178683386 loss: 0.13813089951872826\n",
      "epoch 1742 total_train_acc: 0.9843260188087775 loss: 0.14841769635677338\n",
      "epoch 1743 total_train_acc: 0.9831504702194357 loss: 0.1469663269817829\n",
      "epoch 1744 total_train_acc: 0.9823667711598746 loss: 0.1507643572986126\n",
      "epoch 1745 total_train_acc: 0.9839341692789969 loss: 0.1500803343951702\n",
      "epoch 1746 total_train_acc: 0.9855015673981191 loss: 0.13079118728637695\n",
      "epoch 1747 total_train_acc: 0.9831504702194357 loss: 0.15091309696435928\n",
      "epoch 1748 total_train_acc: 0.9835423197492164 loss: 0.14503150060772896\n",
      "epoch 1749 total_train_acc: 0.981974921630094 loss: 0.13939634338021278\n",
      "epoch 1750 total_train_acc: 0.984717868338558 loss: 0.15041986107826233\n",
      "epoch 1751 total_train_acc: 0.9831504702194357 loss: 0.15382131189107895\n",
      "epoch 1752 total_train_acc: 0.9851097178683386 loss: 0.144572414457798\n",
      "epoch 1753 total_train_acc: 0.9827586206896551 loss: 0.1469762809574604\n",
      "epoch 1754 total_train_acc: 0.9843260188087775 loss: 0.14285960420966148\n",
      "epoch 1755 total_train_acc: 0.9843260188087775 loss: 0.14201753586530685\n",
      "epoch 1756 total_train_acc: 0.9851097178683386 loss: 0.14461326599121094\n",
      "epoch 1757 total_train_acc: 0.9851097178683386 loss: 0.1382121704518795\n",
      "epoch 1758 total_train_acc: 0.984717868338558 loss: 0.13931646198034286\n",
      "epoch 1759 total_train_acc: 0.9827586206896551 loss: 0.14227695018053055\n",
      "epoch 1760 total_train_acc: 0.9835423197492164 loss: 0.1495506763458252\n",
      "epoch 1761 total_train_acc: 0.9835423197492164 loss: 0.14556934684515\n",
      "epoch 1762 total_train_acc: 0.9870689655172413 loss: 0.14433187618851662\n",
      "epoch 1763 total_train_acc: 0.9839341692789969 loss: 0.14757024869322777\n",
      "epoch 1764 total_train_acc: 0.9831504702194357 loss: 0.1565045863389969\n",
      "epoch 1765 total_train_acc: 0.9843260188087775 loss: 0.1444927081465721\n",
      "epoch 1766 total_train_acc: 0.9878526645768025 loss: 0.14201564341783524\n",
      "epoch 1767 total_train_acc: 0.9835423197492164 loss: 0.1535997986793518\n",
      "epoch 1768 total_train_acc: 0.9835423197492164 loss: 0.15487530827522278\n",
      "epoch 1769 total_train_acc: 0.9839341692789969 loss: 0.14394908770918846\n",
      "epoch 1770 total_train_acc: 0.9862852664576802 loss: 0.1390843614935875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7396/1250540139.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mviz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvizx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'trainloss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'append'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'trainloss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     viz2.line([float(optimizer.state_dict()['param_groups'][0]['lr'])],[vizx],\\\n\u001b[0m\u001b[0;32m     52\u001b[0m         win='lr', update='append',opts=dict(title='lr'))\n\u001b[0;32m     53\u001b[0m     viz3.line([float(total_train_acc)],[vizx],\\\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\visdom\\__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\visdom\\__init__.py\u001b[0m in \u001b[0;36mline\u001b[1;34m(self, Y, X, win, env, opts, update, name)\u001b[0m\n\u001b[0;32m   1712\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'F'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1714\u001b[1;33m         return self.scatter(X=linedata, Y=labels, opts=opts, win=win, env=env,\n\u001b[0m\u001b[0;32m   1715\u001b[0m                             update=update, name=name)\n\u001b[0;32m   1716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\visdom\\__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\visdom\\__init__.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, X, Y, win, env, opts, update, name)\u001b[0m\n\u001b[0;32m   1638\u001b[0m             \u001b[0mendpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'update'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1640\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_send\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1642\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mpytorch_wrap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\visdom\\__init__.py\u001b[0m in \u001b[0;36m_send\u001b[1;34m(self, msg, endpoint, quiet, from_log, create)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m             return self._handle_post(\n\u001b[0m\u001b[0;32m    709\u001b[0m                 \"{0}:{1}{2}/{3}\".format(self.server, self.port,\n\u001b[0;32m    710\u001b[0m                                         self.base_url, endpoint),\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\visdom\\__init__.py\u001b[0m in \u001b[0;36m_handle_post\u001b[1;34m(self, url, data)\u001b[0m\n\u001b[0;32m    675\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \"\"\"\n\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'POST'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1346\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # 训练过程可视化，hiddenlayer\n",
    "# history = h.History()\n",
    "# canvas = h.Canvas()\n",
    "\n",
    "# 设置使用的训练设备\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "net = net.to(device)\n",
    "# 加载数据，设置优化器\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=1000,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100,shuffle=True)\n",
    "optimizer = torch.optim.Adam(net.parameters(),\n",
    "        lr=0.0002)\n",
    "lr_schedule = torch.optim.lr_scheduler.StepLR(\\\n",
    "        optimizer, 400, gamma=0.5, last_epoch=-1)\n",
    "total_test_acc = 0\n",
    "total_test_correct = 0\n",
    "totaltest = 0\n",
    "vizx = 0\n",
    "# 开始迭代\n",
    "for epoch in range(50000):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    curr_total_correct = 0\n",
    "    i = 0\n",
    "    net.train()\n",
    "    for batch in train_loader: # Get Batch\n",
    "        # batch = next(iter(train_loader)) # Get Batch\n",
    "        i+=1\n",
    "        images, labels = batch\n",
    "        # 数据和标签转为所需数据类型\n",
    "        images = images.to(torch.float32)\n",
    "        labels = labels.long()\n",
    "        preds = net(images.to(device)) # Pass Batch\n",
    "        trainloss = F.cross_entropy(preds.to(device), labels.to(device)) # Calculate Loss\n",
    "        optimizer.zero_grad()\n",
    "        trainloss.backward() # Calculate Gradients\n",
    "        optimizer.step() # Update Weights\n",
    "        total_loss += trainloss.item()\n",
    "        # total_correct += get_num_correct(preds.to(device), labels.to(device))\n",
    "        curr_total_correct = (get_num_correct(preds.to(device), labels.to(device)))\n",
    "        total_correct += curr_total_correct\n",
    "    total_train_acc = total_correct/(trainlabel.size)\n",
    "    total_correct = 0\n",
    "\n",
    "        # 可视化\n",
    "    vizx+=1\n",
    "    viz.line([float(trainloss)],[vizx],win='trainloss', update='append',opts=dict(title='trainloss'))\n",
    "    \n",
    "    viz2.line([float(optimizer.state_dict()['param_groups'][0]['lr'])],[vizx],\\\n",
    "        win='lr', update='append',opts=dict(title='lr'))\n",
    "    viz3.line([float(total_train_acc)],[vizx],\\\n",
    "        win='train_acc', update='append',opts=dict(title='train_acc'))\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    for testemgdatas, testemglabels in test_loader: # Get Batch\n",
    "        testemgdatas = testemgdatas.to(torch.float32)\n",
    "        testemglabels = testemglabels.long()\n",
    "        predstest = net(testemgdatas.to(device))\n",
    "        testloss = F.cross_entropy(predstest.to(device), testemglabels.to(device)) # Calculate Loss\n",
    "        curr_test_correct = (get_num_correct(predstest.to(device), testemglabels.to(device)))\n",
    "        total += testemglabels.size(0)\n",
    "        total_test_correct += curr_test_correct\n",
    "        # totaltest += testemglabels.size(0)\n",
    "    # total_test_acc = total_test_correct/(trainlabel.size)\n",
    "    total_test_acc = total_test_correct/total\n",
    "    \n",
    "    viz1.line([float(testloss)],[vizx],win='testloss', update='append',opts=dict(title='testloss'))\n",
    "    viz4.line([float(total_test_acc)],[vizx],\\\n",
    "        win='test_acc', update='append',opts=dict(title='test_acc'))\n",
    "    total_test_correct = 0\n",
    "\n",
    "    print(\n",
    "        \"epoch\", epoch, \n",
    "        \"total_train_acc:\", total_train_acc, \n",
    "        \"loss:\", total_loss\n",
    "    )\n",
    "   # 更新学习率\n",
    "    lr_schedule.step()\n",
    "   # 定期保存\n",
    "    if epoch%200 == 0:\n",
    "        timeForSave = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "        checkpointPath = ckpDir+'c6_ep_'+str(epoch)+'_'+timeForSave+'.pth'\n",
    "        state = {'model': net.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch}\n",
    "        torch.save(state, checkpointPath)\n",
    "\n",
    "torch.save(net.state_dict(),'./cnnet.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def openset_fuxin(dataloader, netC):\n",
    "#     openset_scores = []\n",
    "#     for i, (images, labels) in enumerate(dataloader):\n",
    "#         images = Variable(images, volatile=True)\n",
    "#         logits = netC(images)\n",
    "#         augmented_logits = F.pad(logits, pad=(0,1))\n",
    "#         # The implicit K+1th class (the open set class) is computed\n",
    "#         #  by assuming an extra linear output with constant value 0\n",
    "#         preds = F.softmax(augmented_logits)\n",
    "#         #preds = augmented_logits\n",
    "#         prob_unknown = preds[:, -1]\n",
    "#         prob_known = preds[:, :-1].max(dim=1)[0]\n",
    "#         prob_open = prob_unknown - prob_known\n",
    "\n",
    "#         openset_scores.extend(prob_open.data.cpu().numpy())\n",
    "#     return np.array(openset_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重新加载模型\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=32 * 4 * 34, out_features=128)\n",
    "        self.out = nn.Linear(in_features=128, out_features=6)\n",
    "        self.dr1 = nn.Dropout2d(0.2)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        # t = self.dr1(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 32 * 4 * 34)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.dr1(t)\n",
    "\n",
    "        # (5) output layer\n",
    "        t = self.out(t)\n",
    "\n",
    "        return t\n",
    "        \n",
    "net_eval = Network()\n",
    "\n",
    "checkpoint_eval = torch.load('./ckp/c6c6_ep_1600_2021_12_05_16_38_46.pth')\n",
    "net_eval.load_state_dict(checkpoint_eval['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "[tensor([[[[0.0024, 0.0024, 0.0098, 0.0024, 0.0024, 0.0024, 0.0024, 0.0537,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0024, 0.0024, 0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.0488,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0024, 0.0024, 0.0122, 0.0024, 0.0024, 0.0024, 0.0024, 0.0415,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0024, 0.0024, 0.0073, 0.0024, 0.0024, 0.0024, 0.0024, 0.0439,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0439,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0513,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0562,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0073, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0586,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0122, 0.0024, 0.0073, 0.0024, 0.0024, 0.0024, 0.0024, 0.0610,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0098, 0.0024, 0.0098, 0.0024, 0.0024, 0.0024, 0.0024, 0.0757,\n",
      "           0.0073, 0.0024],\n",
      "          [0.0073, 0.0024, 0.0049, 0.0024, 0.0024, 0.0024, 0.0024, 0.0977,\n",
      "           0.0098, 0.0024],\n",
      "          [0.0122, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.1050,\n",
      "           0.0049, 0.0073],\n",
      "          [0.0122, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.1001,\n",
      "           0.0024, 0.0073],\n",
      "          [0.0122, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0049, 0.0977,\n",
      "           0.0024, 0.0073],\n",
      "          [0.0098, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0928,\n",
      "           0.0024, 0.0049],\n",
      "          [0.0073, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0049, 0.0879,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0098, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0098, 0.0879,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0122, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0146, 0.0903,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0171, 0.0024, 0.0073, 0.0024, 0.0024, 0.0024, 0.0122, 0.0952,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0342, 0.0024, 0.0098, 0.0024, 0.0024, 0.0024, 0.0146, 0.1489,\n",
      "           0.0122, 0.0024],\n",
      "          [0.0391, 0.0024, 0.0073, 0.0024, 0.0024, 0.0024, 0.0171, 0.1978,\n",
      "           0.0146, 0.0024],\n",
      "          [0.0415, 0.0024, 0.0049, 0.0024, 0.0024, 0.0024, 0.0342, 0.2246,\n",
      "           0.0098, 0.0024],\n",
      "          [0.0366, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0415, 0.2295,\n",
      "           0.0049, 0.0024],\n",
      "          [0.0317, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0391, 0.2148,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0220, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0317, 0.1904,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0171, 0.0024, 0.0049, 0.0024, 0.0024, 0.0024, 0.0244, 0.1709,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0220, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0244, 0.1660,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0293, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0415, 0.1636,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0366, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0684, 0.1855,\n",
      "           0.0073, 0.0049],\n",
      "          [0.0391, 0.0024, 0.0098, 0.0024, 0.0024, 0.0024, 0.0732, 0.2319,\n",
      "           0.0122, 0.0073],\n",
      "          [0.0391, 0.0024, 0.0317, 0.0024, 0.0024, 0.0024, 0.0708, 0.2979,\n",
      "           0.0073, 0.0122],\n",
      "          [0.0391, 0.0024, 0.0488, 0.0024, 0.0024, 0.0024, 0.0684, 0.3271,\n",
      "           0.0024, 0.0122],\n",
      "          [0.0366, 0.0024, 0.0586, 0.0024, 0.0024, 0.0024, 0.0610, 0.3198,\n",
      "           0.0024, 0.0122],\n",
      "          [0.0317, 0.0024, 0.0537, 0.0024, 0.0024, 0.0024, 0.0513, 0.2954,\n",
      "           0.0024, 0.0098],\n",
      "          [0.0269, 0.0024, 0.0464, 0.0024, 0.0024, 0.0024, 0.0415, 0.2612,\n",
      "           0.0024, 0.0098],\n",
      "          [0.0244, 0.0024, 0.0366, 0.0024, 0.0024, 0.0024, 0.0439, 0.2271,\n",
      "           0.0024, 0.0049],\n",
      "          [0.0244, 0.0024, 0.0293, 0.0024, 0.0024, 0.0024, 0.0464, 0.2100,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0293, 0.0024, 0.0244, 0.0024, 0.0024, 0.0024, 0.0415, 0.1929,\n",
      "           0.0098, 0.0024],\n",
      "          [0.0293, 0.0024, 0.0195, 0.0024, 0.0024, 0.0024, 0.0366, 0.1733,\n",
      "           0.0146, 0.0024],\n",
      "          [0.0244, 0.0024, 0.0244, 0.0024, 0.0024, 0.0024, 0.0439, 0.1904,\n",
      "           0.0122, 0.0024]]]], dtype=torch.float64), tensor([9], dtype=torch.int16)]\n"
     ]
    }
   ],
   "source": [
    "# 自定义数据集类\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图片转换为Tensor,归一化至[0,1]\n",
    "])\n",
    "\n",
    "class EMGDataset(Dataset):\n",
    " \n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transforms = transform\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        emgData = self.data[index,:,:,:]\n",
    "        emgData = np.squeeze(emgData)#似乎不应该压缩了\n",
    "        emglabel = self.label[index]\n",
    "        emglabel = emglabel.astype(np.int16)\n",
    "        emgData = self.transforms(emgData)      \n",
    "        \n",
    "        return emgData,emglabel\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    " \n",
    " \n",
    "# if __name__ == '__main__':\n",
    "dataarray = np.load('../data/OpenSetDataSet.npy',allow_pickle=True)\n",
    "CNNdataset = dataarray.item()\n",
    "print(type(CNNdataset))\n",
    "opensetdata = CNNdataset['X_oo']\n",
    "opensetlabel = CNNdataset['Y_oo']\n",
    "# trainlabel = CNNdataset['Ytrain']\n",
    "# testdata = CNNdataset['Xtest']\n",
    "# testlabel = CNNdataset['Ytest']\n",
    "# # print(trainlabel[:,0])\n",
    "\n",
    "opensetlabel = opensetlabel[:,0]\n",
    "# testlabel = testlabel[:,0]\n",
    "# print(type(trainlabel))\n",
    "open_set = EMGDataset(opensetdata, opensetlabel)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, pin_memory=True,\n",
    "#                                             num_workers=3)\n",
    "openset_loader = torch.utils.data.DataLoader(open_set, batch_size=1,shuffle=True)\n",
    "sample = next(iter(openset_loader))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n",
      "C:\\Users\\cwdbo\\AppData\\Local\\Temp/ipykernel_7396/609198364.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  preds = F.softmax(augmented_logits)\n",
      "C:\\Users\\cwdbo\\AppData\\Local\\Temp/ipykernel_7396/609198364.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  preds2 = F.softmax(augmented_logits2)\n"
     ]
    }
   ],
   "source": [
    "openset_loader = torch.utils.data.DataLoader(open_set, batch_size=1,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1,shuffle=True)\n",
    "openset_scores = []\n",
    "openset_scores2 = []\n",
    "# 可视化\n",
    "viz6 = Visdom()\n",
    "viz7 = Visdom()\n",
    "vizx = 0\n",
    "for opensetdata, opensetlabel in openset_loader: # Get Batch\n",
    "        opensetdata = opensetdata.to(torch.float32)\n",
    "        opensetlabel = opensetlabel.long()\n",
    "        logits = net_eval(opensetdata)\n",
    "        augmented_logits = F.pad(logits, pad=(0,1))\n",
    "        preds = F.softmax(augmented_logits)\n",
    "        prob_unknown = preds[:, -1]\n",
    "        prob_known = preds[:, :-1].max(dim=1)[0]\n",
    "        prob_open = prob_unknown - prob_known\n",
    "        openset_scores.extend(prob_open.data.cpu().numpy())\n",
    "        prob_openforvis = prob_open.float()\n",
    "        viz6.line([float(prob_openforvis)],[vizx],\\\n",
    "                win='openscore', update='append',opts=dict(title='openscore for unknown'))\n",
    "        vizx += 1\n",
    "\n",
    "vizx = 0\n",
    "for testemgdatas, testemglabels in test_loader: # Get Batch\n",
    "        testemgdatas = testemgdatas.to(torch.float32)\n",
    "        testemglabels = testemglabels.long()\n",
    "        logits2 = net_eval(testemgdatas)\n",
    "        augmented_logits2 = F.pad(logits2, pad=(0,1))\n",
    "        preds2 = F.softmax(augmented_logits2)\n",
    "        prob_unknown2 = preds2[:, -1]\n",
    "        prob_known2 = preds2[:, :-1].max(dim=1)[0]\n",
    "        prob_open2 = prob_unknown2 - prob_known2\n",
    "        openset_scores2.extend(prob_open2.data.cpu().numpy())\n",
    "        prob_openforvis2 = prob_open2.float()\n",
    "        viz7.line([float(prob_openforvis2)],[vizx],\\\n",
    "                win='openscore2', update='append',opts=dict(title='openscore for known'))\n",
    "        vizx += 1\n",
    "# for i, (images, labels) in enumerate(dataloader):\n",
    "#     images = Variable(images, volatile=True)\n",
    "#     logits = netC(images)\n",
    "#     augmented_logits = F.pad(logits, pad=(0,1))\n",
    "#     # The implicit K+1th class (the open set class) is computed\n",
    "#     #  by assuming an extra linear output with constant value 0\n",
    "#     preds = F.softmax(augmented_logits)\n",
    "#     #preds = augmented_logits\n",
    "#     prob_unknown = preds[:, -1]\n",
    "#     prob_known = preds[:, :-1].max(dim=1)[0]\n",
    "#     prob_open = prob_unknown - prob_known\n",
    "#     openset_scores.extend(prob_open.data.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "122afd33e14e141e8feafe6109b3cf33c81901f42114774f6f58cb0f50546406"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('ml2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
