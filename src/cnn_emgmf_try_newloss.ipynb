{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "# from sklearn.preprocessing import label_binarize\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn import tree \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import scipy.io as scio\n",
    "# import hiddenlayer as h\n",
    "from visdom import Visdom\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "\n",
    "import datetime\n",
    "import os \n",
    "\n",
    "import sys\n",
    "from utils.reuse import *\n",
    "from utils.networks import *\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_03_29_19_46_20\n"
     ]
    }
   ],
   "source": [
    "# 以下是检查点路径\n",
    "# 请在当前环境下 CMD 输入python -m visdom.server 或 visdom 启动监视器\n",
    "# 数据处理现在已移至 emgDataprocess.ipynb\n",
    "model_Dir = './/model//emgmk_cnn_0323_nl//'\n",
    "if not os.path.exists(model_Dir):\n",
    "    os.makedirs(model_Dir)\n",
    "\n",
    "ckpDir = model_Dir+'ckp//'\n",
    "if not os.path.exists(ckpDir):\n",
    "    os.makedirs(ckpDir)\n",
    "\n",
    "ckpDir_auc = './/ckp//emgmk_cnn_0323_nl//auc//'\n",
    "if not os.path.exists(ckpDir_auc):\n",
    "    os.makedirs(ckpDir_auc)\n",
    "\n",
    "def get_current_time():\n",
    "    return datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "print(get_current_time())\n",
    "\n",
    "timeForSave = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visdom has started\n"
     ]
    }
   ],
   "source": [
    "# 以下是 visdom 监视窗口初始化，实现每次启用时重新加载，这里只写了 NameError 以防其他错误不能被发现\n",
    "class visdom_account:\n",
    "    def __init__(self):    \n",
    "        self.port = 8097\n",
    "        self.server = \"http://localhost\"\n",
    "        self.base_url = \"/\"\n",
    "        self.username = \"admin\"\n",
    "        self.passward = \"1234\"\n",
    "        self.evns = \"train4\"\n",
    "viz_acnt = visdom_account()\n",
    "vislogDir = model_Dir+'vislog//'\n",
    "if not os.path.exists(vislogDir):\n",
    "    os.makedirs(vislogDir)\n",
    "def viz_init():\n",
    "    try:\n",
    "        viz\n",
    "    except NameError:\n",
    "        viz = Visdom(env=viz_acnt.evns,log_to_filename=vislogDir+'vislog_'+timeForSave)\n",
    "        print('visdom has started')\n",
    "    else:\n",
    "        viz.close()\n",
    "        del viz\n",
    "        print('last visdom session closed')\n",
    "        viz = Visdom(env=viz_acnt.evns,log_to_filename=vislogDir+'vislog_'+timeForSave)\n",
    "        print('visdom has restarted')\n",
    "    return viz\n",
    "viz = viz_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [1, 128, 1, 3]           1,280\n",
      "            Linear-2                   [1, 128]          49,280\n",
      "            Linear-3                   [1, 128]          16,512\n",
      "            Linear-4                     [1, 3]             387\n",
      "================================================================\n",
      "Total params: 67,459\n",
      "Trainable params: 67,459\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.26\n",
      "Estimated Total Size (MB): 0.26\n",
      "----------------------------------------------------------------\n",
      "Outputshape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# 自定义神经网络,CNN\n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "# hdreshape = 32*1*3\n",
    "hdreshape = 2\n",
    "hdlayer_1 = 16\n",
    "hdlayer_2 = 16\n",
    "hdlayer_3 = 256\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=128,\\\n",
    "             kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32,\\\n",
    "             kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, \\\n",
    "            kernel_size=3, padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=128*1*3, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=128)\n",
    "        # self.fc3 = nn.Linear(in_features=hdlayer_2, out_features=hdlayer_3)\n",
    "        self.out = nn.Linear(in_features=128, out_features=3)\n",
    "        self.dr1 = nn.Dropout2d(0.2)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "        # t = t.reshape(1,1,3)\n",
    "        # t = t.unsqueeze(0)\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        # t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        # t = self.conv2(t)\n",
    "        # t = F.relu(t)\n",
    "        # t = self.dr1(t)\n",
    "        # t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 128*1*3)\n",
    "        # t = t.flatten(start_dim=0)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        # t = self.fc3(t)\n",
    "        # t = F.relu(t)\n",
    "        # t = self.dr1(t)\n",
    "\n",
    "        # (5) output layer\n",
    "        t = self.out(t)\n",
    "\n",
    "        return t\n",
    "\n",
    "net = Network()\n",
    "# 打印网络，检查输入输出 shape是否正确\n",
    "# print(net)\n",
    "samplebatchsize = 1\n",
    "summary(net,(1,1,3),batch_size = samplebatchsize,device = \"cpu\")\n",
    "sampleInput = torch.randn(samplebatchsize,1,1,3).requires_grad_(True)\n",
    "sampleOutput = net(sampleInput)\n",
    "print('Outputshape:',sampleOutput.shape)\n",
    "# framevision = make_dot(sampleOutput, params=dict(list(net.named_parameters()) + [('x',sampleInput)]))\n",
    "# framevision.format = \"png\"\n",
    "# framevision.direcory = \"./\"\n",
    "# framevision.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "(81, 1) (20, 1)\n",
      "(76, 10) (15, 1)\n",
      "torch.Size([20, 2]) torch.Size([81, 2]) torch.Size([81, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataarray = np.load('../data/220319emgsk/withTrueTime/EMGSKdata-220323.npy',allow_pickle=True)\n",
    "CNNdataset = dataarray.item()\n",
    "print(type(CNNdataset))\n",
    "# 加载自变量：\n",
    "data_cyc = CNNdataset['data_cyc']\n",
    "data_emg_rf_l = CNNdataset['data_emg_rf_l']\n",
    "data_emg_lh_l = CNNdataset['data_emg_lh_l']\n",
    "data_mf_rf_l = CNNdataset['data_mf_rf_l']\n",
    "data_mf_bm_l = CNNdataset['data_mf_bm_l']\n",
    "data_ka_l = CNNdataset['data_ka_l']\n",
    "data_Ma = CNNdataset['data_Ma']\n",
    "data_Cv = CNNdataset['data_Cv']\n",
    "data_G_theta = CNNdataset['data_G_theta']\n",
    "data_M_Arf = CNNdataset['data_M_Arf']\n",
    "data_M_Abif = CNNdataset['data_M_Abif']\n",
    "data_time = CNNdataset['data_time']\n",
    "\n",
    "# 划分训练集与测试集\n",
    "def DataSpliter(data):\n",
    "    data_tr = []\n",
    "    data_te = []\n",
    "    for i in range(len(data)):\n",
    "        if (i+1)%5 == 0:\n",
    "            data_te.append(data[i,:])\n",
    "        else:\n",
    "            data_tr.append(data[i,:])\n",
    "    data_tr = np.array(data_tr)\n",
    "    data_te = np.array(data_te)\n",
    "    return data_tr, data_te\n",
    "tr_cyc,te_cyc = DataSpliter(data_cyc)\n",
    "print(tr_cyc.shape,te_cyc.shape)\n",
    "tr_mf_bm_l,te_mf_bm_l = DataSpliter(data_mf_bm_l)\n",
    "tr_emg_rf_l,te_emg_rf_l = DataSpliter(data_emg_rf_l)\n",
    "tr_emg_lh_l,te_emg_lh_l = DataSpliter(data_emg_lh_l)\n",
    "tr_mf_rf_l,te_mf_rf_l = DataSpliter(data_mf_rf_l)\n",
    "tr_ka_l,te_ka_l = DataSpliter(data_ka_l)\n",
    "# 新增部分，常量的部分数据也需要跟随划分成训练集和测试集\n",
    "tr_data_Ma,te_data_Ma = DataSpliter(data_Ma)\n",
    "tr_data_Cv,te_data_Cv = DataSpliter(data_Cv)\n",
    "tr_data_G_theta,te_data_G_theta = DataSpliter(data_G_theta)\n",
    "tr_data_M_Arf,te_data_M_Arf = DataSpliter(data_M_Arf)\n",
    "tr_data_M_Abif,te_data_M_Abif = DataSpliter(data_M_Abif)\n",
    "tr_data_time,te_data_time = DataSpliter(data_time)\n",
    "# 按需组合数据，这两函数其实可以合并成一个\n",
    "def Data_conbine(data_1,data_2):\n",
    "    data_train = []\n",
    "    for i in range(len(data_1)):\n",
    "        data_1_2 = np.hstack((data_1[i,:],data_2[i,:]))\n",
    "        data_1_2 = data_1_2.flatten()\n",
    "        data_train.append(data_1_2)\n",
    "    data_train = np.array(data_train)\n",
    "    return data_train\n",
    "\n",
    "def establish_multi_timestep_data(data_X,data_X_2,data_Y,expect_time_length):\n",
    "    dataset = []\n",
    "    dataset_y = []\n",
    "    if expect_time_length >0:\n",
    "        length = len(data_X)-expect_time_length\n",
    "        for i in range(length):\n",
    "            data_1 = data_X[i:i+expect_time_length,:]\n",
    "            data_1 = data_1.flatten()\n",
    "            data_2 = data_X_2[i:i+expect_time_length,:]\n",
    "            data_2 = data_2.flatten()\n",
    "            data_12 = np.hstack((data_1,data_2))\n",
    "            dataset.append(data_12)\n",
    "            dataset_y.append(data_Y[i+expect_time_length-1,:])\n",
    "        dataset = np.array(dataset)\n",
    "        dataset_y = np.array(dataset_y)\n",
    "    return dataset, dataset_y\n",
    "\n",
    "data_12_tr = Data_conbine(tr_cyc,tr_emg_rf_l)\n",
    "data_12_te = Data_conbine(te_cyc,te_emg_rf_l)\n",
    "data_13_tr = Data_conbine(tr_cyc,tr_emg_lh_l)\n",
    "data_13_te = Data_conbine(te_cyc,te_emg_lh_l)\n",
    "data_123_tr = Data_conbine(data_12_tr,tr_emg_lh_l)\n",
    "data_123_te = Data_conbine(data_12_te,te_emg_lh_l)\n",
    "data_45_tr_y = Data_conbine(tr_mf_rf_l,tr_mf_bm_l)\n",
    "data_45_te_y = Data_conbine(te_mf_rf_l,te_mf_bm_l)\n",
    "data_457_tr_y = Data_conbine(data_45_tr_y,tr_ka_l)\n",
    "data_457_te_y = Data_conbine(data_45_te_y,te_ka_l)\n",
    "\n",
    "data_13_tr_5t,data_13_tr_5t_y = establish_multi_timestep_data(tr_cyc,tr_emg_lh_l,tr_mf_bm_l,5)\n",
    "data_13_te_5t,data_13_te_5t_y = establish_multi_timestep_data(te_cyc,te_emg_lh_l,te_mf_bm_l,5)\n",
    "print(data_13_tr_5t.shape,data_13_te_5t_y.shape)\n",
    "# 转为 tensor 格式\n",
    "data_12_tr = torch.from_numpy(data_12_tr)\n",
    "data_12_te = torch.from_numpy(data_12_te)\n",
    "data_13_tr = torch.from_numpy(data_13_tr)\n",
    "data_13_te = torch.from_numpy(data_13_te)\n",
    "data_123_tr = torch.from_numpy(data_123_tr)\n",
    "data_123_te = torch.from_numpy(data_123_te)\n",
    "tr_mf_rf_l = torch.from_numpy(tr_mf_rf_l)\n",
    "te_mf_rf_l = torch.from_numpy(te_mf_rf_l)\n",
    "tr_mf_bm_l = torch.from_numpy(tr_mf_bm_l)\n",
    "te_mf_bm_l = torch.from_numpy(te_mf_bm_l)\n",
    "tr_ka_l = torch.from_numpy(tr_ka_l)\n",
    "te_ka_l = torch.from_numpy(te_ka_l)\n",
    "\n",
    "data_12_tr = data_12_tr.to(torch.float32)\n",
    "data_12_te = data_12_te.to(torch.float32)\n",
    "data_13_tr = data_13_tr.to(torch.float32)\n",
    "data_13_te = data_13_te.to(torch.float32)\n",
    "data_123_tr = data_123_tr.to(torch.float32)\n",
    "data_123_te = data_123_te.to(torch.float32)\n",
    "tr_mf_rf_l = tr_mf_rf_l.to(torch.float32)\n",
    "te_mf_rf_l = te_mf_rf_l.to(torch.float32)\n",
    "tr_mf_bm_l = tr_mf_bm_l.to(torch.float32)\n",
    "te_mf_bm_l = te_mf_bm_l.to(torch.float32)\n",
    "tr_ka_l = tr_ka_l.to(torch.float32)\n",
    "te_ka_l = te_ka_l.to(torch.float32)\n",
    "\n",
    "data_13_tr_5t = torch.from_numpy(data_13_tr_5t).to(torch.float32)\n",
    "data_13_te_5t = torch.from_numpy(data_13_te_5t).to(torch.float32)\n",
    "data_13_tr_5t_y = torch.from_numpy(data_13_tr_5t_y).to(torch.float32)\n",
    "data_13_te_5t_y = torch.from_numpy(data_13_te_5t_y).to(torch.float32)\n",
    "\n",
    "tr_data_Ma = torch.from_numpy(tr_data_Ma).to(torch.float32)\n",
    "te_data_Ma = torch.from_numpy(te_data_Ma).to(torch.float32)\n",
    "tr_data_Cv = torch.from_numpy(tr_data_Cv).to(torch.float32)\n",
    "te_data_Cv = torch.from_numpy(te_data_Cv).to(torch.float32)\n",
    "tr_data_G_theta = torch.from_numpy(tr_data_G_theta).to(torch.float32)\n",
    "te_data_G_theta = torch.from_numpy(te_data_G_theta).to(torch.float32)\n",
    "\n",
    "data_45_tr_y = torch.from_numpy(data_45_tr_y).to(torch.float32)\n",
    "data_45_te_y = torch.from_numpy(data_45_te_y).to(torch.float32)\n",
    "data_457_tr_y = torch.from_numpy(data_457_tr_y).to(torch.float32)\n",
    "data_457_te_y = torch.from_numpy(data_457_te_y).to(torch.float32)\n",
    "\n",
    "tr_data_M_Arf = torch.from_numpy(tr_data_M_Arf).to(torch.float32)\n",
    "te_data_M_Arf = torch.from_numpy(te_data_M_Arf).to(torch.float32)\n",
    "tr_data_M_Abif = torch.from_numpy(tr_data_M_Abif).to(torch.float32)\n",
    "te_data_M_Abif = torch.from_numpy(te_data_M_Abif).to(torch.float32)\n",
    "print(data_12_te.shape,data_13_tr.shape,data_123_tr.shape)\n",
    "\n",
    "# 需要一个指示前一时刻的标签，居然没注意到 cyc 每五取一后不再连续了，害\n",
    "data_index_tr = np.linspace(0,80,81)\n",
    "data_index_te = np.linspace(0,19,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<iterator object at 0x00000181C3F53B80>\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图片转换为Tensor,归一化至[0,1]\n",
    "])\n",
    "class EMGSK_Dataset(Dataset):\n",
    " \n",
    "    def __init__(self, data_x, data_y, data_Ma,data_Cv,data_G_theta,data_M_Arf,data_M_Abif):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        self.data_Ma = data_Ma\n",
    "        self.data_Cv = data_Cv\n",
    "        self.data_G_theta = data_G_theta\n",
    "        self.data_M_Arf = data_M_Arf\n",
    "        self.data_M_Abif = data_M_Abif\n",
    "\n",
    "        self.transforms = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample_x = self.data_x[index,:]\n",
    "        sample_y = self.data_y[index,:]\n",
    "        sample_Ma = self.data_Ma[index,:]\n",
    "        sample_Cv = self.data_Cv[index,:]\n",
    "        sample_G_theta = self.data_G_theta[index,:]\n",
    "        sample_M_Arf = self.data_M_Arf[index,:]\n",
    "        sample_M_Abif = self.data_M_Abif[index,:]\n",
    "\n",
    "        # for CNN\n",
    "        sample_x = sample_x.unsqueeze(0)\n",
    "        sample_x = sample_x.numpy()\n",
    "        sample_x = self.transforms(sample_x)\n",
    "\n",
    "        # kmmf_Data = self.data_sample[index,:,:]\n",
    "        # kmmfData = np.squeeze(kmmfData)\n",
    "        # kmmf_label = self.data_label[index]\n",
    "        # kmmf_label = kmmf_label.astype(np.int16)\n",
    "        # kmmf_Data = self.transforms(kmmf_Data)\n",
    "        # kmmf_label = self.transforms(kmmf_label)\n",
    "        # 一维数据用下面的这个就行\n",
    "        # emgData = torch.Tensor(emgData)     \n",
    "        return sample_x,sample_y,sample_Ma,sample_Cv,sample_G_theta\\\n",
    "            ,sample_M_Arf,sample_M_Abif\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "\n",
    "class EMGSK_nl_Dataset(Dataset):\n",
    " \n",
    "    def __init__(self, data_x, data_y, data_time, data_M_Arf, data_M_Abif,data_idx):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        self.data_angle = self.data_y[:,2]\n",
    "        self.data_time = data_time\n",
    "        self.data_M_Arf = data_M_Arf\n",
    "        self.data_M_Abif = data_M_Abif\n",
    "        self.data_idx = data_idx\n",
    "        self.transforms = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample_x = self.data_x[index,:]\n",
    "        sample_y = self.data_y[index,:]\n",
    "        sample_cyc = self.data_x[index,0]\n",
    "        sample_angle = self.data_angle[index]\n",
    "        sample_time = self.data_time[index,:]\n",
    "        sample_M_Arf = self.data_M_Arf[index,:]\n",
    "        sample_M_Abif = self.data_M_Abif[index,:]\n",
    "        sample_idx = self.data_idx[index]\n",
    "        # for CNN\n",
    "        sample_x = sample_x.unsqueeze(0)\n",
    "        sample_x = sample_x.numpy()\n",
    "        sample_x = self.transforms(sample_x)\n",
    "\n",
    "        # kmmf_Data = self.data_sample[index,:,:]\n",
    "        # kmmfData = np.squeeze(kmmfData)\n",
    "        # kmmf_label = self.data_label[index]\n",
    "        # kmmf_label = kmmf_label.astype(np.int16)\n",
    "        # kmmf_Data = self.transforms(kmmf_Data)\n",
    "        # kmmf_label = self.transforms(kmmf_label)\n",
    "        # 一维数据用下面的这个就行\n",
    "        # emgData = torch.Tensor(emgData)     \n",
    "        return sample_x,sample_y,sample_angle,sample_time,\\\n",
    "            sample_M_Arf,sample_M_Abif,sample_cyc,sample_idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "\n",
    "train_set = EMGSK_nl_Dataset(data_123_tr,data_457_tr_y,tr_data_time,\\\n",
    "    tr_data_M_Arf,tr_data_M_Abif,data_index_tr)\n",
    "test_set = EMGSK_nl_Dataset(data_123_te,data_457_te_y,te_data_time,\\\n",
    "    te_data_M_Arf,te_data_M_Abif,data_index_te)\n",
    "\n",
    "sample = iter(test_set)\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size=2,shuffle=False)\n",
    "# batch1 = iter(train_loader)\n",
    "# # dassy_2 = data_457_tr_y[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([2, 1, 1, 3]) tensor([[[[0.0000, 0.0239, 0.0078]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0290, 0.0082]]]]) \n",
      " y: torch.Size([2, 3]) tensor([[ 41.8424,  28.0413, -12.0472],\n",
      "        [ 42.0158,  28.2712, -11.5392]]) \n",
      " a: torch.Size([2]) tensor([-12.0472, -11.5392]) \n",
      " t: torch.Size([2, 1]) tensor([[0.2167],\n",
      "        [0.2258]], dtype=torch.float64) \n",
      " ma: torch.Size([2, 1]) tensor([[1.5534],\n",
      "        [1.5534]]) \n",
      " mb: torch.Size([2, 1]) tensor([[0.0436],\n",
      "        [0.0436]]) \n",
      " c: torch.Size([2]) tensor([0., 1.]) \n",
      " idx: torch.Size([2]) tensor([0., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# batch2 = next(batch1)\n",
    "# d_x,d_y,d_a,d_t,d_Ma,d_Mb,d_c,d_i= batch2\n",
    "# print('x:',d_x.shape,d_x,'\\n','y:',d_y.shape,d_y,'\\n',\\\n",
    "#     'a:',d_a.shape,d_a,'\\n','t:',d_t.shape,d_t,'\\n',\\\n",
    "#         'ma:',d_Ma.shape,d_Ma,'\\n','mb:',d_Mb.shape,d_Mb,\\\n",
    "#             '\\n','c:',d_c.shape,d_c,'\\n','idx:',d_i.shape,d_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建 loss 时需要注意所有的 tensor 操作都应该在 torch 工具包里面出现，\n",
    "# 当然了基本的加减乘除不用，参考：https://blog.csdn.net/bornfree5511/\n",
    "# article/details/104118437?utm_medium=distribute.pc_relevant.\n",
    "# none-task-blog-2~default~baidujs_title~default-0.pc_relevant_aa&spm\n",
    "# =1001.2101.3001.4242.1&utm_relevant_index=3，\n",
    "# 当然我已经将它复制到一个 excel 并转为 pdf 永久保存了，看哪个都一样\n",
    "class LossMSE2(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super(LossMSE2,self).__init__()\n",
    "        self.Loss_MSE = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self,s_ma,s_cv,s_g,s_m_ar,s_m_ab,preds,s_y):\n",
    "        p1,p2 = torch.chunk(preds,2,1)\n",
    "        new_loss = s_ma + s_cv + s_g - \\\n",
    "            (p1 * s_m_ar) - (p2 * s_m_ab)\n",
    "        old_loss = self.Loss_MSE(preds,s_y)\n",
    "        loss = new_loss+old_loss\n",
    "\n",
    "        return loss\n",
    "# 引入角度后的新损失函数\n",
    "class LossMSE3(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super(LossMSE3,self).__init__()\n",
    "        self.Loss_MSE = torch.nn.MSELoss()\n",
    "        self.pred_angle_list = [torch.tensor([],requires_grad=False) \\\n",
    "        for i in range(81)]\n",
    "        self.time_list = [torch.tensor([],requires_grad=False) \\\n",
    "        for i in range(81)]\n",
    "        self.vel_list = [torch.tensor([],requires_grad=False) \\\n",
    "        for i in range(81)]\n",
    "\n",
    "    def forward(self,preds,s_y,s_cyc,s_time,s_m_ar,s_m_ab,s_idx):\n",
    "        # p1,肌肉力 rf , p2,肌肉力 bm, p3, 关节角度\n",
    "        p1,p2,p3 = torch.chunk(preds,3,1)\n",
    "        # 角度转弧度\n",
    "        p3 = p3/360*2*np.pi\n",
    "        s_v = 0\n",
    "        s_a = 0\n",
    "        s_time.requires_grad_(False)\n",
    "        s_m_ar.requires_grad_(False)\n",
    "        s_m_ab.requires_grad_(False)\n",
    "        s_cyc.requires_grad_(False)\n",
    "        s_idx.requires_grad_(False)\n",
    "\n",
    "        if s_cyc == 0:\n",
    "            s_v = 0\n",
    "            s_a = 0\n",
    "            p_a = float(p3)\n",
    "            self.pred_angle_list[int(s_idx)] = p_a\n",
    "            s_tt = float(s_time)\n",
    "            self.time_list[int(s_idx)] = s_tt\n",
    "            s_vv = float(s_v)\n",
    "            self.vel_list[int(s_idx)] = s_vv\n",
    "        elif s_cyc == 1:\n",
    "            s_a = 0\n",
    "            p_a = float(p3)\n",
    "            self.pred_angle_list[int(s_idx)] = p_a\n",
    "            s_tt = float(s_time)\n",
    "            self.time_list[int(s_idx)] = s_tt\n",
    "            s_time_p = self.time_list[int(s_idx)-1]\n",
    "            s_angle_p = self.pred_angle_list[int(s_idx)-1]\n",
    "            s_v = (p3.to(device) - s_angle_p) / (s_time.to(device) - s_time_p)\n",
    "            s_vv = float(s_v)\n",
    "            self.vel_list[int(s_idx)] = s_vv\n",
    "        else:\n",
    "            # 当前值存入列表备用，同时取出前一刻角度值计算速度与加速度\n",
    "            p_a = float(p3)\n",
    "            self.pred_angle_list[int(s_idx)] = p_a\n",
    "            s_tt = float(s_time)\n",
    "            self.time_list[int(s_idx)] = s_tt\n",
    "            s_angle_p = self.pred_angle_list[int(s_idx)-1]\n",
    "            s_time_p = self.time_list[int(s_idx)-1]\n",
    "            s_v = (p3.to(device) - s_angle_p) / (s_time.to(device) - s_time_p)\n",
    "            s_vv = float(s_v)\n",
    "            self.vel_list[int(s_idx)] = s_vv\n",
    "            s_v_p = self.vel_list[int(s_idx)-1]\n",
    "            s_a = (s_v - s_v_p)/ (s_time.to(device) - s_time_p)\n",
    "\n",
    "\n",
    "        M_knee = 0.054596 *5.74*0.174*0.174\n",
    "        C_knee = 5.74*0.174*0.4*torch.sin(p3)\n",
    "        G_theta = 5.74*9.8*0.174*0.174*torch.cos(p3)\n",
    "\n",
    "\n",
    "        new_loss = M_knee * s_a + C_knee.to(device) * s_v+ G_theta.to(device) * p3.to(device) - \\\n",
    "            (p1.to(device) * s_m_ar.to(device)) - (p2.to(device) * s_m_ab.to(device))\n",
    "        old_loss = self.Loss_MSE(preds,s_y)\n",
    "        loss = new_loss+old_loss\n",
    "\n",
    "        return loss\n",
    "new_loss2 = LossMSE2()\n",
    "new_loss3 = LossMSE3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visdom has started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cwdbo\\AppData\\Local\\Temp/ipykernel_24292/1150572650.py:48: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 MSE_tr: 24261.860042791843\n",
      "epoch 2 MSE_tr: 22129.6528256741\n",
      "epoch 3 MSE_tr: 25354.823457233204\n",
      "epoch 4 MSE_tr: 28245.649229437637\n",
      "epoch 5 MSE_tr: 59465.619004380496\n",
      "epoch 6 MSE_tr: 40984.219520112194\n",
      "epoch 7 MSE_tr: 23945.26918948475\n",
      "epoch 8 MSE_tr: 24942.05997470343\n",
      "epoch 9 MSE_tr: 26573.743751867478\n",
      "epoch 10 MSE_tr: 35210.36351712608\n",
      "epoch 11 MSE_tr: 26236.039091289855\n",
      "epoch 12 MSE_tr: 21931.253702276448\n",
      "epoch 13 MSE_tr: 26411.74759148982\n",
      "epoch 14 MSE_tr: 17483.96287102579\n",
      "epoch 15 MSE_tr: 23627.53287026539\n",
      "epoch 16 MSE_tr: 25796.21892659911\n",
      "epoch 17 MSE_tr: 22275.91175798905\n",
      "epoch 18 MSE_tr: 48724.09046342755\n",
      "epoch 19 MSE_tr: 30437.519596731723\n",
      "epoch 20 MSE_tr: 31809.863926439277\n",
      "epoch 21 MSE_tr: 35551.8309792849\n",
      "epoch 22 MSE_tr: 34302.80655751379\n",
      "epoch 23 MSE_tr: 43663.93584658797\n",
      "epoch 24 MSE_tr: 32644.64947029519\n",
      "epoch 25 MSE_tr: 32962.055789187405\n",
      "epoch 26 MSE_tr: 25094.122859169678\n",
      "epoch 27 MSE_tr: 25526.57481274876\n",
      "epoch 28 MSE_tr: 25002.989213894256\n",
      "epoch 29 MSE_tr: 28700.095619850046\n",
      "epoch 30 MSE_tr: 26689.888176609245\n",
      "epoch 31 MSE_tr: 24566.917027463198\n",
      "epoch 32 MSE_tr: 21724.537558114487\n",
      "epoch 33 MSE_tr: 19726.892341072453\n",
      "epoch 34 MSE_tr: 22402.88312862918\n",
      "epoch 35 MSE_tr: 20949.712926840773\n",
      "epoch 36 MSE_tr: 22592.73142751731\n",
      "epoch 37 MSE_tr: 20231.147041393997\n",
      "epoch 38 MSE_tr: 22234.7798091279\n",
      "epoch 39 MSE_tr: 24081.495981944638\n",
      "epoch 40 MSE_tr: 19848.63966398431\n",
      "epoch 41 MSE_tr: 23162.159738828635\n",
      "epoch 42 MSE_tr: 28036.690906331147\n",
      "epoch 43 MSE_tr: 25296.209993427638\n",
      "epoch 44 MSE_tr: 23579.219270435588\n",
      "epoch 45 MSE_tr: 25587.112224291075\n",
      "epoch 46 MSE_tr: 23559.67973156792\n",
      "epoch 47 MSE_tr: 21129.49523693163\n",
      "epoch 48 MSE_tr: 21938.370291773717\n",
      "epoch 49 MSE_tr: 18486.44537323366\n",
      "epoch 50 MSE_tr: 19917.21133664873\n",
      "epoch 51 MSE_tr: 20485.873527658798\n",
      "epoch 52 MSE_tr: 19460.21939975641\n",
      "epoch 53 MSE_tr: 22209.79026311624\n",
      "epoch 54 MSE_tr: 23325.273230649113\n",
      "epoch 55 MSE_tr: 22650.64817180351\n",
      "epoch 56 MSE_tr: 28248.72498621813\n",
      "epoch 57 MSE_tr: 30496.00468466257\n",
      "epoch 58 MSE_tr: 24408.449336145106\n",
      "epoch 59 MSE_tr: 23742.90026990827\n",
      "epoch 60 MSE_tr: 19915.79727313939\n",
      "epoch 61 MSE_tr: 18118.71560977919\n",
      "epoch 62 MSE_tr: 21281.633667493556\n",
      "epoch 63 MSE_tr: 18617.93608457735\n",
      "epoch 64 MSE_tr: 25269.222562471008\n",
      "epoch 65 MSE_tr: 25234.17539522991\n",
      "epoch 66 MSE_tr: 22498.400477682324\n",
      "epoch 67 MSE_tr: 29196.66383566412\n",
      "epoch 68 MSE_tr: 21082.731159841696\n",
      "epoch 69 MSE_tr: 22537.529082785943\n",
      "epoch 70 MSE_tr: 19817.12119333209\n",
      "epoch 71 MSE_tr: 22134.61671641605\n",
      "epoch 72 MSE_tr: 20876.46727926855\n",
      "epoch 73 MSE_tr: 19445.238612213958\n",
      "epoch 74 MSE_tr: 19209.763590001858\n",
      "epoch 75 MSE_tr: 22926.86248427659\n",
      "epoch 76 MSE_tr: 22798.59853714323\n",
      "epoch 77 MSE_tr: 20420.313563743966\n",
      "epoch 78 MSE_tr: 34201.29851115594\n",
      "epoch 79 MSE_tr: 21813.657981799093\n",
      "epoch 80 MSE_tr: 22535.87905903155\n",
      "epoch 81 MSE_tr: 20053.060961807187\n",
      "epoch 82 MSE_tr: 19313.57795239759\n",
      "epoch 83 MSE_tr: 18134.40176858586\n",
      "epoch 84 MSE_tr: 19698.403514346865\n",
      "epoch 85 MSE_tr: 17921.927615891003\n",
      "epoch 86 MSE_tr: 18258.786468883693\n",
      "epoch 87 MSE_tr: 19294.736932338867\n",
      "epoch 88 MSE_tr: 18705.393267041916\n",
      "epoch 89 MSE_tr: 19198.980435602178\n",
      "epoch 90 MSE_tr: 18512.582793129797\n",
      "epoch 91 MSE_tr: 20381.1828092262\n",
      "epoch 92 MSE_tr: 22551.718262467028\n",
      "epoch 93 MSE_tr: 23635.975196355932\n",
      "epoch 94 MSE_tr: 22457.378998870143\n",
      "epoch 95 MSE_tr: 23138.326680374375\n",
      "epoch 96 MSE_tr: 21113.353728096343\n",
      "epoch 97 MSE_tr: 20237.897049989704\n",
      "epoch 98 MSE_tr: 20521.345186288076\n",
      "epoch 99 MSE_tr: 21343.913137945237\n",
      "epoch 100 MSE_tr: 23522.85403135184\n",
      "epoch 101 MSE_tr: 19138.309172166875\n",
      "epoch 102 MSE_tr: 17308.34932662156\n",
      "epoch 103 MSE_tr: 17203.704310557776\n",
      "epoch 104 MSE_tr: 17444.923353260383\n",
      "epoch 105 MSE_tr: 17751.612125199597\n",
      "epoch 106 MSE_tr: 17236.473333946687\n",
      "epoch 107 MSE_tr: 16163.78953397985\n",
      "epoch 108 MSE_tr: 16580.466936862926\n",
      "epoch 109 MSE_tr: 16430.725302253\n",
      "epoch 110 MSE_tr: 16414.875389362995\n",
      "epoch 111 MSE_tr: 16804.8808652594\n",
      "epoch 112 MSE_tr: 15737.392014798574\n",
      "epoch 113 MSE_tr: 15504.238521946918\n",
      "epoch 114 MSE_tr: 14853.6082832132\n",
      "epoch 115 MSE_tr: 17981.43488058823\n",
      "epoch 116 MSE_tr: 16452.702695446187\n",
      "epoch 117 MSE_tr: 15846.516608665612\n",
      "epoch 118 MSE_tr: 16017.508764106531\n",
      "epoch 119 MSE_tr: 15624.34576727819\n",
      "epoch 120 MSE_tr: 16239.950388682904\n",
      "epoch 121 MSE_tr: 17461.11599622794\n",
      "epoch 122 MSE_tr: 18306.80919874082\n",
      "epoch 123 MSE_tr: 17368.483909945277\n",
      "epoch 124 MSE_tr: 17695.958071450128\n",
      "epoch 125 MSE_tr: 17279.609923078304\n",
      "epoch 126 MSE_tr: 18234.49245906864\n",
      "epoch 127 MSE_tr: 19569.05921801913\n",
      "epoch 128 MSE_tr: 17076.91088398604\n",
      "epoch 129 MSE_tr: 21495.735374640044\n",
      "epoch 130 MSE_tr: 18518.17500612959\n",
      "epoch 131 MSE_tr: 29371.01013402175\n",
      "epoch 132 MSE_tr: 23268.85939196054\n",
      "epoch 133 MSE_tr: 25746.492777904445\n",
      "epoch 134 MSE_tr: 22824.035007051145\n",
      "epoch 135 MSE_tr: 19643.96169548281\n",
      "epoch 136 MSE_tr: 20607.68764683613\n",
      "epoch 137 MSE_tr: 20645.353906555603\n",
      "epoch 138 MSE_tr: 21751.004794187455\n",
      "epoch 139 MSE_tr: 20538.06331822473\n",
      "epoch 140 MSE_tr: 20651.912044519322\n",
      "epoch 141 MSE_tr: 21712.498449577804\n",
      "epoch 142 MSE_tr: 23036.572199379247\n",
      "epoch 143 MSE_tr: 24942.81842635918\n",
      "epoch 144 MSE_tr: 25877.684810865245\n",
      "epoch 145 MSE_tr: 23459.03007291156\n",
      "epoch 146 MSE_tr: 25493.823383057217\n",
      "epoch 147 MSE_tr: 23179.720718277706\n",
      "epoch 148 MSE_tr: 24226.136275161327\n",
      "epoch 149 MSE_tr: 25556.50412064066\n",
      "epoch 150 MSE_tr: 27455.87249473215\n",
      "epoch 151 MSE_tr: 21156.75667421007\n",
      "epoch 152 MSE_tr: 21288.10080830657\n",
      "epoch 153 MSE_tr: 24400.380104041353\n",
      "epoch 154 MSE_tr: 24842.075978997123\n",
      "epoch 155 MSE_tr: 24604.54630612562\n",
      "epoch 156 MSE_tr: 21992.320943005732\n",
      "epoch 157 MSE_tr: 24580.9044097825\n",
      "epoch 158 MSE_tr: 26309.048810795288\n",
      "epoch 159 MSE_tr: 24180.39367301119\n",
      "epoch 160 MSE_tr: 26453.483642627587\n",
      "epoch 161 MSE_tr: 29298.909929478425\n",
      "epoch 162 MSE_tr: 33268.420811289885\n",
      "epoch 163 MSE_tr: 33168.56152070993\n",
      "epoch 164 MSE_tr: 28833.583591105198\n",
      "epoch 165 MSE_tr: 30007.484609267813\n",
      "epoch 166 MSE_tr: 28049.411195712466\n",
      "epoch 167 MSE_tr: 27693.577248256253\n",
      "epoch 168 MSE_tr: 27035.08975111301\n",
      "epoch 169 MSE_tr: 29241.73371314492\n",
      "epoch 170 MSE_tr: 32920.13008987743\n",
      "epoch 171 MSE_tr: 29404.624999501604\n",
      "epoch 172 MSE_tr: 27390.36483579338\n",
      "epoch 173 MSE_tr: 30064.621002334778\n",
      "epoch 174 MSE_tr: 33271.14125408085\n",
      "epoch 175 MSE_tr: 29542.380329197404\n",
      "epoch 176 MSE_tr: 29630.45226307517\n",
      "epoch 177 MSE_tr: 29841.774231091615\n",
      "epoch 178 MSE_tr: 29367.578334288475\n",
      "epoch 179 MSE_tr: 30925.334679266012\n",
      "epoch 180 MSE_tr: 34358.649266066444\n",
      "epoch 181 MSE_tr: 34510.8819949721\n",
      "epoch 182 MSE_tr: 34512.91828120564\n",
      "epoch 183 MSE_tr: 30781.681915941546\n",
      "epoch 184 MSE_tr: 33522.192800420395\n",
      "epoch 185 MSE_tr: 38386.86658181535\n",
      "epoch 186 MSE_tr: 39795.97378150003\n",
      "epoch 187 MSE_tr: 45215.13313294939\n",
      "epoch 188 MSE_tr: 36797.025645257076\n",
      "epoch 189 MSE_tr: 35365.50918688348\n",
      "epoch 190 MSE_tr: 33723.40257652022\n",
      "epoch 191 MSE_tr: 35133.519899214654\n",
      "epoch 192 MSE_tr: 37147.81118388237\n",
      "epoch 193 MSE_tr: 36634.56324082985\n",
      "epoch 194 MSE_tr: 35793.85987715845\n",
      "epoch 195 MSE_tr: 36470.299461730276\n",
      "epoch 196 MSE_tr: 34954.11268250713\n",
      "epoch 197 MSE_tr: 34422.31057742431\n",
      "epoch 198 MSE_tr: 32783.552092202466\n",
      "epoch 199 MSE_tr: 33823.82910048424\n",
      "epoch 200 MSE_tr: 31049.229437820075\n",
      "epoch 201 MSE_tr: 28937.299406196118\n",
      "epoch 202 MSE_tr: 28107.35660858381\n",
      "epoch 203 MSE_tr: 30945.438156359058\n",
      "epoch 204 MSE_tr: 33752.598488670046\n",
      "epoch 205 MSE_tr: 30143.785456070702\n",
      "epoch 206 MSE_tr: 28921.910095190895\n",
      "epoch 207 MSE_tr: 28289.864593766488\n",
      "epoch 208 MSE_tr: 31194.560905071747\n",
      "epoch 209 MSE_tr: 29817.136901232745\n",
      "epoch 210 MSE_tr: 35639.79401171923\n",
      "epoch 211 MSE_tr: 33635.134705748664\n",
      "epoch 212 MSE_tr: 32845.23668625791\n",
      "epoch 213 MSE_tr: 33917.554275190145\n",
      "epoch 214 MSE_tr: 33965.94762548693\n",
      "epoch 215 MSE_tr: 30456.057202495256\n",
      "epoch 216 MSE_tr: 30007.321538208722\n",
      "epoch 217 MSE_tr: 33589.354325274966\n",
      "epoch 218 MSE_tr: 31623.382073633915\n",
      "epoch 219 MSE_tr: 33479.18537892975\n",
      "epoch 220 MSE_tr: 31243.663797530102\n",
      "epoch 221 MSE_tr: 38188.504536429886\n",
      "epoch 222 MSE_tr: 36513.55827686634\n",
      "epoch 223 MSE_tr: 34155.42816235015\n",
      "epoch 224 MSE_tr: 30906.223516736765\n",
      "epoch 225 MSE_tr: 32429.140027121743\n",
      "epoch 226 MSE_tr: 35985.94867192215\n",
      "epoch 227 MSE_tr: 41579.210336830205\n",
      "epoch 228 MSE_tr: 36418.3317302086\n",
      "epoch 229 MSE_tr: 38312.69337526616\n",
      "epoch 230 MSE_tr: 36650.74210394201\n",
      "epoch 231 MSE_tr: 30144.109583602953\n",
      "epoch 232 MSE_tr: 29298.714052053972\n",
      "epoch 233 MSE_tr: 31157.103229770404\n",
      "epoch 234 MSE_tr: 33988.88992342324\n",
      "epoch 235 MSE_tr: 32253.811427515695\n",
      "epoch 236 MSE_tr: 30707.12960715365\n",
      "epoch 237 MSE_tr: 30676.802799292447\n",
      "epoch 238 MSE_tr: 31297.966146207968\n",
      "epoch 239 MSE_tr: 30449.783310660085\n",
      "epoch 240 MSE_tr: 27146.695299786763\n",
      "epoch 241 MSE_tr: 28373.078007746637\n",
      "epoch 242 MSE_tr: 32123.5334770293\n",
      "epoch 243 MSE_tr: 28966.676937254742\n",
      "epoch 244 MSE_tr: 27417.72190048561\n",
      "epoch 245 MSE_tr: 30571.73014879445\n",
      "epoch 246 MSE_tr: 29562.61242364861\n",
      "epoch 247 MSE_tr: 28744.173335546697\n",
      "epoch 248 MSE_tr: 28730.717877856092\n",
      "epoch 249 MSE_tr: 35209.00076507116\n",
      "epoch 250 MSE_tr: 22791.021395409658\n",
      "epoch 251 MSE_tr: 22168.654844716584\n",
      "epoch 252 MSE_tr: 24519.848384081015\n",
      "epoch 253 MSE_tr: 24718.10043724014\n",
      "epoch 254 MSE_tr: 23925.04086826519\n",
      "epoch 255 MSE_tr: 30726.123401873403\n",
      "epoch 256 MSE_tr: 28437.547172773793\n",
      "epoch 257 MSE_tr: 33139.78936859062\n",
      "epoch 258 MSE_tr: 32507.669703813513\n",
      "epoch 259 MSE_tr: 31649.80424687107\n",
      "epoch 260 MSE_tr: 33925.34268393218\n",
      "epoch 261 MSE_tr: 33064.01911578108\n",
      "epoch 262 MSE_tr: 31274.313343351205\n",
      "epoch 263 MSE_tr: 32324.65406312862\n",
      "epoch 264 MSE_tr: 28736.27125863622\n",
      "epoch 265 MSE_tr: 31803.291120955662\n",
      "epoch 266 MSE_tr: 29617.819605926463\n",
      "epoch 267 MSE_tr: 32278.66052383611\n",
      "epoch 268 MSE_tr: 30746.276608105476\n",
      "epoch 269 MSE_tr: 34966.65929396299\n",
      "epoch 270 MSE_tr: 32152.277031649835\n",
      "epoch 271 MSE_tr: 33036.22550169831\n",
      "epoch 272 MSE_tr: 35321.46588755771\n",
      "epoch 273 MSE_tr: 32735.54027731054\n",
      "epoch 274 MSE_tr: 36244.97401270443\n",
      "epoch 275 MSE_tr: 36621.45026763268\n",
      "epoch 276 MSE_tr: 35209.05849078223\n",
      "epoch 277 MSE_tr: 30864.775268952933\n",
      "epoch 278 MSE_tr: 32965.8033038207\n",
      "epoch 279 MSE_tr: 29111.4658368935\n",
      "epoch 280 MSE_tr: 31109.677560060234\n",
      "epoch 281 MSE_tr: 29781.89087289882\n",
      "epoch 282 MSE_tr: 32028.547525795602\n",
      "epoch 283 MSE_tr: 26938.742996176166\n",
      "epoch 284 MSE_tr: 27713.7771503872\n",
      "epoch 285 MSE_tr: 27694.239940223386\n",
      "epoch 286 MSE_tr: 34486.047474148254\n",
      "epoch 287 MSE_tr: 34016.41655177328\n",
      "epoch 288 MSE_tr: 29429.03397293138\n",
      "epoch 289 MSE_tr: 32530.77919219341\n",
      "epoch 290 MSE_tr: 28330.41370878968\n",
      "epoch 291 MSE_tr: 29516.532203308263\n",
      "epoch 292 MSE_tr: 30605.27170163605\n",
      "epoch 293 MSE_tr: 31352.264692712117\n",
      "epoch 294 MSE_tr: 32086.24094663565\n",
      "epoch 295 MSE_tr: 29611.1080502785\n",
      "epoch 296 MSE_tr: 28060.69862910621\n",
      "epoch 297 MSE_tr: 27474.104780768776\n",
      "epoch 298 MSE_tr: 28945.31931024734\n",
      "epoch 299 MSE_tr: 30229.630092704312\n",
      "epoch 300 MSE_tr: 29747.127524304375\n",
      "epoch 301 MSE_tr: 28715.06861118704\n",
      "epoch 302 MSE_tr: 28321.12958988888\n",
      "epoch 303 MSE_tr: 30156.60274213452\n",
      "epoch 304 MSE_tr: 27509.150587027223\n",
      "epoch 305 MSE_tr: 26885.341568492768\n",
      "epoch 306 MSE_tr: 30914.731537622018\n",
      "epoch 307 MSE_tr: 30055.361701874433\n",
      "epoch 308 MSE_tr: 29936.749642667826\n",
      "epoch 309 MSE_tr: 25763.748367159227\n",
      "epoch 310 MSE_tr: 24738.635978917835\n",
      "epoch 311 MSE_tr: 28033.23245831499\n",
      "epoch 312 MSE_tr: 24032.049895048007\n",
      "epoch 313 MSE_tr: 25964.28561349037\n",
      "epoch 314 MSE_tr: 29894.829656015867\n",
      "epoch 315 MSE_tr: 23818.943955578634\n",
      "epoch 316 MSE_tr: 28204.542216736078\n",
      "epoch 317 MSE_tr: 28514.74344798223\n",
      "epoch 318 MSE_tr: 29159.144398304175\n",
      "epoch 319 MSE_tr: 28263.240062789395\n",
      "epoch 320 MSE_tr: 28374.1306491297\n",
      "epoch 321 MSE_tr: 30812.827573196442\n",
      "epoch 322 MSE_tr: 28319.662446511596\n",
      "epoch 323 MSE_tr: 28195.41784870906\n",
      "epoch 324 MSE_tr: 30657.922421768882\n",
      "epoch 325 MSE_tr: 28900.153060029996\n",
      "epoch 326 MSE_tr: 26774.134587910434\n",
      "epoch 327 MSE_tr: 27051.150216262806\n",
      "epoch 328 MSE_tr: 29383.002162121866\n",
      "epoch 329 MSE_tr: 26122.42384714778\n",
      "epoch 330 MSE_tr: 27768.041321156074\n",
      "epoch 331 MSE_tr: 27786.015681480756\n",
      "epoch 332 MSE_tr: 29036.743321712405\n",
      "epoch 333 MSE_tr: 26678.376553473772\n",
      "epoch 334 MSE_tr: 33859.773145843035\n",
      "epoch 335 MSE_tr: 28743.298196263113\n",
      "epoch 336 MSE_tr: 28879.27460525694\n",
      "epoch 337 MSE_tr: 29404.064908617962\n",
      "epoch 338 MSE_tr: 29188.936508782677\n",
      "epoch 339 MSE_tr: 31264.788298074265\n",
      "epoch 340 MSE_tr: 27763.48431860295\n",
      "epoch 341 MSE_tr: 28166.826550893245\n",
      "epoch 342 MSE_tr: 27220.018315389134\n",
      "epoch 343 MSE_tr: 32462.81552491044\n",
      "epoch 344 MSE_tr: 28276.267507695535\n",
      "epoch 345 MSE_tr: 32830.01956360487\n",
      "epoch 346 MSE_tr: 31312.349054581122\n",
      "epoch 347 MSE_tr: 28319.734957181194\n",
      "epoch 348 MSE_tr: 30593.665848301287\n",
      "epoch 349 MSE_tr: 31415.944227436983\n",
      "epoch 350 MSE_tr: 35966.84736155833\n",
      "epoch 351 MSE_tr: 29957.550628816276\n",
      "epoch 352 MSE_tr: 29230.951308165437\n",
      "epoch 353 MSE_tr: 28979.47237292081\n",
      "epoch 354 MSE_tr: 28721.868022997478\n",
      "epoch 355 MSE_tr: 33270.16109228766\n",
      "epoch 356 MSE_tr: 29573.650287457414\n",
      "epoch 357 MSE_tr: 35173.0122552113\n",
      "epoch 358 MSE_tr: 34588.111095496824\n",
      "epoch 359 MSE_tr: 37698.75716505481\n",
      "epoch 360 MSE_tr: 36558.46788557403\n",
      "epoch 361 MSE_tr: 38982.367104200326\n",
      "epoch 362 MSE_tr: 39934.96870346322\n",
      "epoch 363 MSE_tr: 38810.08623114318\n",
      "epoch 364 MSE_tr: 35550.99339580535\n",
      "epoch 365 MSE_tr: 37118.589606820846\n",
      "epoch 366 MSE_tr: 32263.50253588077\n",
      "epoch 367 MSE_tr: 33053.032896167766\n",
      "epoch 368 MSE_tr: 30570.356995242542\n",
      "epoch 369 MSE_tr: 28229.91188497369\n",
      "epoch 370 MSE_tr: 27272.28088996423\n",
      "epoch 371 MSE_tr: 28618.353884257973\n",
      "epoch 372 MSE_tr: 26416.62091675198\n",
      "epoch 373 MSE_tr: 27888.509914245056\n",
      "epoch 374 MSE_tr: 25905.37365587666\n",
      "epoch 375 MSE_tr: 26935.57388947094\n",
      "epoch 376 MSE_tr: 26108.614523307224\n",
      "epoch 377 MSE_tr: 25251.565673822635\n",
      "epoch 378 MSE_tr: 26230.998510932426\n",
      "epoch 379 MSE_tr: 27842.918701329985\n",
      "epoch 380 MSE_tr: 30515.960402624984\n",
      "epoch 381 MSE_tr: 31628.490595545976\n",
      "epoch 382 MSE_tr: 29056.550630290934\n",
      "epoch 383 MSE_tr: 30963.32846074668\n",
      "epoch 384 MSE_tr: 24389.730288323695\n",
      "epoch 385 MSE_tr: 25327.23587025371\n",
      "epoch 386 MSE_tr: 25888.632877547603\n",
      "epoch 387 MSE_tr: 26025.92558069717\n",
      "epoch 388 MSE_tr: 27456.892842359473\n",
      "epoch 389 MSE_tr: 26919.76967185597\n",
      "epoch 390 MSE_tr: 28800.688920108067\n",
      "epoch 391 MSE_tr: 26743.018162074888\n",
      "epoch 392 MSE_tr: 25459.52480424052\n",
      "epoch 393 MSE_tr: 27907.576929918097\n",
      "epoch 394 MSE_tr: 27561.25233517013\n",
      "epoch 395 MSE_tr: 26459.301081510264\n",
      "epoch 396 MSE_tr: 25522.975059823082\n",
      "epoch 397 MSE_tr: 27528.562157312357\n",
      "epoch 398 MSE_tr: 31988.80708433241\n",
      "epoch 399 MSE_tr: 28522.722924274254\n",
      "epoch 400 MSE_tr: 27482.534922862207\n",
      "epoch 401 MSE_tr: 27576.52662056494\n",
      "epoch 402 MSE_tr: 26956.964663344344\n",
      "epoch 403 MSE_tr: 24656.39876509055\n",
      "epoch 404 MSE_tr: 25705.413060328236\n",
      "epoch 405 MSE_tr: 24486.06881657203\n",
      "epoch 406 MSE_tr: 22938.289309851778\n",
      "epoch 407 MSE_tr: 23944.51868650396\n",
      "epoch 408 MSE_tr: 25687.896650428058\n",
      "epoch 409 MSE_tr: 26824.890434137087\n",
      "epoch 410 MSE_tr: 29298.042120608294\n",
      "epoch 411 MSE_tr: 27591.23706282085\n",
      "epoch 412 MSE_tr: 30416.477303182928\n",
      "epoch 413 MSE_tr: 31847.180244166146\n",
      "epoch 414 MSE_tr: 41586.45977502298\n",
      "epoch 415 MSE_tr: 28967.952377475172\n",
      "epoch 416 MSE_tr: 27666.05118750091\n",
      "epoch 417 MSE_tr: 29463.337996680857\n",
      "epoch 418 MSE_tr: 25733.84990733158\n",
      "epoch 419 MSE_tr: 25981.840198472204\n",
      "epoch 420 MSE_tr: 26159.908219750963\n",
      "epoch 421 MSE_tr: 20778.586706069418\n",
      "epoch 422 MSE_tr: 22290.287086326734\n",
      "epoch 423 MSE_tr: 29885.887621313963\n",
      "epoch 424 MSE_tr: 27119.429424817903\n",
      "epoch 425 MSE_tr: 25990.462087349144\n",
      "epoch 426 MSE_tr: 26459.13889843985\n",
      "epoch 427 MSE_tr: 27827.842465752834\n",
      "epoch 428 MSE_tr: 24213.577608421554\n",
      "epoch 429 MSE_tr: 24964.262717767568\n",
      "epoch 430 MSE_tr: 23739.312207716426\n",
      "epoch 431 MSE_tr: 25287.85484091864\n",
      "epoch 432 MSE_tr: 26871.356076096054\n",
      "epoch 433 MSE_tr: 27705.63423317538\n",
      "epoch 434 MSE_tr: 25473.8603996621\n",
      "epoch 435 MSE_tr: 29122.27160036534\n",
      "epoch 436 MSE_tr: 29355.740550875096\n",
      "epoch 437 MSE_tr: 26975.788195863075\n",
      "epoch 438 MSE_tr: 25391.05549078022\n",
      "epoch 439 MSE_tr: 25931.6046018854\n",
      "epoch 440 MSE_tr: 27808.528753453895\n",
      "epoch 441 MSE_tr: 26600.944927090135\n",
      "epoch 442 MSE_tr: 30009.281046145356\n",
      "epoch 443 MSE_tr: 27115.591533421306\n",
      "epoch 444 MSE_tr: 26859.25262693959\n",
      "epoch 445 MSE_tr: 29894.27098070032\n",
      "epoch 446 MSE_tr: 32151.90535403784\n",
      "epoch 447 MSE_tr: 31608.338921738803\n",
      "epoch 448 MSE_tr: 25587.036908499875\n",
      "epoch 449 MSE_tr: 25356.113309296725\n",
      "epoch 450 MSE_tr: 28203.27257612272\n",
      "epoch 451 MSE_tr: 26547.147383857755\n",
      "epoch 452 MSE_tr: 27828.337344884694\n",
      "epoch 453 MSE_tr: 30814.968545709387\n",
      "epoch 454 MSE_tr: 28935.73057545968\n",
      "epoch 455 MSE_tr: 25057.779995502235\n",
      "epoch 456 MSE_tr: 23548.9124595702\n",
      "epoch 457 MSE_tr: 23968.549231449106\n",
      "epoch 458 MSE_tr: 25620.1133462504\n",
      "epoch 459 MSE_tr: 26278.334539257918\n",
      "epoch 460 MSE_tr: 27187.075795863082\n",
      "epoch 461 MSE_tr: 26840.31557735803\n",
      "epoch 462 MSE_tr: 23010.905279489838\n",
      "epoch 463 MSE_tr: 24180.845467153085\n",
      "epoch 464 MSE_tr: 24542.38336114996\n",
      "epoch 465 MSE_tr: 25120.23095955392\n",
      "epoch 466 MSE_tr: 24527.562177185362\n",
      "epoch 467 MSE_tr: 26059.067626174103\n",
      "epoch 468 MSE_tr: 26979.77491159257\n",
      "epoch 469 MSE_tr: 28031.474473919458\n",
      "epoch 470 MSE_tr: 26557.73506879079\n",
      "epoch 471 MSE_tr: 23615.21658041347\n",
      "epoch 472 MSE_tr: 26459.268197982285\n",
      "epoch 473 MSE_tr: 26893.87544396493\n",
      "epoch 474 MSE_tr: 26361.9653848168\n",
      "epoch 475 MSE_tr: 26749.970824467207\n",
      "epoch 476 MSE_tr: 25814.531813474106\n",
      "epoch 477 MSE_tr: 25409.33618357337\n",
      "epoch 478 MSE_tr: 28482.81875086258\n",
      "epoch 479 MSE_tr: 29449.198660083162\n",
      "epoch 480 MSE_tr: 33782.26985749454\n",
      "epoch 481 MSE_tr: 26756.181024977403\n",
      "epoch 482 MSE_tr: 27115.862155170216\n",
      "epoch 483 MSE_tr: 28858.38871453824\n",
      "epoch 484 MSE_tr: 22175.21447636256\n",
      "epoch 485 MSE_tr: 29831.3640374711\n",
      "epoch 486 MSE_tr: 26732.843371803396\n",
      "epoch 487 MSE_tr: 27911.019914724122\n",
      "epoch 488 MSE_tr: 31395.51452611021\n",
      "epoch 489 MSE_tr: 29507.03229412464\n",
      "epoch 490 MSE_tr: 29892.206250095234\n",
      "epoch 491 MSE_tr: 26106.434936067297\n",
      "epoch 492 MSE_tr: 25837.54709913811\n",
      "epoch 493 MSE_tr: 25886.050698417082\n",
      "epoch 494 MSE_tr: 29618.67763022373\n",
      "epoch 495 MSE_tr: 28859.33865244568\n",
      "epoch 496 MSE_tr: 32773.92854046125\n",
      "epoch 497 MSE_tr: 34506.63356923943\n",
      "epoch 498 MSE_tr: 28079.116742327315\n",
      "epoch 499 MSE_tr: 23986.97674696506\n",
      "epoch 500 MSE_tr: 27021.61200573911\n",
      "epoch 501 MSE_tr: 25651.508895697047\n",
      "epoch 502 MSE_tr: 28666.407963405338\n",
      "epoch 503 MSE_tr: 26115.492087282433\n",
      "epoch 504 MSE_tr: 23469.44473043743\n",
      "epoch 505 MSE_tr: 24289.194939949684\n",
      "epoch 506 MSE_tr: 28416.334292866482\n",
      "epoch 507 MSE_tr: 25818.094783806537\n",
      "epoch 508 MSE_tr: 27335.63838943888\n",
      "epoch 509 MSE_tr: 29562.397847015727\n",
      "epoch 510 MSE_tr: 35485.995213584065\n",
      "epoch 511 MSE_tr: 31864.458905241616\n",
      "epoch 512 MSE_tr: 31787.002038435166\n",
      "epoch 513 MSE_tr: 31063.23725326272\n",
      "epoch 514 MSE_tr: 29807.746816170784\n",
      "epoch 515 MSE_tr: 28783.759581755297\n",
      "epoch 516 MSE_tr: 32500.38160768279\n",
      "epoch 517 MSE_tr: 27896.18380114849\n",
      "epoch 518 MSE_tr: 26404.932000865967\n",
      "epoch 519 MSE_tr: 26899.638885495413\n",
      "epoch 520 MSE_tr: 25935.042274039937\n",
      "epoch 521 MSE_tr: 25463.294925710485\n",
      "epoch 522 MSE_tr: 30241.224339488457\n",
      "epoch 523 MSE_tr: 31625.94681216713\n",
      "epoch 524 MSE_tr: 26628.84951808509\n",
      "epoch 525 MSE_tr: 27969.397148604272\n",
      "epoch 526 MSE_tr: 26189.922126801666\n",
      "epoch 527 MSE_tr: 24347.827179760552\n",
      "epoch 528 MSE_tr: 24786.635117720878\n",
      "epoch 529 MSE_tr: 28462.38056138638\n",
      "epoch 530 MSE_tr: 26034.155898016776\n",
      "epoch 531 MSE_tr: 26773.61878054643\n",
      "epoch 532 MSE_tr: 25578.127997921572\n",
      "epoch 533 MSE_tr: 23147.079058283074\n",
      "epoch 534 MSE_tr: 25818.084333066494\n",
      "epoch 535 MSE_tr: 24072.47581323379\n",
      "epoch 536 MSE_tr: 20796.16941193825\n",
      "epoch 537 MSE_tr: 21876.725491538287\n",
      "epoch 538 MSE_tr: 19105.95165684584\n",
      "epoch 539 MSE_tr: 19811.32013455472\n",
      "epoch 540 MSE_tr: 19685.19399747082\n",
      "epoch 541 MSE_tr: 18349.552391520927\n",
      "epoch 542 MSE_tr: 18686.89913172308\n",
      "epoch 543 MSE_tr: 19406.881885632654\n",
      "epoch 544 MSE_tr: 20272.621794212933\n",
      "epoch 545 MSE_tr: 20539.241861073762\n",
      "epoch 546 MSE_tr: 20686.042702055278\n",
      "epoch 547 MSE_tr: 20819.39012674937\n",
      "epoch 548 MSE_tr: 20803.342505819935\n",
      "epoch 549 MSE_tr: 22106.373313367276\n",
      "epoch 550 MSE_tr: 21924.652714460164\n",
      "epoch 551 MSE_tr: 25061.587065361084\n",
      "epoch 552 MSE_tr: 28230.076768683823\n",
      "epoch 553 MSE_tr: 26524.609001542973\n",
      "epoch 554 MSE_tr: 25850.6111544854\n",
      "epoch 555 MSE_tr: 22574.790324249476\n",
      "epoch 556 MSE_tr: 24528.260596048458\n",
      "epoch 557 MSE_tr: 21980.524234199333\n",
      "epoch 558 MSE_tr: 21063.43215471393\n",
      "epoch 559 MSE_tr: 24801.011668236973\n",
      "epoch 560 MSE_tr: 24878.019580496028\n",
      "epoch 561 MSE_tr: 23776.62286632229\n",
      "epoch 562 MSE_tr: 26663.01750696199\n",
      "epoch 563 MSE_tr: 24317.23571322648\n",
      "epoch 564 MSE_tr: 22841.48723380065\n",
      "epoch 565 MSE_tr: 22023.315988138576\n",
      "epoch 566 MSE_tr: 21929.625949013323\n",
      "epoch 567 MSE_tr: 23171.015293528166\n",
      "epoch 568 MSE_tr: 22855.284630162176\n",
      "epoch 569 MSE_tr: 28488.265898414447\n",
      "epoch 570 MSE_tr: 23025.00432609475\n",
      "epoch 571 MSE_tr: 24020.815214082322\n",
      "epoch 572 MSE_tr: 19589.191155588505\n",
      "epoch 573 MSE_tr: 19364.600735800374\n",
      "epoch 574 MSE_tr: 21027.483202750565\n",
      "epoch 575 MSE_tr: 20858.38208067393\n",
      "epoch 576 MSE_tr: 20149.846230092306\n",
      "epoch 577 MSE_tr: 21012.48458904433\n",
      "epoch 578 MSE_tr: 20705.52043503813\n",
      "epoch 579 MSE_tr: 21576.505391665105\n",
      "epoch 580 MSE_tr: 22112.000718928844\n",
      "epoch 581 MSE_tr: 23432.56978154121\n",
      "epoch 582 MSE_tr: 20767.1382948939\n",
      "epoch 583 MSE_tr: 24336.18666965357\n",
      "epoch 584 MSE_tr: 25239.86047604659\n",
      "epoch 585 MSE_tr: 25806.99859025094\n",
      "epoch 586 MSE_tr: 21418.276171190202\n",
      "epoch 587 MSE_tr: 26373.533752813102\n",
      "epoch 588 MSE_tr: 22168.82874298444\n",
      "epoch 589 MSE_tr: 23451.281920147696\n",
      "epoch 590 MSE_tr: 24615.20999904556\n",
      "epoch 591 MSE_tr: 31249.810126834636\n",
      "epoch 592 MSE_tr: 27559.510040276313\n",
      "epoch 593 MSE_tr: 31591.83427059335\n",
      "epoch 594 MSE_tr: 25041.385660679636\n",
      "epoch 595 MSE_tr: 26716.70563157522\n",
      "epoch 596 MSE_tr: 27139.909023567947\n",
      "epoch 597 MSE_tr: 24974.079708918278\n",
      "epoch 598 MSE_tr: 23599.599923206264\n",
      "epoch 599 MSE_tr: 24348.976384649737\n",
      "epoch 600 MSE_tr: 24557.58872062926\n",
      "epoch 601 MSE_tr: 25006.695713038283\n",
      "epoch 602 MSE_tr: 22245.80892278211\n",
      "epoch 603 MSE_tr: 23719.542009365334\n",
      "epoch 604 MSE_tr: 23318.903317556127\n",
      "epoch 605 MSE_tr: 26020.83152130072\n",
      "epoch 606 MSE_tr: 24404.891004638863\n",
      "epoch 607 MSE_tr: 22746.013626591102\n",
      "epoch 608 MSE_tr: 29891.715424891125\n",
      "epoch 609 MSE_tr: 25451.54076144015\n",
      "epoch 610 MSE_tr: 22973.14993766883\n",
      "epoch 611 MSE_tr: 21297.741738590426\n",
      "epoch 612 MSE_tr: 22823.785359791265\n",
      "epoch 613 MSE_tr: 20645.937793184108\n",
      "epoch 614 MSE_tr: 22826.99949880225\n",
      "epoch 615 MSE_tr: 21085.743567556197\n",
      "epoch 616 MSE_tr: 19927.101411411473\n",
      "epoch 617 MSE_tr: 19997.337729768664\n",
      "epoch 618 MSE_tr: 20258.407674736656\n",
      "epoch 619 MSE_tr: 25484.495196155236\n",
      "epoch 620 MSE_tr: 24495.939105241287\n",
      "epoch 621 MSE_tr: 25295.152679391555\n",
      "epoch 622 MSE_tr: 19686.0871693187\n",
      "epoch 623 MSE_tr: 24687.20409143935\n",
      "epoch 624 MSE_tr: 21286.715231035632\n",
      "epoch 625 MSE_tr: 23044.786775761928\n",
      "epoch 626 MSE_tr: 23428.460135470403\n",
      "epoch 627 MSE_tr: 19858.187435755688\n",
      "epoch 628 MSE_tr: 21421.165594955757\n",
      "epoch 629 MSE_tr: 20864.25614848784\n",
      "epoch 630 MSE_tr: 25829.187097607846\n",
      "epoch 631 MSE_tr: 29138.567760358917\n",
      "epoch 632 MSE_tr: 26489.58715777537\n",
      "epoch 633 MSE_tr: 23692.60690924111\n",
      "epoch 634 MSE_tr: 26838.271483927583\n",
      "epoch 635 MSE_tr: 21529.021125839077\n",
      "epoch 636 MSE_tr: 22869.52141511434\n",
      "epoch 637 MSE_tr: 26604.942011619867\n",
      "epoch 638 MSE_tr: 22055.757629507374\n",
      "epoch 639 MSE_tr: 22808.4758484779\n",
      "epoch 640 MSE_tr: 19485.435774582496\n",
      "epoch 641 MSE_tr: 22213.639794182003\n",
      "epoch 642 MSE_tr: 22395.007039804936\n",
      "epoch 643 MSE_tr: 18833.424194211988\n",
      "epoch 644 MSE_tr: 23630.882739857745\n",
      "epoch 645 MSE_tr: 20369.520247690387\n",
      "epoch 646 MSE_tr: 19811.018085582415\n",
      "epoch 647 MSE_tr: 19739.576969835456\n",
      "epoch 648 MSE_tr: 17038.67607654379\n",
      "epoch 649 MSE_tr: 18813.62205589086\n",
      "epoch 650 MSE_tr: 19759.651630854787\n",
      "epoch 651 MSE_tr: 19289.88184724755\n",
      "epoch 652 MSE_tr: 18311.92552655503\n",
      "epoch 653 MSE_tr: 16979.92729519295\n",
      "epoch 654 MSE_tr: 18104.138955258255\n",
      "epoch 655 MSE_tr: 19077.386168546545\n",
      "epoch 656 MSE_tr: 19182.71305644351\n",
      "epoch 657 MSE_tr: 18601.48072157352\n",
      "epoch 658 MSE_tr: 19848.202716258522\n",
      "epoch 659 MSE_tr: 16408.56715091745\n",
      "epoch 660 MSE_tr: 16674.07992519244\n",
      "epoch 661 MSE_tr: 16081.704516258993\n",
      "epoch 662 MSE_tr: 15841.170073239808\n",
      "epoch 663 MSE_tr: 16163.30810559946\n",
      "epoch 664 MSE_tr: 15600.306076403607\n",
      "epoch 665 MSE_tr: 16258.170516247481\n",
      "epoch 666 MSE_tr: 16168.081850998578\n",
      "epoch 667 MSE_tr: 17508.352564354558\n",
      "epoch 668 MSE_tr: 15921.017094766772\n",
      "epoch 669 MSE_tr: 15947.295695629355\n",
      "epoch 670 MSE_tr: 17563.038821430724\n",
      "epoch 671 MSE_tr: 14985.38847094778\n",
      "epoch 672 MSE_tr: 19756.789192939905\n",
      "epoch 673 MSE_tr: 17168.300488524892\n",
      "epoch 674 MSE_tr: 17331.028849832015\n",
      "epoch 675 MSE_tr: 16437.291871470443\n",
      "epoch 676 MSE_tr: 20597.322629725073\n",
      "epoch 677 MSE_tr: 17188.732607861373\n",
      "epoch 678 MSE_tr: 23673.759188824853\n",
      "epoch 679 MSE_tr: 21457.641065143565\n",
      "epoch 680 MSE_tr: 22991.423178096633\n",
      "epoch 681 MSE_tr: 21431.3981553958\n",
      "epoch 682 MSE_tr: 18993.645281098186\n",
      "epoch 683 MSE_tr: 20653.935328015014\n",
      "epoch 684 MSE_tr: 18686.777765920848\n",
      "epoch 685 MSE_tr: 17759.465460239582\n",
      "epoch 686 MSE_tr: 21753.060431639948\n",
      "epoch 687 MSE_tr: 19872.013868133512\n",
      "epoch 688 MSE_tr: 21156.395235958524\n",
      "epoch 689 MSE_tr: 18870.971446230116\n",
      "epoch 690 MSE_tr: 18999.011303603198\n",
      "epoch 691 MSE_tr: 21454.417028227945\n",
      "epoch 692 MSE_tr: 19656.40878559137\n",
      "epoch 693 MSE_tr: 19180.78247144064\n",
      "epoch 694 MSE_tr: 20175.009146798504\n",
      "epoch 695 MSE_tr: 20448.25471337246\n",
      "epoch 696 MSE_tr: 20161.66097400788\n",
      "epoch 697 MSE_tr: 21235.97876207083\n",
      "epoch 698 MSE_tr: 20041.38058540756\n",
      "epoch 699 MSE_tr: 19061.38637309498\n",
      "epoch 700 MSE_tr: 17105.216156375\n",
      "epoch 701 MSE_tr: 15390.933408400644\n",
      "epoch 702 MSE_tr: 19627.2870169643\n",
      "epoch 703 MSE_tr: 15894.093015075377\n",
      "epoch 704 MSE_tr: 14795.065611193277\n",
      "epoch 705 MSE_tr: 15661.085183118505\n",
      "epoch 706 MSE_tr: 18685.963804621228\n",
      "epoch 707 MSE_tr: 14488.907389011907\n",
      "epoch 708 MSE_tr: 15630.455851300645\n",
      "epoch 709 MSE_tr: 18919.50568267753\n",
      "epoch 710 MSE_tr: 21631.339244732113\n",
      "epoch 711 MSE_tr: 20817.378939494625\n",
      "epoch 712 MSE_tr: 22074.18489999388\n",
      "epoch 713 MSE_tr: 21452.125060905975\n",
      "epoch 714 MSE_tr: 23872.63553872202\n",
      "epoch 715 MSE_tr: 18773.87262311681\n",
      "epoch 716 MSE_tr: 18057.868148018715\n",
      "epoch 717 MSE_tr: 20362.271809699974\n",
      "epoch 718 MSE_tr: 17740.093222854834\n",
      "epoch 719 MSE_tr: 22376.513307645593\n",
      "epoch 720 MSE_tr: 22879.620865557103\n",
      "epoch 721 MSE_tr: 21886.042653077093\n",
      "epoch 722 MSE_tr: 19610.57504927853\n",
      "epoch 723 MSE_tr: 21508.885986137502\n",
      "epoch 724 MSE_tr: 18440.050247966723\n",
      "epoch 725 MSE_tr: 18792.530927623506\n",
      "epoch 726 MSE_tr: 18886.15694999416\n",
      "epoch 727 MSE_tr: 21903.246284789482\n",
      "epoch 728 MSE_tr: 21266.362173219222\n",
      "epoch 729 MSE_tr: 21024.742416599453\n",
      "epoch 730 MSE_tr: 22040.583400875847\n",
      "epoch 731 MSE_tr: 23222.99468930344\n",
      "epoch 732 MSE_tr: 20174.79196725485\n",
      "epoch 733 MSE_tr: 18407.933805258697\n",
      "epoch 734 MSE_tr: 17840.406020624494\n",
      "epoch 735 MSE_tr: 15670.706248435275\n",
      "epoch 736 MSE_tr: 18297.75074109889\n",
      "epoch 737 MSE_tr: 15614.14543740655\n",
      "epoch 738 MSE_tr: 21858.857012159264\n",
      "epoch 739 MSE_tr: 21106.41233082046\n",
      "epoch 740 MSE_tr: 20264.542324371105\n",
      "epoch 741 MSE_tr: 17391.76172464337\n",
      "epoch 742 MSE_tr: 21262.769382401802\n",
      "epoch 743 MSE_tr: 15803.793840776754\n",
      "epoch 744 MSE_tr: 20471.123001357795\n",
      "epoch 745 MSE_tr: 18497.30394022904\n",
      "epoch 746 MSE_tr: 22808.924957200645\n",
      "epoch 747 MSE_tr: 20089.36478180644\n",
      "epoch 748 MSE_tr: 25541.869585718283\n",
      "epoch 749 MSE_tr: 28604.440897549783\n",
      "epoch 750 MSE_tr: 22048.361714591265\n",
      "epoch 751 MSE_tr: 20235.52718084724\n",
      "epoch 752 MSE_tr: 17188.799329955968\n",
      "epoch 753 MSE_tr: 19545.488083792523\n",
      "epoch 754 MSE_tr: 18114.814434468815\n",
      "epoch 755 MSE_tr: 17032.61012889404\n",
      "epoch 756 MSE_tr: 26474.030893298142\n",
      "epoch 757 MSE_tr: 20012.714404591283\n",
      "epoch 758 MSE_tr: 19323.72875473582\n",
      "epoch 759 MSE_tr: 19577.21878910452\n",
      "epoch 760 MSE_tr: 17351.98142933359\n",
      "epoch 761 MSE_tr: 17376.112801973522\n",
      "epoch 762 MSE_tr: 20735.70667721952\n",
      "epoch 763 MSE_tr: 20349.01221015866\n",
      "epoch 764 MSE_tr: 18680.419861447874\n",
      "epoch 765 MSE_tr: 16721.934747354517\n",
      "epoch 766 MSE_tr: 17828.899626506267\n",
      "epoch 767 MSE_tr: 20214.00468158216\n",
      "epoch 768 MSE_tr: 18334.953583396233\n",
      "epoch 769 MSE_tr: 18664.512509810236\n",
      "epoch 770 MSE_tr: 17974.115122256844\n",
      "epoch 771 MSE_tr: 19064.969951908846\n",
      "epoch 772 MSE_tr: 18996.91658695376\n",
      "epoch 773 MSE_tr: 17164.46550029972\n",
      "epoch 774 MSE_tr: 19073.974186601405\n",
      "epoch 775 MSE_tr: 21090.81185231951\n",
      "epoch 776 MSE_tr: 18989.03245529467\n",
      "epoch 777 MSE_tr: 22150.319174906843\n",
      "epoch 778 MSE_tr: 23094.14530175415\n",
      "epoch 779 MSE_tr: 18418.042067209295\n",
      "epoch 780 MSE_tr: 18753.275938371942\n",
      "epoch 781 MSE_tr: 19804.934877252468\n",
      "epoch 782 MSE_tr: 17347.743115688932\n",
      "epoch 783 MSE_tr: 17252.384945291935\n",
      "epoch 784 MSE_tr: 22537.999643610026\n",
      "epoch 785 MSE_tr: 24121.109220491464\n",
      "epoch 786 MSE_tr: 22546.80953991526\n",
      "epoch 787 MSE_tr: 16886.811692259187\n",
      "epoch 788 MSE_tr: 16966.369772995502\n",
      "epoch 789 MSE_tr: 20551.577645097677\n",
      "epoch 790 MSE_tr: 19322.448383164327\n",
      "epoch 791 MSE_tr: 22296.75382456351\n",
      "epoch 792 MSE_tr: 16509.797658570875\n",
      "epoch 793 MSE_tr: 16684.307255768967\n",
      "epoch 794 MSE_tr: 18928.67763487743\n",
      "epoch 795 MSE_tr: 17976.098183624315\n",
      "epoch 796 MSE_tr: 18593.45733398371\n",
      "epoch 797 MSE_tr: 18367.96002683013\n",
      "epoch 798 MSE_tr: 18627.629504234203\n",
      "epoch 799 MSE_tr: 18300.837299159688\n",
      "epoch 800 MSE_tr: 19294.05118934028\n",
      "epoch 801 MSE_tr: 18216.228382659938\n",
      "epoch 802 MSE_tr: 26847.698829270033\n",
      "epoch 803 MSE_tr: 19896.809293192084\n",
      "epoch 804 MSE_tr: 21875.103712514974\n",
      "epoch 805 MSE_tr: 20614.688656813316\n",
      "epoch 806 MSE_tr: 22565.641271063316\n",
      "epoch 807 MSE_tr: 19474.248017671936\n",
      "epoch 808 MSE_tr: 20196.74136316204\n",
      "epoch 809 MSE_tr: 21761.22740304184\n",
      "epoch 810 MSE_tr: 19814.758806564394\n",
      "epoch 811 MSE_tr: 17248.18816952387\n",
      "epoch 812 MSE_tr: 20490.354551461132\n",
      "epoch 813 MSE_tr: 19644.989167357395\n",
      "epoch 814 MSE_tr: 24883.43506527829\n",
      "epoch 815 MSE_tr: 28280.96582955192\n",
      "epoch 816 MSE_tr: 27202.175689810985\n",
      "epoch 817 MSE_tr: 20880.625024405806\n",
      "epoch 818 MSE_tr: 24245.1568193959\n",
      "epoch 819 MSE_tr: 22350.325687723052\n",
      "epoch 820 MSE_tr: 24756.51472353831\n",
      "epoch 821 MSE_tr: 20530.647343155037\n",
      "epoch 822 MSE_tr: 23679.904510153134\n",
      "epoch 823 MSE_tr: 19466.122745103963\n",
      "epoch 824 MSE_tr: 27005.420465508672\n",
      "epoch 825 MSE_tr: 16766.138397429604\n",
      "epoch 826 MSE_tr: 18916.73023584909\n",
      "epoch 827 MSE_tr: 18145.42460768932\n",
      "epoch 828 MSE_tr: 18189.539422050097\n",
      "epoch 829 MSE_tr: 22007.241815011374\n",
      "epoch 830 MSE_tr: 18124.683641170875\n",
      "epoch 831 MSE_tr: 15750.13090273531\n",
      "epoch 832 MSE_tr: 19064.08698112291\n",
      "epoch 833 MSE_tr: 18600.041789443316\n",
      "epoch 834 MSE_tr: 20938.89165082569\n",
      "epoch 835 MSE_tr: 21145.28952787986\n",
      "epoch 836 MSE_tr: 26145.68035576063\n",
      "epoch 837 MSE_tr: 23731.597556526754\n",
      "epoch 838 MSE_tr: 23848.703631126336\n",
      "epoch 839 MSE_tr: 17408.814481094254\n",
      "epoch 840 MSE_tr: 17627.099287377434\n",
      "epoch 841 MSE_tr: 17705.910113614274\n",
      "epoch 842 MSE_tr: 17400.336626010125\n",
      "epoch 843 MSE_tr: 19737.60152623647\n",
      "epoch 844 MSE_tr: 21813.073075591845\n",
      "epoch 845 MSE_tr: 19507.73660590517\n",
      "epoch 846 MSE_tr: 19471.847140619204\n",
      "epoch 847 MSE_tr: 27035.538439798765\n",
      "epoch 848 MSE_tr: 22644.029832178785\n",
      "epoch 849 MSE_tr: 18096.07907750706\n",
      "epoch 850 MSE_tr: 24579.417027982505\n",
      "epoch 851 MSE_tr: 29472.742637570795\n",
      "epoch 852 MSE_tr: 22908.647929802246\n",
      "epoch 853 MSE_tr: 25829.534549121818\n",
      "epoch 854 MSE_tr: 21227.591650517108\n",
      "epoch 855 MSE_tr: 22369.440389953965\n",
      "epoch 856 MSE_tr: 25964.207706092562\n",
      "epoch 857 MSE_tr: 18962.58883877485\n",
      "epoch 858 MSE_tr: 25037.05109679393\n",
      "epoch 859 MSE_tr: 17806.159615955585\n",
      "epoch 860 MSE_tr: 20596.55096273663\n",
      "epoch 861 MSE_tr: 17868.745128663173\n",
      "epoch 862 MSE_tr: 20449.801103134836\n",
      "epoch 863 MSE_tr: 23306.496780584166\n",
      "epoch 864 MSE_tr: 27215.883817646176\n",
      "epoch 865 MSE_tr: 21421.332063719452\n",
      "epoch 866 MSE_tr: 25654.748898454724\n",
      "epoch 867 MSE_tr: 19612.156285669418\n",
      "epoch 868 MSE_tr: 22871.526648643296\n",
      "epoch 869 MSE_tr: 26170.00063977509\n",
      "epoch 870 MSE_tr: 23091.009155629126\n",
      "epoch 871 MSE_tr: 17145.18481541097\n",
      "epoch 872 MSE_tr: 18900.127576468843\n",
      "epoch 873 MSE_tr: 23353.096610897173\n",
      "epoch 874 MSE_tr: 17492.5780657181\n",
      "epoch 875 MSE_tr: 23341.685735551833\n",
      "epoch 876 MSE_tr: 18956.784195642245\n",
      "epoch 877 MSE_tr: 21857.00775126102\n",
      "epoch 878 MSE_tr: 17446.974125659806\n",
      "epoch 879 MSE_tr: 14775.732630093591\n",
      "epoch 880 MSE_tr: 15417.99133177637\n",
      "epoch 881 MSE_tr: 20182.526488853753\n",
      "epoch 882 MSE_tr: 21067.784772527717\n",
      "epoch 883 MSE_tr: 24317.376206928326\n",
      "epoch 884 MSE_tr: 22088.721718550554\n",
      "epoch 885 MSE_tr: 25159.99166506762\n",
      "epoch 886 MSE_tr: 16999.18145649124\n",
      "epoch 887 MSE_tr: 21015.727290740357\n",
      "epoch 888 MSE_tr: 18783.961641294318\n",
      "epoch 889 MSE_tr: 20671.376623501983\n",
      "epoch 890 MSE_tr: 20504.807509217364\n",
      "epoch 891 MSE_tr: 20507.607285654994\n",
      "epoch 892 MSE_tr: 18379.263315624205\n",
      "epoch 893 MSE_tr: 21802.27510453072\n",
      "epoch 894 MSE_tr: 19492.912756493566\n",
      "epoch 895 MSE_tr: 23442.96563170629\n",
      "epoch 896 MSE_tr: 20401.31965096904\n",
      "epoch 897 MSE_tr: 23733.147882711375\n",
      "epoch 898 MSE_tr: 24475.620479091278\n",
      "epoch 899 MSE_tr: 19374.33056790443\n",
      "epoch 900 MSE_tr: 19087.3960085706\n",
      "epoch 901 MSE_tr: 22754.630547332516\n",
      "epoch 902 MSE_tr: 22077.197869060026\n",
      "epoch 903 MSE_tr: 17141.443907302706\n",
      "epoch 904 MSE_tr: 17369.27918235938\n",
      "epoch 905 MSE_tr: 22828.259593217645\n",
      "epoch 906 MSE_tr: 16433.22241107608\n",
      "epoch 907 MSE_tr: 19803.428820464705\n",
      "epoch 908 MSE_tr: 16105.38331657726\n",
      "epoch 909 MSE_tr: 14776.893492462294\n",
      "epoch 910 MSE_tr: 24841.9811211773\n",
      "epoch 911 MSE_tr: 18365.077681592462\n",
      "epoch 912 MSE_tr: 18463.790250950184\n",
      "epoch 913 MSE_tr: 19780.162856326595\n",
      "epoch 914 MSE_tr: 17040.45687522767\n",
      "epoch 915 MSE_tr: 18395.618681615997\n",
      "epoch 916 MSE_tr: 15879.929395754987\n",
      "epoch 917 MSE_tr: 19702.258094230972\n",
      "epoch 918 MSE_tr: 20928.70237762507\n",
      "epoch 919 MSE_tr: 15661.457734508833\n",
      "epoch 920 MSE_tr: 13679.253997502603\n",
      "epoch 921 MSE_tr: 17741.21287861084\n",
      "epoch 922 MSE_tr: 18208.766504720283\n",
      "epoch 923 MSE_tr: 18946.885232470828\n",
      "epoch 924 MSE_tr: 20569.941682381712\n",
      "epoch 925 MSE_tr: 15068.413567918406\n",
      "epoch 926 MSE_tr: 16931.717991326994\n",
      "epoch 927 MSE_tr: 19518.27969151389\n",
      "epoch 928 MSE_tr: 18909.967502229523\n",
      "epoch 929 MSE_tr: 21119.910965522624\n",
      "epoch 930 MSE_tr: 18106.969274682986\n",
      "epoch 931 MSE_tr: 22870.34977297766\n",
      "epoch 932 MSE_tr: 19676.881306751817\n",
      "epoch 933 MSE_tr: 26402.25922281918\n",
      "epoch 934 MSE_tr: 17327.14471098576\n",
      "epoch 935 MSE_tr: 16108.674844464938\n",
      "epoch 936 MSE_tr: 14501.670644901284\n",
      "epoch 937 MSE_tr: 15677.314490570287\n",
      "epoch 938 MSE_tr: 14230.039565776275\n",
      "epoch 939 MSE_tr: 15381.240759262988\n",
      "epoch 940 MSE_tr: 20231.739029432996\n",
      "epoch 941 MSE_tr: 16137.98327726487\n",
      "epoch 942 MSE_tr: 19290.24363580854\n",
      "epoch 943 MSE_tr: 16026.680464847977\n",
      "epoch 944 MSE_tr: 18801.774045166327\n",
      "epoch 945 MSE_tr: 22774.648944350898\n",
      "epoch 946 MSE_tr: 18874.053961327976\n",
      "epoch 947 MSE_tr: 25554.799650684767\n",
      "epoch 948 MSE_tr: 19529.32489873162\n",
      "epoch 949 MSE_tr: 17622.342853014088\n",
      "epoch 950 MSE_tr: 16366.053304713372\n",
      "epoch 951 MSE_tr: 21009.74725782399\n",
      "epoch 952 MSE_tr: 13626.001453234006\n",
      "epoch 953 MSE_tr: 16101.443410322727\n",
      "epoch 954 MSE_tr: 14915.011858600004\n",
      "epoch 955 MSE_tr: 20625.865193128342\n",
      "epoch 956 MSE_tr: 15161.100703259444\n",
      "epoch 957 MSE_tr: 17862.361216549885\n",
      "epoch 958 MSE_tr: 17684.78685631999\n",
      "epoch 959 MSE_tr: 22125.368229163716\n",
      "epoch 960 MSE_tr: 23820.693909750313\n",
      "epoch 961 MSE_tr: 22251.59767508071\n",
      "epoch 962 MSE_tr: 36376.919629069875\n",
      "epoch 963 MSE_tr: 19987.67679469494\n",
      "epoch 964 MSE_tr: 22736.161129032953\n",
      "epoch 965 MSE_tr: 21052.91123609153\n",
      "epoch 966 MSE_tr: 18770.958437240843\n",
      "epoch 967 MSE_tr: 28289.26587858604\n",
      "epoch 968 MSE_tr: 18696.777141803977\n",
      "epoch 969 MSE_tr: 19088.458358120653\n",
      "epoch 970 MSE_tr: 19233.308130535705\n",
      "epoch 971 MSE_tr: 17220.98362251782\n",
      "epoch 972 MSE_tr: 21764.401276856253\n",
      "epoch 973 MSE_tr: 18403.85044991774\n",
      "epoch 974 MSE_tr: 19766.27656138805\n",
      "epoch 975 MSE_tr: 20105.137048189634\n",
      "epoch 976 MSE_tr: 23430.30505194353\n",
      "epoch 977 MSE_tr: 16870.81959536261\n",
      "epoch 978 MSE_tr: 22517.714360666836\n",
      "epoch 979 MSE_tr: 22166.710089609027\n",
      "epoch 980 MSE_tr: 22690.408601742896\n",
      "epoch 981 MSE_tr: 27414.282128694867\n",
      "epoch 982 MSE_tr: 25172.353037782552\n",
      "epoch 983 MSE_tr: 19535.228527562704\n",
      "epoch 984 MSE_tr: 16638.575713703693\n",
      "epoch 985 MSE_tr: 26453.747968044412\n",
      "epoch 986 MSE_tr: 14979.702354124769\n",
      "epoch 987 MSE_tr: 28725.226605950484\n",
      "epoch 988 MSE_tr: 15002.192837001377\n",
      "epoch 989 MSE_tr: 13036.322497576937\n",
      "epoch 990 MSE_tr: 16166.442998716524\n",
      "epoch 991 MSE_tr: 12383.393094525349\n",
      "epoch 992 MSE_tr: 13865.748702637535\n",
      "epoch 993 MSE_tr: 16219.59132476035\n",
      "epoch 994 MSE_tr: 13885.828631405808\n",
      "epoch 995 MSE_tr: 12663.240457066286\n",
      "epoch 996 MSE_tr: 15230.538699496248\n",
      "epoch 997 MSE_tr: 14538.622946482206\n",
      "epoch 998 MSE_tr: 16851.8731635983\n",
      "epoch 999 MSE_tr: 16354.433727474254\n",
      "epoch 1000 MSE_tr: 12662.712382324022\n",
      "epoch 1001 MSE_tr: 13521.167922920926\n",
      "epoch 1002 MSE_tr: 11510.448217341906\n",
      "epoch 1003 MSE_tr: 16694.70207525098\n",
      "epoch 1004 MSE_tr: 13736.305771793513\n",
      "epoch 1005 MSE_tr: 14986.262038047314\n",
      "epoch 1006 MSE_tr: 18789.37332538883\n",
      "epoch 1007 MSE_tr: 15732.48321883552\n",
      "epoch 1008 MSE_tr: 15490.869578631864\n",
      "epoch 1009 MSE_tr: 20050.274306365664\n",
      "epoch 1010 MSE_tr: 15797.133508179448\n",
      "epoch 1011 MSE_tr: 17492.938884613814\n",
      "epoch 1012 MSE_tr: 16245.243814728412\n",
      "epoch 1013 MSE_tr: 23446.00956911268\n",
      "epoch 1014 MSE_tr: 24075.764612942046\n",
      "epoch 1015 MSE_tr: 37645.50362348475\n",
      "epoch 1016 MSE_tr: 22647.21053982934\n",
      "epoch 1017 MSE_tr: 34760.319015080415\n",
      "epoch 1018 MSE_tr: 17892.351992938304\n",
      "epoch 1019 MSE_tr: 22506.93051104598\n",
      "epoch 1020 MSE_tr: 14436.976072435667\n",
      "epoch 1021 MSE_tr: 27794.51088888898\n",
      "epoch 1022 MSE_tr: 23564.280580053804\n",
      "epoch 1023 MSE_tr: 18165.965147739676\n",
      "epoch 1024 MSE_tr: 29744.27567766263\n",
      "epoch 1025 MSE_tr: 17053.291003893715\n",
      "epoch 1026 MSE_tr: 23974.40178179157\n",
      "epoch 1027 MSE_tr: 25945.285725115293\n",
      "epoch 1028 MSE_tr: 27158.34880165265\n",
      "epoch 1029 MSE_tr: 20293.097286443302\n",
      "epoch 1030 MSE_tr: 18949.713318831553\n",
      "epoch 1031 MSE_tr: 22149.24220604952\n",
      "epoch 1032 MSE_tr: 16791.42966335631\n",
      "epoch 1033 MSE_tr: 14302.255260714981\n",
      "epoch 1034 MSE_tr: 13507.401402434592\n",
      "epoch 1035 MSE_tr: 15754.475564971\n",
      "epoch 1036 MSE_tr: 12301.275111873305\n",
      "epoch 1037 MSE_tr: 14654.306335429925\n",
      "epoch 1038 MSE_tr: 13838.754207592698\n",
      "epoch 1039 MSE_tr: 14960.127112234792\n",
      "epoch 1040 MSE_tr: 13483.449033875455\n",
      "epoch 1041 MSE_tr: 26144.008112469448\n",
      "epoch 1042 MSE_tr: 15457.325248173775\n",
      "epoch 1043 MSE_tr: 11150.619304522666\n",
      "epoch 1044 MSE_tr: 25035.289217302023\n",
      "epoch 1045 MSE_tr: 15352.53486096442\n",
      "epoch 1046 MSE_tr: 14607.775162675473\n",
      "epoch 1047 MSE_tr: 17172.485898746596\n",
      "epoch 1048 MSE_tr: 13869.02669696025\n",
      "epoch 1049 MSE_tr: 17955.547047678847\n",
      "epoch 1050 MSE_tr: 29275.75359411588\n",
      "epoch 1051 MSE_tr: 19484.930235943248\n",
      "epoch 1052 MSE_tr: 13942.990736631757\n",
      "epoch 1053 MSE_tr: 18478.944911660474\n",
      "epoch 1054 MSE_tr: 13202.488281886044\n",
      "epoch 1055 MSE_tr: 16300.218907277504\n",
      "epoch 1056 MSE_tr: 20715.950643032123\n",
      "epoch 1057 MSE_tr: 14786.251543956567\n",
      "epoch 1058 MSE_tr: 25091.065556124613\n",
      "epoch 1059 MSE_tr: 15764.684153742446\n",
      "epoch 1060 MSE_tr: 15561.356499962096\n",
      "epoch 1061 MSE_tr: 22552.403801968758\n",
      "epoch 1062 MSE_tr: 17656.8266736933\n",
      "epoch 1063 MSE_tr: 18070.813554351353\n",
      "epoch 1064 MSE_tr: 15608.780540694857\n",
      "epoch 1065 MSE_tr: 16440.580733936065\n",
      "epoch 1066 MSE_tr: 19070.063848428417\n",
      "epoch 1067 MSE_tr: 15231.724066616178\n",
      "epoch 1068 MSE_tr: 17863.504020245273\n",
      "epoch 1069 MSE_tr: 21383.852250659766\n",
      "epoch 1070 MSE_tr: 19143.8646295034\n",
      "epoch 1071 MSE_tr: 33031.25181080573\n",
      "epoch 1072 MSE_tr: 22255.189574447417\n",
      "epoch 1073 MSE_tr: 29630.28210857466\n",
      "epoch 1074 MSE_tr: 16869.00196738992\n",
      "epoch 1075 MSE_tr: 15983.604132157841\n",
      "epoch 1076 MSE_tr: 20017.567202127433\n",
      "epoch 1077 MSE_tr: 15646.409916102883\n",
      "epoch 1078 MSE_tr: 15681.063053945392\n",
      "epoch 1079 MSE_tr: 16213.591957427985\n",
      "epoch 1080 MSE_tr: 16300.55105802185\n",
      "epoch 1081 MSE_tr: 16922.974260746552\n",
      "epoch 1082 MSE_tr: 28442.391168859544\n",
      "epoch 1083 MSE_tr: 23979.557098335797\n",
      "epoch 1084 MSE_tr: 13482.857633331178\n",
      "epoch 1085 MSE_tr: 18749.419379824027\n",
      "epoch 1086 MSE_tr: 13556.912136043753\n",
      "epoch 1087 MSE_tr: 14826.823231582623\n",
      "epoch 1088 MSE_tr: 17902.60368820875\n",
      "epoch 1089 MSE_tr: 13785.725180327223\n",
      "epoch 1090 MSE_tr: 17169.0656775044\n",
      "epoch 1091 MSE_tr: 18088.312812430213\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24292/1150572650.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# trainloss.backward(trainloss.clone().detach()) # Calculate Gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect_anomaly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mtrainloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;31m# trainloss.backward(retain_graph=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Update Weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CNN 训练\n",
    "net = Network()\n",
    "# 损失\n",
    "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# criterion = nn.SmoothL1Loss()\n",
    "# loss = torch.sqrt(criterion(x, y))\n",
    "# 加载数据，设置优化器\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=1,shuffle=False)\n",
    "# val_loader = torch.utils.data.DataLoader(train_set, batch_size=1,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1,shuffle=True)\n",
    "# optimizer = torch.optim.Adam(net.parameters(),\n",
    "#         lr=0.00002)\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "# lr_schedule = torch.optim.lr_scheduler.StepLR(\\\n",
    "#         optimizer, 1, gamma=0.8, last_epoch=-1)\n",
    "# 初始化 visdom \n",
    "viz.close()\n",
    "viz = viz_init()\n",
    "vislogDir = './/vislog_ndata//cnn//'\n",
    "if not os.path.exists(vislogDir):\n",
    "    os.makedirs(vislogDir)\n",
    "viz = Visdom(env=viz_acnt.evns, log_to_filename=vislogDir+'vislog_'+get_current_time())\n",
    "vizx = 0\n",
    "# viz.text('MONITOR: Show train process~~',win='Monitor', opts = {'title':'ProcessMonitor',},)\n",
    "\n",
    "# total_test_acc = 0\n",
    "# total_test_correct = 0\n",
    "# totaltest = 0\n",
    "# 训练过程\n",
    "epoch_num = 40000\n",
    "net.to(device)\n",
    "for epoch in range(epoch_num):\n",
    "    # 训练部分\n",
    "    net.to(device)\n",
    "    net.train()\n",
    "    for batch in train_loader:\n",
    "        s_x,s_y,s_a,s_t,s_m_ar,s_m_ab,s_c,s_i = batch\n",
    "        preds = net(s_x.to(device)) \n",
    "        # trainloss = torch.sqrt(criterion(preds.to(device), data_train_MF.to(device)))\n",
    "        trainloss = criterion(preds.to(device), s_y.to(device))\n",
    "        old_loss = trainloss\n",
    "        trainloss = new_loss3(preds.to(device),s_y.to(device),s_c.to(device),\\\n",
    "            s_t.to(device),s_m_ar.to(device),s_m_ab.to(device),s_i.to(device))\n",
    "        # trainloss.requires_grad_()\n",
    "        optimizer.zero_grad()\n",
    "        # trainloss.backward(trainloss.clone().detach()) # Calculate Gradients\n",
    "        with torch.autograd.detect_anomaly():\n",
    "            trainloss.backward()\n",
    "        # trainloss.backward(retain_graph=True)\n",
    "        optimizer.step() # Update Weight\n",
    "    # 定期展示当前的训练效果，即三个变量的 RMSE 值，用以挑选网络\n",
    "    predict_show = []\n",
    "    y_show = []\n",
    "    RMSE_list = [1.,2.,3.]\n",
    "    val_loader = torch.utils.data.DataLoader(train_set, batch_size=1,shuffle=False)\n",
    "    net.eval()\n",
    "    net.to('cpu')\n",
    "    for batch in val_loader:\n",
    "        s_x,s_y,_,_,_,_,_,_ = batch\n",
    "        predict = net(s_x) \n",
    "        pred2 = predict.detach().numpy()\n",
    "        predict_show.append(pred2) \n",
    "        valloss = criterion(predict, s_y)\n",
    "        y_s = s_y.detach().numpy()\n",
    "        y_show.append(y_s)\n",
    "    predict_show = np.array(predict_show)\n",
    "    y_show = np.array(y_show)\n",
    "    for column in range(3):\n",
    "        RMSE_list[column] =  torch.sqrt(criterion(torch.Tensor(predict_show[:,:,column]), torch.Tensor(y_show[:,:,column])))\n",
    "    \n",
    "    viz.line([float(RMSE_list[0])],[vizx],win='MF&KA_RMSE', name='mf_rf',\\\n",
    "        update='append',opts=dict(title='RMSE',xlabel='epoch',ylabel='RMSE',legend = ['mf_rf','mf_bm','ka']))\n",
    "    viz.line([float(RMSE_list[1])],[vizx],win='MF&KA_RMSE', name='mf_bm', update='append')\n",
    "    viz.line([float(RMSE_list[2])],[vizx],win='MF&KA_RMSE', name='ka', update='append')\n",
    "\n",
    "    # 展示参数\n",
    "    if (epoch+1) % 1 ==0:\n",
    "        vizx+=1\n",
    "        print(\n",
    "            \"epoch\", epoch+1, \n",
    "            \"MSE_tr:\", float(trainloss),\n",
    "        )\n",
    "        viz.line([float(trainloss)],[vizx],win='loss_per_1_Epoch', name='Total_loss',\\\n",
    "            update='append',opts=dict(title='loss_per_1_Epoch',xlabel='1epoch',ylabel='loss'))\n",
    "        viz.line([float(old_loss)],[vizx],win='loss_per_1_Epoch', name='MSE_loss', update='append')\n",
    "        # viz.line([float(new_loss)],[vizx],win='new_loss_per_100_Epoch', name='New_loss',\\\n",
    "        #     update='append',opts=dict(title='new_loss_per 100 Epoch',xlabel='100 epoch',ylabel='loss'))\n",
    "        # viz.line([float(torch.sigmoid(new_loss))],[vizx],win='sig_new_loss_per_100_Epoch', name='New_loss',\\\n",
    "        #     update='append',opts=dict(title='sigmoid(new_loss)_per 100 Epoch',xlabel='100 epoch',ylabel='loss'))\n",
    "    # 定期保存\n",
    "    if (epoch+1)%1 == 0:\n",
    "        timeForSave = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "        checkpointPath = ckpDir+'emgsk_ep_'+str(epoch+1)+'_'+timeForSave+'.pth'\n",
    "        c_state = {'model': net.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch}\n",
    "        torch.save(c_state, checkpointPath)\n",
    "\n",
    "# checkpointPath_model = model_Dir+'c_final_'+'acc'+str(int(total_test_acc*10000))+'_'+timeForSave+'.pth'\n",
    "checkpointPath_model = model_Dir+'kmmf_final_'+'_'+timeForSave+'.pth'\n",
    "torch.save(net.state_dict(),checkpointPath_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_x = data_123_te\n",
    "show_y = data_457_te_y\n",
    "show_l = 20\n",
    "column = 2\n",
    "# net = net2\n",
    "net.eval()\n",
    "net.to('cpu')\n",
    "criterion = torch.nn.MSELoss()\n",
    "# predict = net(show_x)\n",
    "# predict = predict.data.numpy()\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=1,shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1,shuffle=False)\n",
    "predict_show = []\n",
    "y_show = []\n",
    "for batch in test_loader:\n",
    "    s_x,s_y,_,_,_,_,_,_ = batch\n",
    "    predict = net(s_x) \n",
    "    pred2 = predict.detach().numpy()\n",
    "    predict_show.append(pred2) \n",
    "    trainloss = criterion(predict, s_y)\n",
    "    y_s = s_y.detach().numpy()\n",
    "    y_show.append(y_s)\n",
    "predict_show = np.array(predict_show)\n",
    "y_show = np.array(y_show)\n",
    "# MSE_show = criterion(predict[:,column], show_y[:,column])\n",
    "MSE_show =  criterion(torch.Tensor(predict_show[:,:,column]), torch.Tensor(y_show[:,:,column]))\n",
    "predict = predict.data.numpy()\n",
    "# 建立等差数列，（起始，终止，个数）\n",
    "x = np.linspace(1,show_l,show_l)\n",
    "\n",
    "plt.title('KA(RMSE:'+str('%.5g' % torch.sqrt(MSE_show))+')')\n",
    "# plt.plot(x , show_y[:,column], label='origin')\n",
    "plt.plot(x , show_y[:,column], label='origin')\n",
    "# plt.plot(x, predict[:,column], color='red', label='predict')\n",
    "plt.plot(x, predict_show[:,0,column], color='red', label='predict')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpointPath_model = model_Dir+'emgsk_fornl_CNN_01'+'_nl_agle_re'+timeForSave+'.pth'\n",
    "# torch.save(net.state_dict(),checkpointPath_model)\n",
    "checkpointPath_model = model_Dir+'emgsk_fornl_CNN_01_norad'+'_agle_'+timeForSave+'.pth'\n",
    "torch.save(net.state_dict(),checkpointPath_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=384, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (out): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (dr1): Dropout2d(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_for_net2 = torch.load(ckpDir+'emgsk_ep_1658_2022_03_29_20_14_09.pth')\n",
    "net2 = Network()\n",
    "# net2.load_state_dict(checkpoint_for_net2)\n",
    "net2.load_state_dict(checkpoint_for_net2['model'])\n",
    "net2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#对网络进行验证\n",
    "# column = 0\n",
    "# show_x = data_123_te\n",
    "# show_y = data_45_te_y\n",
    "# show_l = 20\n",
    "# net.eval()\n",
    "# net.to('cpu')\n",
    "# predict = net(show_x)\n",
    "# # predict = predict.data.numpy()\n",
    "\n",
    "# # MSE_show = criterion(predict[:,column], show_y[:,column])\n",
    "# MSE_show = criterion(predict, show_y)\n",
    "# predict = predict.data.numpy()\n",
    "# # 建立等差数列，（起始，终止，个数）\n",
    "# x = np.linspace(1,show_l,show_l)\n",
    "# # column = 0\n",
    "# plt.title('MF(RMSE:'+str('%.5g' % torch.sqrt(MSE_show))+')')\n",
    "# # plt.plot(x , show_y[:,column], label='origin')\n",
    "# plt.plot(x , show_y, label='origin')\n",
    "# # plt.plot(x, predict[:,column], color='red', label='predict')\n",
    "# plt.plot(x, predict, color='red', label='predict')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "122afd33e14e141e8feafe6109b3cf33c81901f42114774f6f58cb0f50546406"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('ml2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
